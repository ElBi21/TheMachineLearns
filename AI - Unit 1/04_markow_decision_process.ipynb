{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1 align=\"center\"><b>Markow's Decision Process (MDP)</b></h1>\n",
        "<h5 align=\"center\">Practical application of MDPs</h5>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3F5nZPvYzi-"
      },
      "source": [
        "### Preliminary steps\n",
        "\n",
        "Some utils are here defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Z4j_BDGcQxU"
      },
      "outputs": [],
      "source": [
        "def vector_add(a, b):\n",
        "    \"\"\"Component-wise addition of two vectors.\"\"\"\n",
        "    if not (a and b):\n",
        "        return a or b\n",
        "    if hasattr(a, '__iter__') and hasattr(b, '__iter__'):\n",
        "        assert len(a) == len(b)\n",
        "        return list(map(vector_add, a, b))\n",
        "    else:\n",
        "        return a + b\n",
        "\n",
        "def isnumber(x) -> bool:\n",
        "    \"\"\"Is x a number?\"\"\"\n",
        "    return hasattr(x, '__int__')\n",
        "\n",
        "def print_table(table, header=None, sep='', numfmt='{}'):\n",
        "    \"\"\"Print a list of lists as a table, so that columns line up nicely.\n",
        "    header, if specified, will be printed as the first row.\n",
        "    numfmt is the format for all numbers; you might want e.g. '{:.2f}'.\n",
        "    (If you want different formats in different columns,\n",
        "    don't use print_table.) sep is the separator between columns.\"\"\"\n",
        "    justs = ['rjust' if isnumber(x) else 'ljust' for x in table[0]]\n",
        "\n",
        "    if header:\n",
        "        table.insert(0, header)\n",
        "\n",
        "    table = [[numfmt.format(x) if isnumber(x)\n",
        "                else \"███\" if x==None\n",
        "                else x for x in row]\n",
        "             for row in table]\n",
        "    sizes = list(\n",
        "        map(lambda seq: max(map(len, seq)),\n",
        "            list(zip(*[map(str, row) for row in table]))))\n",
        "\n",
        "    for row in table:\n",
        "        print(sep.join(getattr(\n",
        "            str(x), j)(size) for (j, size, x) in zip(justs, sizes, row)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxOKwMlRY2eK"
      },
      "source": [
        "### Markow's Decision Process\n",
        "\n",
        "Here we define a `GridMDP` class, which will be the base for the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jSphAKV3aZeU"
      },
      "outputs": [],
      "source": [
        "class GridMDP:\n",
        "    \"\"\"\n",
        "    A Markov Decision Process on a two-dimensional grid.\n",
        "\n",
        "    Attributes:\n",
        "        `grid` (`list of lists`): Reward grid, where None indicates obstacles.\n",
        "        `terminals` (`set`): Terminal states.\n",
        "        `init` (`tuple`): Initial state.\n",
        "        `gamma` (`float`): Discount factor (0 < gamma <= 1).\n",
        "        `rows` (`int`): Number of rows in the grid.\n",
        "        `cols` (`int`): Number of columns in the grid.\n",
        "        `orientations` (`tuple`): Valid directions as unit vectors: (east, north, west, south).\n",
        "        `turns` (`tuple`): Turn directions: (left, right).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, grid, terminals, init=(1, 1), gamma=0.99):\n",
        "        # Reverse grid for bottom-to-top indexing\n",
        "        self.grid = grid[::-1]\n",
        "        self.rows = len(grid)\n",
        "        self.cols = len(grid[0])\n",
        "\n",
        "        # Extract states, reward, and validate input\n",
        "        self.states = set()\n",
        "        self.reward = {}\n",
        "        for y in range(self.rows):\n",
        "            for x in range(self.cols):\n",
        "                if self.grid[y][x] is not None:\n",
        "                    self.states.add((x, y))\n",
        "                    self.reward[(x, y)] = self.grid[y][x]\n",
        "\n",
        "        if init not in self.states:\n",
        "            raise ValueError(\"Invalid initial state:\", init)\n",
        "        if any(t not in self.states for t in terminals):\n",
        "            raise ValueError(\"Invalid terminal states:\", terminals)\n",
        "\n",
        "        self.terminals = terminals\n",
        "        self.init = init\n",
        "        self.gamma = gamma\n",
        "        self.orientations = EAST, NORTH, WEST, SOUTH = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
        "        self.turns = LEFT, RIGHT = (+1, -1)\n",
        "\n",
        "        # Precompute transition probabilities for efficiency\n",
        "        self.transitions = {s: self._calculate_T(s) for s in self.states}\n",
        "\n",
        "    def _calculate_T(self, s) -> dict:\n",
        "        \"\"\"\n",
        "        Calculate transition probabilities for all actions from a state.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): Current state.\n",
        "\n",
        "        Returns:\n",
        "            dict: Mapping from action to list of (probability, next_state) pairs.\n",
        "        \"\"\"\n",
        "        transitions = {action: [(0.8, self._go(s, action))]\n",
        "                   for action in self.orientations}\n",
        "        for action in transitions:\n",
        "            transitions[action].append((0.1, self._go(s, self._turn_direction(action, -1))))\n",
        "            transitions[action].append((0.1, self._go(s, self._turn_direction(action, +1))))\n",
        "        return transitions\n",
        "\n",
        "    def _turn_direction(self, direction, turn) -> tuple:\n",
        "        \"\"\"\n",
        "        Turn the given direction by the specified amount.\n",
        "\n",
        "        Args:\n",
        "            direction (tuple): Current direction.\n",
        "            turn (int): direction to turn (left: -1, right: 1).\n",
        "\n",
        "        Returns:\n",
        "            tuple: New direction.\n",
        "        \"\"\"\n",
        "        index = self.orientations.index(direction)\n",
        "        return self.orientations[(index + turn) % len(self.orientations)]\n",
        "\n",
        "    def _go(self, state, direction) -> tuple:\n",
        "        \"\"\"\n",
        "        Move one step in the given direction, handling boundaries.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): Current state.\n",
        "            direction (tuple): Direction to move.\n",
        "\n",
        "        Returns:\n",
        "            tuple: New state.\n",
        "        \"\"\"\n",
        "        new_state = tuple(vector_add(state, direction))\n",
        "        return new_state if new_state in self.states else state\n",
        "\n",
        "    def R(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for a state.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): State.\n",
        "\n",
        "        Returns:\n",
        "            float: Reward.\n",
        "        \"\"\"\n",
        "        return self.reward[state]\n",
        "\n",
        "    def T(self, state, action):\n",
        "        \"\"\"\n",
        "        Get the transition probabilities for a state and action.\n",
        "\n",
        "        Args:\n",
        "            state (tuple): State.\n",
        "            action (tuple): Action.\n",
        "\n",
        "        Returns:\n",
        "            list: List of (probability, next_state) pairs.\n",
        "        \"\"\"\n",
        "        return self.transitions[state][action] if action else [(0.0, state)]\n",
        "\n",
        "\n",
        "    def actions(self, state):\n",
        "        \"\"\"\n",
        "        Get the available actions in a state (always oriented actions).\n",
        "\n",
        "        Args:\n",
        "            state (tuple): State.\n",
        "\n",
        "        Returns:\n",
        "            list: List of actions (possible directions).\n",
        "        \"\"\"\n",
        "        if state in self.terminals:\n",
        "            return [None]\n",
        "        else:\n",
        "            return self.orientations\n",
        "\n",
        "    def to_grid(self, mapping):\n",
        "        \"\"\"\n",
        "        Convert a mapping from (x, y) to values into a grid representation.\n",
        "\n",
        "        Args:\n",
        "            mapping (dict): Mapping from (x, y) to values.\n",
        "\n",
        "        Returns:\n",
        "            list of lists: Grid representation.\n",
        "        \"\"\"\n",
        "        return list(reversed([[mapping.get((x, y), None) for x in range(self.cols)]\n",
        "                              for y in range(self.rows)]))\n",
        "\n",
        "    def to_arrows(self, policy):\n",
        "        \"\"\"\n",
        "        Convert a policy (mapping from state to action) into a grid showing corresponding arrow directions.\n",
        "\n",
        "        Args:\n",
        "            policy (dict): Mapping from state to action.\n",
        "\n",
        "        Returns:\n",
        "            list of lists: Grid representation with arrows.\n",
        "        \"\"\"\n",
        "        chars = {(1, 0): \" → \", (0, 1): ' ↑ ', (-1, 0): ' ← ', (0, -1): ' ↓ ', None: ' G '}\n",
        "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Qyljo1rSiL"
      },
      "source": [
        "### Environment\n",
        "\n",
        "Here we specify the environment on which our robot will walk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XQuBcxnrbTf5"
      },
      "outputs": [],
      "source": [
        "grid = [\n",
        "    [None, None, None, None, None, None, None, None, None, None, None],\n",
        "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None, +5.0, None],\n",
        "    [None, -0.1, None, None, None, None, None, None, None, -0.1, None],\n",
        "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None],\n",
        "    [None, -0.1, None, None, None, None, None, None, None, None, None],\n",
        "    [None, -0.1, None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None],\n",
        "    [None, -0.1, None, None, None, None, None, -0.1, -0.1, -0.1, None],\n",
        "    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None],\n",
        "    [None, None, None, None, None, -0.1, None, -0.1, -0.1, -0.1, None],\n",
        "    [None, -5.0, -0.1, -0.1, -0.1, -0.1, None, -0.1, -0.1, -0.1, None],\n",
        "    [None, None, None, None, None, None, None, None, None, None, None]\n",
        "]\n",
        "terminals = [(9, 9)]\n",
        "maze = GridMDP(grid, terminals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6ceKmrYrcZe"
      },
      "source": [
        "### Q-Values and Best Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tUQz01ZhdAj-"
      },
      "outputs": [],
      "source": [
        "def q_values(mdp, s, V):\n",
        "    res = [sum(p*V[si] for p, si in mdp.T(s, a))\n",
        "            for a in mdp.actions(s)]\n",
        "    return res\n",
        "\n",
        "\n",
        "def best_policy(mdp, V):\n",
        "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
        "    as a mapping from state to action.\"\"\"\n",
        "    pi = {}\n",
        "    for s in mdp.states:\n",
        "        if s in mdp.terminals:  # Skip terminal states.\n",
        "            pi[s] = None\n",
        "            continue\n",
        "        qs = q_values(mdp, s, V)\n",
        "        pi[s]=  mdp.actions(s)[qs.index(max(qs))]\n",
        "    return pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9_lp8MErj-6"
      },
      "source": [
        "### Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UOf-pcruis50"
      },
      "outputs": [],
      "source": [
        "def value_iteration(self, iterations=20, epsilon=1e-3) -> dict:\n",
        "    \"\"\"\n",
        "    Perform value iteration algorithm to solve the MDP.\n",
        "\n",
        "    Args:\n",
        "        iterations (int): Number of iterations.\n",
        "    Returns:\n",
        "        dict: Mapping from state to value.\n",
        "    \"\"\"\n",
        "\n",
        "    V = {s: 0 for s in self.states}\n",
        "    for _ in range(iterations):\n",
        "        _V = V.copy()\n",
        "        delta = 0\n",
        "\n",
        "        for s in self.states:\n",
        "            V[s] = self.R(s) + self.gamma * max(q_values(self, s, V))\n",
        "            delta = max(delta, abs(_V[s]-V[s]))\n",
        "\n",
        "        if delta <= epsilon * (1 - self.gamma) / self.gamma:\n",
        "            break\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HSkw8mArqfG"
      },
      "source": [
        "### Run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo6iDZsWiVLZ",
        "outputId": "e6d05c7b-d874-4fbb-a5f9-15ee69c0b384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "█████████████████████████████████\n",
            "███ ↓  ←  ←  ←  ←  ←  ← ███ G ███\n",
            "███ ↓ █████████████████████ ↑ ███\n",
            "███ →  →  →  →  →  →  →  →  ↑ ███\n",
            "███ ↑ ███████████████████████████\n",
            "███ ↑ ███ →  →  →  →  ↓  ←  ← ███\n",
            "███ ↑ ███████████████ ↓  ←  ← ███\n",
            "███ ↑  ←  ←  ←  ←  ←  ←  ←  ← ███\n",
            "███████████████ ↑ ███ ↑  ←  ← ███\n",
            "███ →  →  →  →  ↑ ███ ↑  ←  ← ███\n",
            "█████████████████████████████████\n"
          ]
        }
      ],
      "source": [
        "V = value_iteration(maze)\n",
        "pi = best_policy(maze, V)\n",
        "print_table(maze.to_arrows(pi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AaInvl3ubfH"
      },
      "source": [
        "### Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nQjAf9rSkMxj"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(mdp, pi, V, k=20):\n",
        "    \"\"\"Return an updated utility mapping V from each state in the MDP to its\n",
        "    utility, using an approximation (modified policy iteration).\"\"\"\n",
        "    for i in range(k):\n",
        "        for s in mdp.states:\n",
        "            V[s] = mdp.R(s) + mdp.gamma*sum(p*V[si] for p, si in mdp.T(s, pi[s]))\n",
        "    return V\n",
        "\n",
        "\n",
        "def policy_iteration(mdp, iterations=10):\n",
        "\n",
        "    import random\n",
        "    V = {s: 0 for s in mdp.states}\n",
        "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        V = policy_evaluation(mdp, pi, V)\n",
        "        unchanged = True\n",
        "\n",
        "        for s in mdp.states:\n",
        "            qs = q_values(mdp, s, V)\n",
        "            q_max = max(qs)\n",
        "            a_max = mdp.actions(s)[qs.index(q_max)]\n",
        "\n",
        "            if q_max > sum(p*V[si] for p, si in mdp.T(s, pi[s])):\n",
        "                pi[s] = a_max\n",
        "                unchanged = False\n",
        "\n",
        "        if unchanged:\n",
        "            break\n",
        "\n",
        "    return pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7vP2ReNu71D"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN9aier_qjzR",
        "outputId": "f7ebe408-fdef-4028-a192-9d5b59c09896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " → With maximum 3 iterations\n",
            "█████████████████████████████████\n",
            "███ ↓  ←  ←  ←  ←  ←  ← ███ G ███\n",
            "███ ↓ █████████████████████ ↑ ███\n",
            "███ →  →  →  →  →  →  →  →  ↑ ███\n",
            "███ ↑ ███████████████████████████\n",
            "███ ↑ ███ ←  ←  ←  ←  ←  ←  → ███\n",
            "███ ↑ ███████████████ ↑  →  ↑ ███\n",
            "███ ←  ←  →  ↑  ↓  →  →  →  ↓ ███\n",
            "███████████████ ↓ ███ ↓  →  → ███\n",
            "███ →  →  →  →  → ███ ↓  →  → ███\n",
            "█████████████████████████████████\n",
            "\n",
            "\n",
            " → With maximum 10 iterations\n",
            "█████████████████████████████████\n",
            "███ ↓  ←  ←  ←  ←  ←  ← ███ G ███\n",
            "███ ↓ █████████████████████ ↑ ███\n",
            "███ →  →  →  →  →  →  →  →  ↑ ███\n",
            "███ ↑ ███████████████████████████\n",
            "███ ↑ ███ →  →  →  →  ↓  ↓  ← ███\n",
            "███ ↑ ███████████████ ↓  ←  ← ███\n",
            "███ ↑  ←  ←  ←  ←  ←  ←  ←  ← ███\n",
            "███████████████ ↑ ███ ↑  ↑  ← ███\n",
            "███ →  →  →  →  ↑ ███ ↑  ↑  ↑ ███\n",
            "█████████████████████████████████\n"
          ]
        }
      ],
      "source": [
        "pi_3 = policy_iteration(maze, iterations=3)\n",
        "pi_10 = policy_iteration(maze, iterations=10)\n",
        "\n",
        "print(\" → With maximum 3 iterations\")\n",
        "print_table(maze.to_arrows(pi_3))\n",
        "print(\"\\n\\n → With maximum 10 iterations\")\n",
        "print_table(maze.to_arrows(pi_10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise\n",
        "\n",
        "Blackjack is a card game where, by having 2 cards, you must get as close as possible to 21, without going over 21. Let's try to model a MDP that can play a simplified game of blackjack.\n",
        "\n",
        "This simplified version uses only 3 possible cards in the deck: `2`, `3` and `4`. The goal is to reach 5. Assume that there is an infinite number of `2`s, `3`s and `4`s in the deck.\n",
        "\n",
        "The possible states (considering both the positive and the negative states) are the following:\n",
        "```python\n",
        "[\n",
        "    0, # no cards are dealt\n",
        "    2, # one card (2) is dealt\n",
        "    3, # one card (3) is dealt\n",
        "    4, # either one card (4) is dealt or two cards (2, 2) have been dealt\n",
        "    5, # with (2, 3) and (3, 2), the objective\n",
        "    # Negative\n",
        "    6, # with (3, 3) and (2, 2, 2)\n",
        "    7, # with (3, 4), (3, 2, 2) and the permutations of both tuples\n",
        "    # and so on...\n",
        "]\n",
        "```\n",
        "Now, if we were in state $0$, we would have the following probabilities of getting the following cards:\n",
        "\n",
        "$$\n",
        "0 \\rightarrow_{\\text{draw}} \\frac{1}{3} \\text{ for } \\Bigg( 2, \\; 3, \\; 4 \\Bigg)\n",
        "$$\n",
        "\n",
        "If we were in state $3$ instead, we would have\n",
        "\n",
        "$$\n",
        "3 \\rightarrow_{\\text{draw}} \\frac{1}{3} \\text{ for } \\Bigg( 5 \\Bigg) + \\frac{2}{3} \\text{ for losing } (S \\geq 6)\n",
        "$$\n",
        "\n",
        "In order to do it, try to do it with either `R(S)`, `R(S, a)` or `R(S, a, S')`"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
