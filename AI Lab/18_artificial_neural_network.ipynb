{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><b>AI Lab: Computer Vision and NLP</b></h1>\n",
    "<h3 align=\"center\">Lessons 17-19: Artificial Neural Networks</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our brain works with neurons, which are small cells which can retain data and make reasonments. Computer scientists have tried along the past years to replicate a human brain and thus allow a computer to have an artificial one. Since neurons work with small quantities of electricity, then such small pulses of energy could be translated with a binary pulse. One of the first applications of a neuron was to implement logic gates, such as the `AND` ($y = x_1 \\wedge x_2$) or the `OR` ($y = x_1 \\vee x_2$).\n",
    "\n",
    "An artificial neuron is very similar to a logic gate: with an `AND` gate with 2 inputs $x_1$ and $x_2$, we would have that the output $y$ is equal to 1 if, by summing the two inputs, we reach a threshold $T \\geq 2$. The neuron has, instead of a fixed threshold, a variable one. Such threshold represents thus the **activation function** of a neuron.\n",
    "\n",
    "We can compare how powerful tools such as `pytorch` are compared to other tools such as `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to load the dataset, and thanks to the passing of the option `return_X_y = true` we can tell `scikit-learn` to pass us the dataset already splitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, y = fetch_openml('mnist_784', return_X_y=True)\n",
    "digits = load_digits()\n",
    "x, y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create the test and train datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a multilevel perceptron (**MLP**), and with `scikit-learn` we can provide some hidden layers which are not considering the input and output layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(20,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardo/miniconda3/envs/ailab/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(20,))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(20,))</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can test the model and see how well it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937037037037037\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, this was the version with `scikit-learn`. We said thought previously that `scikit-learn` runs only on CPUs, and this is not helpful while trying to train a large model. So we can use instead `pytorch`, and we can try to do it all over again from scratch. The problem with `pytorch` is that we have to do everything from scratch. In order to use `pytorch`, we start by importing the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a dataset with some images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"pytorch_datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor() # This function will transform the data into tensors,\n",
    "                         # we just need to specify the transformation function\n",
    ") # type: ignore\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"pytorch_datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access to the dataset with the syntax\n",
    "```python\n",
    "image, label = data[index]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = training_data[25]\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here it comes the most important part: we have to fetch the GPU in order to use it instead of the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start to create our multilevel perceptron. It's important to define meticulously the two methods of the MLP class: the `init` method contains the basis for the MLP, while the `forward` method passes the data to the next layer.\n",
    "\n",
    "The `nn.Sequential()` function is useful when creating the MLP because it allows to execute all the instructions inside such function sequentially. Within the `nn.Sequential()` function we have another function, `nn.Linear()`. Such function allows to define the input and output size in edges of our linear perceptron layer:\n",
    "\n",
    "$$\n",
    "\\text{size}(a_{\\text{in}}) \\; \\longrightarrow \\; \\text{size}(a_{\\text{out}})\n",
    "$$\n",
    "$$\n",
    "\\mathbb{R}^i \\; \\longmapsto \\; \\mathbb{R}^j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurMLP(nn.Module):\n",
    "    def __init__(self, seq=True):\n",
    "        if seq:\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(\n",
    "                # From this layer x we will export 50 edges to the next layer...\n",
    "                nn.Linear(28 * 28, 50),\n",
    "                nn.Sigmoid(),\n",
    "                # ...then from 50 to 100...\n",
    "                nn.Linear(50, 100),\n",
    "                nn.Sigmoid(),\n",
    "                # ...then from 100 to 50...\n",
    "                nn.Linear(100, 50),\n",
    "                nn.Sigmoid(),\n",
    "                # ...then from 50 to 120, and so on and so forth...\n",
    "                #nn.Linear(50, 120),\n",
    "                #nn.Sigmoid(),\n",
    "                #nn.Linear(120, 60),\n",
    "                #nn.Sigmoid(),\n",
    "                #nn.Linear(60, 50),\n",
    "                #nn.Sigmoid(),\n",
    "\n",
    "                # Output layer\n",
    "                nn.Linear(50, 10)\n",
    "            )\n",
    "            self.flatten = nn.Flatten()\n",
    "        else:\n",
    "            super().__init__()\n",
    "            self.input_layer = nn.Linear(28*28, 50)\n",
    "            self.hidden1 = nn.Linear(50, 100)\n",
    "            self.hidden2 = nn.Linear(100, 50)\n",
    "            self.hidden3 = nn.Linear(50, 120)\n",
    "            self.hidden4 = nn.Linear(120, 60)\n",
    "            self.hidden5 = nn.Linear(60, 50)\n",
    "            self.output_layer = nn.Linear(50, 10)\n",
    "            self.activation = nn.Sigmoid()\n",
    "            self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x, seq=True):\n",
    "        if seq:\n",
    "            x = self.flatten(x)\n",
    "            logits = self.mlp(x)\n",
    "            return logits\n",
    "        else:\n",
    "            x = self.input_layer(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.flatten(x)\n",
    "            logits = self.mlp(x)\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must instantiate the model by defining the hyperparameters and the optimizer. The optimizer is the function in charge of tuning the parameters of the model, so it computes the necessary derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OurMLP().to(device)\n",
    "model_no_seq = OurMLP(seq=False).to(device)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer. We can use either SGD or AdamW, but AdamW is more performant than SGD\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create the data loader, which is in charge of taking the data from the disk and give it to the `pytorch` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get some metrics from the model's performances, thanks to the `torchmetrics` package. We are using the `multiclass` parameter because our model will label each object with only one label, assigning such object to one class only; with the `multilabel` parameter our model would've assigned one object to multiple classes, thus assigning multiple labels to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, we define the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch=None):\n",
    "    \"\"\"Trains an epoch of the model\n",
    "    \n",
    "    Parameters:\n",
    "        - `dataloader`: the dataloader of the dataset\n",
    "        - `model`: the model used\n",
    "        - `loss_fn`: the loss function of the model\n",
    "        - `optimizer`: the optimizer\n",
    "        - `epoch`: the index of the epoch\n",
    "    \"\"\"\n",
    "    size = len(dataloader)\n",
    "\n",
    "    # Get the batch from the dataset\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        # Move data to the device used\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute the prediction and the loss\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Adjust the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Print some information\n",
    "        if batch % 20 == 0:\n",
    "            loss_value, current_batch = loss.item(), (batch + 1) * len(x)\n",
    "            print(f\"→ Loss: {loss_value} [Batch {current_batch}/{size}, Epoch {epoch}/{epochs}]\")\n",
    "            accuracy = metric(pred, y)\n",
    "            print(f\"Accuracy of batch {current_batch}/{size}: {accuracy}\")\n",
    "        \n",
    "    accuracy = metric.compute()\n",
    "    print(f\"=== The epoch {epoch}/{epochs} has finished training ===\")\n",
    "    print(f\"→ Final accuracy of the epoch: {accuracy}\")\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this previous function is just the training loop for one epoch. We'll have to run it for all the epochs that we want to have. We now just need to define the test loop and we are ready to run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader)\n",
    "\n",
    "    # Disable the updating of the weights\n",
    "    with torch.no_grad():\n",
    "        for index, (x, y) in enumerate(dataloader):\n",
    "            # Move the data to the device used for testing\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Get the model prediction\n",
    "            pred = model(x)\n",
    "\n",
    "            # Get the accuracy score\n",
    "            acc = metric(pred, y)\n",
    "            print(f\"→ Accuracy for image {index}: {acc}\")\n",
    "    acc = metric.compute()\n",
    "    print(f\"=== The testing loop has finished ===\")\n",
    "    print(f\"→ Final testing accuracy of the model: {accuracy}\")\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Loss: 2.3128719329833984 [Batch 16/3750, Epoch 0/5]\n",
      "Accuracy of batch 16/3750: 0.125\n",
      "→ Loss: 2.2943038940429688 [Batch 336/3750, Epoch 0/5]\n",
      "Accuracy of batch 336/3750: 0.0\n",
      "→ Loss: 2.3291964530944824 [Batch 656/3750, Epoch 0/5]\n",
      "Accuracy of batch 656/3750: 0.0\n",
      "→ Loss: 2.2258284091949463 [Batch 976/3750, Epoch 0/5]\n",
      "Accuracy of batch 976/3750: 0.25\n",
      "→ Loss: 2.3320088386535645 [Batch 1296/3750, Epoch 0/5]\n",
      "Accuracy of batch 1296/3750: 0.0625\n",
      "→ Loss: 2.283327341079712 [Batch 1616/3750, Epoch 0/5]\n",
      "Accuracy of batch 1616/3750: 0.0625\n",
      "→ Loss: 2.2853291034698486 [Batch 1936/3750, Epoch 0/5]\n",
      "Accuracy of batch 1936/3750: 0.125\n",
      "→ Loss: 2.3331735134124756 [Batch 2256/3750, Epoch 0/5]\n",
      "Accuracy of batch 2256/3750: 0.0\n",
      "→ Loss: 2.264916181564331 [Batch 2576/3750, Epoch 0/5]\n",
      "Accuracy of batch 2576/3750: 0.25\n",
      "→ Loss: 2.3006837368011475 [Batch 2896/3750, Epoch 0/5]\n",
      "Accuracy of batch 2896/3750: 0.125\n",
      "→ Loss: 2.2652618885040283 [Batch 3216/3750, Epoch 0/5]\n",
      "Accuracy of batch 3216/3750: 0.25\n",
      "→ Loss: 2.3056273460388184 [Batch 3536/3750, Epoch 0/5]\n",
      "Accuracy of batch 3536/3750: 0.0\n",
      "→ Loss: 2.2666430473327637 [Batch 3856/3750, Epoch 0/5]\n",
      "Accuracy of batch 3856/3750: 0.1875\n",
      "→ Loss: 2.26180362701416 [Batch 4176/3750, Epoch 0/5]\n",
      "Accuracy of batch 4176/3750: 0.1875\n",
      "→ Loss: 2.281953811645508 [Batch 4496/3750, Epoch 0/5]\n",
      "Accuracy of batch 4496/3750: 0.0625\n",
      "→ Loss: 2.285351276397705 [Batch 4816/3750, Epoch 0/5]\n",
      "Accuracy of batch 4816/3750: 0.125\n",
      "→ Loss: 2.283463716506958 [Batch 5136/3750, Epoch 0/5]\n",
      "Accuracy of batch 5136/3750: 0.0625\n",
      "→ Loss: 2.2685346603393555 [Batch 5456/3750, Epoch 0/5]\n",
      "Accuracy of batch 5456/3750: 0.1875\n",
      "→ Loss: 2.285411834716797 [Batch 5776/3750, Epoch 0/5]\n",
      "Accuracy of batch 5776/3750: 0.1875\n",
      "→ Loss: 2.289275646209717 [Batch 6096/3750, Epoch 0/5]\n",
      "Accuracy of batch 6096/3750: 0.1875\n",
      "→ Loss: 2.272500991821289 [Batch 6416/3750, Epoch 0/5]\n",
      "Accuracy of batch 6416/3750: 0.125\n",
      "→ Loss: 2.280797243118286 [Batch 6736/3750, Epoch 0/5]\n",
      "Accuracy of batch 6736/3750: 0.0625\n",
      "→ Loss: 2.252791166305542 [Batch 7056/3750, Epoch 0/5]\n",
      "Accuracy of batch 7056/3750: 0.375\n",
      "→ Loss: 2.2694473266601562 [Batch 7376/3750, Epoch 0/5]\n",
      "Accuracy of batch 7376/3750: 0.1875\n",
      "→ Loss: 2.2439002990722656 [Batch 7696/3750, Epoch 0/5]\n",
      "Accuracy of batch 7696/3750: 0.1875\n",
      "→ Loss: 2.2469675540924072 [Batch 8016/3750, Epoch 0/5]\n",
      "Accuracy of batch 8016/3750: 0.0625\n",
      "→ Loss: 2.2285079956054688 [Batch 8336/3750, Epoch 0/5]\n",
      "Accuracy of batch 8336/3750: 0.4375\n",
      "→ Loss: 2.2553341388702393 [Batch 8656/3750, Epoch 0/5]\n",
      "Accuracy of batch 8656/3750: 0.1875\n",
      "→ Loss: 2.1973466873168945 [Batch 8976/3750, Epoch 0/5]\n",
      "Accuracy of batch 8976/3750: 0.3125\n",
      "→ Loss: 2.1724092960357666 [Batch 9296/3750, Epoch 0/5]\n",
      "Accuracy of batch 9296/3750: 0.3125\n",
      "→ Loss: 2.1946539878845215 [Batch 9616/3750, Epoch 0/5]\n",
      "Accuracy of batch 9616/3750: 0.25\n",
      "→ Loss: 2.2244229316711426 [Batch 9936/3750, Epoch 0/5]\n",
      "Accuracy of batch 9936/3750: 0.0\n",
      "→ Loss: 2.1858057975769043 [Batch 10256/3750, Epoch 0/5]\n",
      "Accuracy of batch 10256/3750: 0.25\n",
      "→ Loss: 2.1806185245513916 [Batch 10576/3750, Epoch 0/5]\n",
      "Accuracy of batch 10576/3750: 0.125\n",
      "→ Loss: 2.1863653659820557 [Batch 10896/3750, Epoch 0/5]\n",
      "Accuracy of batch 10896/3750: 0.125\n",
      "→ Loss: 2.1399412155151367 [Batch 11216/3750, Epoch 0/5]\n",
      "Accuracy of batch 11216/3750: 0.25\n",
      "→ Loss: 2.1784870624542236 [Batch 11536/3750, Epoch 0/5]\n",
      "Accuracy of batch 11536/3750: 0.125\n",
      "→ Loss: 2.1320741176605225 [Batch 11856/3750, Epoch 0/5]\n",
      "Accuracy of batch 11856/3750: 0.3125\n",
      "→ Loss: 2.148439884185791 [Batch 12176/3750, Epoch 0/5]\n",
      "Accuracy of batch 12176/3750: 0.1875\n",
      "→ Loss: 2.0982511043548584 [Batch 12496/3750, Epoch 0/5]\n",
      "Accuracy of batch 12496/3750: 0.1875\n",
      "→ Loss: 2.0502476692199707 [Batch 12816/3750, Epoch 0/5]\n",
      "Accuracy of batch 12816/3750: 0.3125\n",
      "→ Loss: 2.0321199893951416 [Batch 13136/3750, Epoch 0/5]\n",
      "Accuracy of batch 13136/3750: 0.3125\n",
      "→ Loss: 2.0384511947631836 [Batch 13456/3750, Epoch 0/5]\n",
      "Accuracy of batch 13456/3750: 0.25\n",
      "→ Loss: 2.1042001247406006 [Batch 13776/3750, Epoch 0/5]\n",
      "Accuracy of batch 13776/3750: 0.1875\n",
      "→ Loss: 2.07084059715271 [Batch 14096/3750, Epoch 0/5]\n",
      "Accuracy of batch 14096/3750: 0.0625\n",
      "→ Loss: 1.9705795049667358 [Batch 14416/3750, Epoch 0/5]\n",
      "Accuracy of batch 14416/3750: 0.375\n",
      "→ Loss: 2.0444369316101074 [Batch 14736/3750, Epoch 0/5]\n",
      "Accuracy of batch 14736/3750: 0.1875\n",
      "→ Loss: 2.0056545734405518 [Batch 15056/3750, Epoch 0/5]\n",
      "Accuracy of batch 15056/3750: 0.25\n",
      "→ Loss: 2.002340793609619 [Batch 15376/3750, Epoch 0/5]\n",
      "Accuracy of batch 15376/3750: 0.25\n",
      "→ Loss: 2.009695529937744 [Batch 15696/3750, Epoch 0/5]\n",
      "Accuracy of batch 15696/3750: 0.3125\n",
      "→ Loss: 2.0342440605163574 [Batch 16016/3750, Epoch 0/5]\n",
      "Accuracy of batch 16016/3750: 0.0\n",
      "→ Loss: 1.9207491874694824 [Batch 16336/3750, Epoch 0/5]\n",
      "Accuracy of batch 16336/3750: 0.4375\n",
      "→ Loss: 1.9387167692184448 [Batch 16656/3750, Epoch 0/5]\n",
      "Accuracy of batch 16656/3750: 0.0625\n",
      "→ Loss: 1.9572848081588745 [Batch 16976/3750, Epoch 0/5]\n",
      "Accuracy of batch 16976/3750: 0.0625\n",
      "→ Loss: 1.9345752000808716 [Batch 17296/3750, Epoch 0/5]\n",
      "Accuracy of batch 17296/3750: 0.1875\n",
      "→ Loss: 1.9595389366149902 [Batch 17616/3750, Epoch 0/5]\n",
      "Accuracy of batch 17616/3750: 0.0625\n",
      "→ Loss: 1.9554665088653564 [Batch 17936/3750, Epoch 0/5]\n",
      "Accuracy of batch 17936/3750: 0.125\n",
      "→ Loss: 1.9623340368270874 [Batch 18256/3750, Epoch 0/5]\n",
      "Accuracy of batch 18256/3750: 0.1875\n",
      "→ Loss: 1.964681625366211 [Batch 18576/3750, Epoch 0/5]\n",
      "Accuracy of batch 18576/3750: 0.1875\n",
      "→ Loss: 1.911257266998291 [Batch 18896/3750, Epoch 0/5]\n",
      "Accuracy of batch 18896/3750: 0.1875\n",
      "→ Loss: 1.8407413959503174 [Batch 19216/3750, Epoch 0/5]\n",
      "Accuracy of batch 19216/3750: 0.1875\n",
      "→ Loss: 1.8740594387054443 [Batch 19536/3750, Epoch 0/5]\n",
      "Accuracy of batch 19536/3750: 0.25\n",
      "→ Loss: 1.9234488010406494 [Batch 19856/3750, Epoch 0/5]\n",
      "Accuracy of batch 19856/3750: 0.25\n",
      "→ Loss: 1.854081392288208 [Batch 20176/3750, Epoch 0/5]\n",
      "Accuracy of batch 20176/3750: 0.1875\n",
      "→ Loss: 1.8305667638778687 [Batch 20496/3750, Epoch 0/5]\n",
      "Accuracy of batch 20496/3750: 0.25\n",
      "→ Loss: 1.8794612884521484 [Batch 20816/3750, Epoch 0/5]\n",
      "Accuracy of batch 20816/3750: 0.375\n",
      "→ Loss: 1.79734468460083 [Batch 21136/3750, Epoch 0/5]\n",
      "Accuracy of batch 21136/3750: 0.375\n",
      "→ Loss: 1.8296531438827515 [Batch 21456/3750, Epoch 0/5]\n",
      "Accuracy of batch 21456/3750: 0.3125\n",
      "→ Loss: 1.8534679412841797 [Batch 21776/3750, Epoch 0/5]\n",
      "Accuracy of batch 21776/3750: 0.125\n",
      "→ Loss: 1.8429615497589111 [Batch 22096/3750, Epoch 0/5]\n",
      "Accuracy of batch 22096/3750: 0.125\n",
      "→ Loss: 1.8088812828063965 [Batch 22416/3750, Epoch 0/5]\n",
      "Accuracy of batch 22416/3750: 0.25\n",
      "→ Loss: 1.775200366973877 [Batch 22736/3750, Epoch 0/5]\n",
      "Accuracy of batch 22736/3750: 0.25\n",
      "→ Loss: 1.8558450937271118 [Batch 23056/3750, Epoch 0/5]\n",
      "Accuracy of batch 23056/3750: 0.1875\n",
      "→ Loss: 1.8040953874588013 [Batch 23376/3750, Epoch 0/5]\n",
      "Accuracy of batch 23376/3750: 0.25\n",
      "→ Loss: 1.8935788869857788 [Batch 23696/3750, Epoch 0/5]\n",
      "Accuracy of batch 23696/3750: 0.1875\n",
      "→ Loss: 1.7563209533691406 [Batch 24016/3750, Epoch 0/5]\n",
      "Accuracy of batch 24016/3750: 0.5625\n",
      "→ Loss: 1.8284436464309692 [Batch 24336/3750, Epoch 0/5]\n",
      "Accuracy of batch 24336/3750: 0.3125\n",
      "→ Loss: 1.823169231414795 [Batch 24656/3750, Epoch 0/5]\n",
      "Accuracy of batch 24656/3750: 0.125\n",
      "→ Loss: 1.8310173749923706 [Batch 24976/3750, Epoch 0/5]\n",
      "Accuracy of batch 24976/3750: 0.3125\n",
      "→ Loss: 1.8304307460784912 [Batch 25296/3750, Epoch 0/5]\n",
      "Accuracy of batch 25296/3750: 0.125\n",
      "→ Loss: 1.742347002029419 [Batch 25616/3750, Epoch 0/5]\n",
      "Accuracy of batch 25616/3750: 0.4375\n",
      "→ Loss: 1.7789629697799683 [Batch 25936/3750, Epoch 0/5]\n",
      "Accuracy of batch 25936/3750: 0.1875\n",
      "→ Loss: 1.6749331951141357 [Batch 26256/3750, Epoch 0/5]\n",
      "Accuracy of batch 26256/3750: 0.25\n",
      "→ Loss: 1.7430363893508911 [Batch 26576/3750, Epoch 0/5]\n",
      "Accuracy of batch 26576/3750: 0.25\n",
      "→ Loss: 1.7674180269241333 [Batch 26896/3750, Epoch 0/5]\n",
      "Accuracy of batch 26896/3750: 0.3125\n",
      "→ Loss: 1.81667160987854 [Batch 27216/3750, Epoch 0/5]\n",
      "Accuracy of batch 27216/3750: 0.1875\n",
      "→ Loss: 1.6850850582122803 [Batch 27536/3750, Epoch 0/5]\n",
      "Accuracy of batch 27536/3750: 0.5\n",
      "→ Loss: 1.8105988502502441 [Batch 27856/3750, Epoch 0/5]\n",
      "Accuracy of batch 27856/3750: 0.1875\n",
      "→ Loss: 1.75074303150177 [Batch 28176/3750, Epoch 0/5]\n",
      "Accuracy of batch 28176/3750: 0.5\n",
      "→ Loss: 1.7880356311798096 [Batch 28496/3750, Epoch 0/5]\n",
      "Accuracy of batch 28496/3750: 0.3125\n",
      "→ Loss: 1.7166416645050049 [Batch 28816/3750, Epoch 0/5]\n",
      "Accuracy of batch 28816/3750: 0.4375\n",
      "→ Loss: 1.795710563659668 [Batch 29136/3750, Epoch 0/5]\n",
      "Accuracy of batch 29136/3750: 0.1875\n",
      "→ Loss: 1.7985389232635498 [Batch 29456/3750, Epoch 0/5]\n",
      "Accuracy of batch 29456/3750: 0.5625\n",
      "→ Loss: 1.6850156784057617 [Batch 29776/3750, Epoch 0/5]\n",
      "Accuracy of batch 29776/3750: 0.125\n",
      "→ Loss: 1.7989962100982666 [Batch 30096/3750, Epoch 0/5]\n",
      "Accuracy of batch 30096/3750: 0.125\n",
      "→ Loss: 1.7396737337112427 [Batch 30416/3750, Epoch 0/5]\n",
      "Accuracy of batch 30416/3750: 0.4375\n",
      "→ Loss: 1.8124346733093262 [Batch 30736/3750, Epoch 0/5]\n",
      "Accuracy of batch 30736/3750: 0.125\n",
      "→ Loss: 1.6264580488204956 [Batch 31056/3750, Epoch 0/5]\n",
      "Accuracy of batch 31056/3750: 0.25\n",
      "→ Loss: 1.7219327688217163 [Batch 31376/3750, Epoch 0/5]\n",
      "Accuracy of batch 31376/3750: 0.1875\n",
      "→ Loss: 1.782902717590332 [Batch 31696/3750, Epoch 0/5]\n",
      "Accuracy of batch 31696/3750: 0.25\n",
      "→ Loss: 1.7844977378845215 [Batch 32016/3750, Epoch 0/5]\n",
      "Accuracy of batch 32016/3750: 0.375\n",
      "→ Loss: 1.7575907707214355 [Batch 32336/3750, Epoch 0/5]\n",
      "Accuracy of batch 32336/3750: 0.1875\n",
      "→ Loss: 1.7197390794754028 [Batch 32656/3750, Epoch 0/5]\n",
      "Accuracy of batch 32656/3750: 0.25\n",
      "→ Loss: 1.6514441967010498 [Batch 32976/3750, Epoch 0/5]\n",
      "Accuracy of batch 32976/3750: 0.1875\n",
      "→ Loss: 1.7181179523468018 [Batch 33296/3750, Epoch 0/5]\n",
      "Accuracy of batch 33296/3750: 0.25\n",
      "→ Loss: 1.6058809757232666 [Batch 33616/3750, Epoch 0/5]\n",
      "Accuracy of batch 33616/3750: 0.4375\n",
      "→ Loss: 1.782251238822937 [Batch 33936/3750, Epoch 0/5]\n",
      "Accuracy of batch 33936/3750: 0.375\n",
      "→ Loss: 1.6920913457870483 [Batch 34256/3750, Epoch 0/5]\n",
      "Accuracy of batch 34256/3750: 0.3125\n",
      "→ Loss: 1.6261111497879028 [Batch 34576/3750, Epoch 0/5]\n",
      "Accuracy of batch 34576/3750: 0.5\n",
      "→ Loss: 1.6829560995101929 [Batch 34896/3750, Epoch 0/5]\n",
      "Accuracy of batch 34896/3750: 0.375\n",
      "→ Loss: 1.7234086990356445 [Batch 35216/3750, Epoch 0/5]\n",
      "Accuracy of batch 35216/3750: 0.4375\n",
      "→ Loss: 1.7097212076187134 [Batch 35536/3750, Epoch 0/5]\n",
      "Accuracy of batch 35536/3750: 0.375\n",
      "→ Loss: 1.7150743007659912 [Batch 35856/3750, Epoch 0/5]\n",
      "Accuracy of batch 35856/3750: 0.375\n",
      "→ Loss: 1.6310402154922485 [Batch 36176/3750, Epoch 0/5]\n",
      "Accuracy of batch 36176/3750: 0.375\n",
      "→ Loss: 1.6138088703155518 [Batch 36496/3750, Epoch 0/5]\n",
      "Accuracy of batch 36496/3750: 0.25\n",
      "→ Loss: 1.6860003471374512 [Batch 36816/3750, Epoch 0/5]\n",
      "Accuracy of batch 36816/3750: 0.4375\n",
      "→ Loss: 1.5445055961608887 [Batch 37136/3750, Epoch 0/5]\n",
      "Accuracy of batch 37136/3750: 0.3125\n",
      "→ Loss: 1.6555439233779907 [Batch 37456/3750, Epoch 0/5]\n",
      "Accuracy of batch 37456/3750: 0.1875\n",
      "→ Loss: 1.8251237869262695 [Batch 37776/3750, Epoch 0/5]\n",
      "Accuracy of batch 37776/3750: 0.0625\n",
      "→ Loss: 1.5961107015609741 [Batch 38096/3750, Epoch 0/5]\n",
      "Accuracy of batch 38096/3750: 0.5\n",
      "→ Loss: 1.6451939344406128 [Batch 38416/3750, Epoch 0/5]\n",
      "Accuracy of batch 38416/3750: 0.125\n",
      "→ Loss: 1.663138747215271 [Batch 38736/3750, Epoch 0/5]\n",
      "Accuracy of batch 38736/3750: 0.375\n",
      "→ Loss: 1.7553447484970093 [Batch 39056/3750, Epoch 0/5]\n",
      "Accuracy of batch 39056/3750: 0.1875\n",
      "→ Loss: 1.5906177759170532 [Batch 39376/3750, Epoch 0/5]\n",
      "Accuracy of batch 39376/3750: 0.3125\n",
      "→ Loss: 1.617859959602356 [Batch 39696/3750, Epoch 0/5]\n",
      "Accuracy of batch 39696/3750: 0.4375\n",
      "→ Loss: 1.621692180633545 [Batch 40016/3750, Epoch 0/5]\n",
      "Accuracy of batch 40016/3750: 0.625\n",
      "→ Loss: 1.6662334203720093 [Batch 40336/3750, Epoch 0/5]\n",
      "Accuracy of batch 40336/3750: 0.375\n",
      "→ Loss: 1.6037042140960693 [Batch 40656/3750, Epoch 0/5]\n",
      "Accuracy of batch 40656/3750: 0.25\n",
      "→ Loss: 1.5134716033935547 [Batch 40976/3750, Epoch 0/5]\n",
      "Accuracy of batch 40976/3750: 0.375\n",
      "→ Loss: 1.6300705671310425 [Batch 41296/3750, Epoch 0/5]\n",
      "Accuracy of batch 41296/3750: 0.375\n",
      "→ Loss: 1.5669517517089844 [Batch 41616/3750, Epoch 0/5]\n",
      "Accuracy of batch 41616/3750: 0.1875\n",
      "→ Loss: 1.5173101425170898 [Batch 41936/3750, Epoch 0/5]\n",
      "Accuracy of batch 41936/3750: 0.3125\n",
      "→ Loss: 1.7566845417022705 [Batch 42256/3750, Epoch 0/5]\n",
      "Accuracy of batch 42256/3750: 0.375\n",
      "→ Loss: 1.399591088294983 [Batch 42576/3750, Epoch 0/5]\n",
      "Accuracy of batch 42576/3750: 0.4375\n",
      "→ Loss: 1.6131856441497803 [Batch 42896/3750, Epoch 0/5]\n",
      "Accuracy of batch 42896/3750: 0.5\n",
      "→ Loss: 1.7171447277069092 [Batch 43216/3750, Epoch 0/5]\n",
      "Accuracy of batch 43216/3750: 0.125\n",
      "→ Loss: 1.7240549325942993 [Batch 43536/3750, Epoch 0/5]\n",
      "Accuracy of batch 43536/3750: 0.1875\n",
      "→ Loss: 1.7056680917739868 [Batch 43856/3750, Epoch 0/5]\n",
      "Accuracy of batch 43856/3750: 0.1875\n",
      "→ Loss: 1.6884773969650269 [Batch 44176/3750, Epoch 0/5]\n",
      "Accuracy of batch 44176/3750: 0.3125\n",
      "→ Loss: 1.4203225374221802 [Batch 44496/3750, Epoch 0/5]\n",
      "Accuracy of batch 44496/3750: 0.5\n",
      "→ Loss: 1.6550885438919067 [Batch 44816/3750, Epoch 0/5]\n",
      "Accuracy of batch 44816/3750: 0.1875\n",
      "→ Loss: 1.541078805923462 [Batch 45136/3750, Epoch 0/5]\n",
      "Accuracy of batch 45136/3750: 0.1875\n",
      "→ Loss: 1.6564269065856934 [Batch 45456/3750, Epoch 0/5]\n",
      "Accuracy of batch 45456/3750: 0.125\n",
      "→ Loss: 1.5747359991073608 [Batch 45776/3750, Epoch 0/5]\n",
      "Accuracy of batch 45776/3750: 0.25\n",
      "→ Loss: 1.5195012092590332 [Batch 46096/3750, Epoch 0/5]\n",
      "Accuracy of batch 46096/3750: 0.4375\n",
      "→ Loss: 1.5744377374649048 [Batch 46416/3750, Epoch 0/5]\n",
      "Accuracy of batch 46416/3750: 0.3125\n",
      "→ Loss: 1.4797720909118652 [Batch 46736/3750, Epoch 0/5]\n",
      "Accuracy of batch 46736/3750: 0.125\n",
      "→ Loss: 1.598881483078003 [Batch 47056/3750, Epoch 0/5]\n",
      "Accuracy of batch 47056/3750: 0.25\n",
      "→ Loss: 1.5627361536026 [Batch 47376/3750, Epoch 0/5]\n",
      "Accuracy of batch 47376/3750: 0.5625\n",
      "→ Loss: 1.4353915452957153 [Batch 47696/3750, Epoch 0/5]\n",
      "Accuracy of batch 47696/3750: 0.3125\n",
      "→ Loss: 1.3976397514343262 [Batch 48016/3750, Epoch 0/5]\n",
      "Accuracy of batch 48016/3750: 0.375\n",
      "→ Loss: 1.715092658996582 [Batch 48336/3750, Epoch 0/5]\n",
      "Accuracy of batch 48336/3750: 0.125\n",
      "→ Loss: 1.6268415451049805 [Batch 48656/3750, Epoch 0/5]\n",
      "Accuracy of batch 48656/3750: 0.4375\n",
      "→ Loss: 1.5498285293579102 [Batch 48976/3750, Epoch 0/5]\n",
      "Accuracy of batch 48976/3750: 0.25\n",
      "→ Loss: 1.5016082525253296 [Batch 49296/3750, Epoch 0/5]\n",
      "Accuracy of batch 49296/3750: 0.3125\n",
      "→ Loss: 1.6386239528656006 [Batch 49616/3750, Epoch 0/5]\n",
      "Accuracy of batch 49616/3750: 0.375\n",
      "→ Loss: 1.5078836679458618 [Batch 49936/3750, Epoch 0/5]\n",
      "Accuracy of batch 49936/3750: 0.4375\n",
      "→ Loss: 1.4486294984817505 [Batch 50256/3750, Epoch 0/5]\n",
      "Accuracy of batch 50256/3750: 0.5\n",
      "→ Loss: 1.6641308069229126 [Batch 50576/3750, Epoch 0/5]\n",
      "Accuracy of batch 50576/3750: 0.25\n",
      "→ Loss: 1.4870855808258057 [Batch 50896/3750, Epoch 0/5]\n",
      "Accuracy of batch 50896/3750: 0.3125\n",
      "→ Loss: 1.4007179737091064 [Batch 51216/3750, Epoch 0/5]\n",
      "Accuracy of batch 51216/3750: 0.4375\n",
      "→ Loss: 1.7583534717559814 [Batch 51536/3750, Epoch 0/5]\n",
      "Accuracy of batch 51536/3750: 0.25\n",
      "→ Loss: 1.69110107421875 [Batch 51856/3750, Epoch 0/5]\n",
      "Accuracy of batch 51856/3750: 0.1875\n",
      "→ Loss: 1.5310099124908447 [Batch 52176/3750, Epoch 0/5]\n",
      "Accuracy of batch 52176/3750: 0.375\n",
      "→ Loss: 1.5849144458770752 [Batch 52496/3750, Epoch 0/5]\n",
      "Accuracy of batch 52496/3750: 0.5625\n",
      "→ Loss: 1.5574140548706055 [Batch 52816/3750, Epoch 0/5]\n",
      "Accuracy of batch 52816/3750: 0.1875\n",
      "→ Loss: 1.5697388648986816 [Batch 53136/3750, Epoch 0/5]\n",
      "Accuracy of batch 53136/3750: 0.25\n",
      "→ Loss: 1.5384496450424194 [Batch 53456/3750, Epoch 0/5]\n",
      "Accuracy of batch 53456/3750: 0.125\n",
      "→ Loss: 1.508296251296997 [Batch 53776/3750, Epoch 0/5]\n",
      "Accuracy of batch 53776/3750: 0.375\n",
      "→ Loss: 1.4654289484024048 [Batch 54096/3750, Epoch 0/5]\n",
      "Accuracy of batch 54096/3750: 0.25\n",
      "→ Loss: 1.7721916437149048 [Batch 54416/3750, Epoch 0/5]\n",
      "Accuracy of batch 54416/3750: 0.375\n",
      "→ Loss: 1.4618198871612549 [Batch 54736/3750, Epoch 0/5]\n",
      "Accuracy of batch 54736/3750: 0.3125\n",
      "→ Loss: 1.5644433498382568 [Batch 55056/3750, Epoch 0/5]\n",
      "Accuracy of batch 55056/3750: 0.3125\n",
      "→ Loss: 1.7153204679489136 [Batch 55376/3750, Epoch 0/5]\n",
      "Accuracy of batch 55376/3750: 0.375\n",
      "→ Loss: 1.4439109563827515 [Batch 55696/3750, Epoch 0/5]\n",
      "Accuracy of batch 55696/3750: 0.3125\n",
      "→ Loss: 1.6700832843780518 [Batch 56016/3750, Epoch 0/5]\n",
      "Accuracy of batch 56016/3750: 0.1875\n",
      "→ Loss: 1.5201531648635864 [Batch 56336/3750, Epoch 0/5]\n",
      "Accuracy of batch 56336/3750: 0.375\n",
      "→ Loss: 1.3919591903686523 [Batch 56656/3750, Epoch 0/5]\n",
      "Accuracy of batch 56656/3750: 0.5\n",
      "→ Loss: 1.6648956537246704 [Batch 56976/3750, Epoch 0/5]\n",
      "Accuracy of batch 56976/3750: 0.5\n",
      "→ Loss: 1.52345871925354 [Batch 57296/3750, Epoch 0/5]\n",
      "Accuracy of batch 57296/3750: 0.3125\n",
      "→ Loss: 1.5192794799804688 [Batch 57616/3750, Epoch 0/5]\n",
      "Accuracy of batch 57616/3750: 0.25\n",
      "→ Loss: 1.5523048639297485 [Batch 57936/3750, Epoch 0/5]\n",
      "Accuracy of batch 57936/3750: 0.3125\n",
      "→ Loss: 1.5512205362319946 [Batch 58256/3750, Epoch 0/5]\n",
      "Accuracy of batch 58256/3750: 0.4375\n",
      "→ Loss: 1.4028656482696533 [Batch 58576/3750, Epoch 0/5]\n",
      "Accuracy of batch 58576/3750: 0.4375\n",
      "→ Loss: 1.4750151634216309 [Batch 58896/3750, Epoch 0/5]\n",
      "Accuracy of batch 58896/3750: 0.25\n",
      "→ Loss: 1.4229487180709839 [Batch 59216/3750, Epoch 0/5]\n",
      "Accuracy of batch 59216/3750: 0.3125\n",
      "→ Loss: 1.539480447769165 [Batch 59536/3750, Epoch 0/5]\n",
      "Accuracy of batch 59536/3750: 0.375\n",
      "→ Loss: 1.5702755451202393 [Batch 59856/3750, Epoch 0/5]\n",
      "Accuracy of batch 59856/3750: 0.125\n",
      "=== The epoch 0/5 has finished training ===\n",
      "→ Final accuracy of the epoch: 0.2639627754688263\n",
      "→ Loss: 1.4759525060653687 [Batch 16/3750, Epoch 1/5]\n",
      "Accuracy of batch 16/3750: 0.1875\n",
      "→ Loss: 1.6677325963974 [Batch 336/3750, Epoch 1/5]\n",
      "Accuracy of batch 336/3750: 0.3125\n",
      "→ Loss: 1.323509931564331 [Batch 656/3750, Epoch 1/5]\n",
      "Accuracy of batch 656/3750: 0.5625\n",
      "→ Loss: 1.5730990171432495 [Batch 976/3750, Epoch 1/5]\n",
      "Accuracy of batch 976/3750: 0.25\n",
      "→ Loss: 1.635381817817688 [Batch 1296/3750, Epoch 1/5]\n",
      "Accuracy of batch 1296/3750: 0.1875\n",
      "→ Loss: 1.4890745878219604 [Batch 1616/3750, Epoch 1/5]\n",
      "Accuracy of batch 1616/3750: 0.375\n",
      "→ Loss: 1.6970492601394653 [Batch 1936/3750, Epoch 1/5]\n",
      "Accuracy of batch 1936/3750: 0.3125\n",
      "→ Loss: 1.5923458337783813 [Batch 2256/3750, Epoch 1/5]\n",
      "Accuracy of batch 2256/3750: 0.5\n",
      "→ Loss: 1.5007061958312988 [Batch 2576/3750, Epoch 1/5]\n",
      "Accuracy of batch 2576/3750: 0.1875\n",
      "→ Loss: 1.5941877365112305 [Batch 2896/3750, Epoch 1/5]\n",
      "Accuracy of batch 2896/3750: 0.3125\n",
      "→ Loss: 1.4658478498458862 [Batch 3216/3750, Epoch 1/5]\n",
      "Accuracy of batch 3216/3750: 0.25\n",
      "→ Loss: 1.5337159633636475 [Batch 3536/3750, Epoch 1/5]\n",
      "Accuracy of batch 3536/3750: 0.4375\n",
      "→ Loss: 1.3732880353927612 [Batch 3856/3750, Epoch 1/5]\n",
      "Accuracy of batch 3856/3750: 0.4375\n",
      "→ Loss: 1.4768551588058472 [Batch 4176/3750, Epoch 1/5]\n",
      "Accuracy of batch 4176/3750: 0.375\n",
      "→ Loss: 1.531320571899414 [Batch 4496/3750, Epoch 1/5]\n",
      "Accuracy of batch 4496/3750: 0.375\n",
      "→ Loss: 1.4627366065979004 [Batch 4816/3750, Epoch 1/5]\n",
      "Accuracy of batch 4816/3750: 0.4375\n",
      "→ Loss: 1.4318310022354126 [Batch 5136/3750, Epoch 1/5]\n",
      "Accuracy of batch 5136/3750: 0.3125\n",
      "→ Loss: 1.481895089149475 [Batch 5456/3750, Epoch 1/5]\n",
      "Accuracy of batch 5456/3750: 0.375\n",
      "→ Loss: 1.7105188369750977 [Batch 5776/3750, Epoch 1/5]\n",
      "Accuracy of batch 5776/3750: 0.1875\n",
      "→ Loss: 1.6052353382110596 [Batch 6096/3750, Epoch 1/5]\n",
      "Accuracy of batch 6096/3750: 0.3125\n",
      "→ Loss: 1.4469035863876343 [Batch 6416/3750, Epoch 1/5]\n",
      "Accuracy of batch 6416/3750: 0.25\n",
      "→ Loss: 1.588816523551941 [Batch 6736/3750, Epoch 1/5]\n",
      "Accuracy of batch 6736/3750: 0.3125\n",
      "→ Loss: 1.6549139022827148 [Batch 7056/3750, Epoch 1/5]\n",
      "Accuracy of batch 7056/3750: 0.1875\n",
      "→ Loss: 1.4845131635665894 [Batch 7376/3750, Epoch 1/5]\n",
      "Accuracy of batch 7376/3750: 0.4375\n",
      "→ Loss: 1.33791184425354 [Batch 7696/3750, Epoch 1/5]\n",
      "Accuracy of batch 7696/3750: 0.625\n",
      "→ Loss: 1.3922538757324219 [Batch 8016/3750, Epoch 1/5]\n",
      "Accuracy of batch 8016/3750: 0.375\n",
      "→ Loss: 1.748982548713684 [Batch 8336/3750, Epoch 1/5]\n",
      "Accuracy of batch 8336/3750: 0.375\n",
      "→ Loss: 1.6905543804168701 [Batch 8656/3750, Epoch 1/5]\n",
      "Accuracy of batch 8656/3750: 0.375\n",
      "→ Loss: 1.6318175792694092 [Batch 8976/3750, Epoch 1/5]\n",
      "Accuracy of batch 8976/3750: 0.375\n",
      "→ Loss: 1.397122859954834 [Batch 9296/3750, Epoch 1/5]\n",
      "Accuracy of batch 9296/3750: 0.375\n",
      "→ Loss: 1.5562584400177002 [Batch 9616/3750, Epoch 1/5]\n",
      "Accuracy of batch 9616/3750: 0.25\n",
      "→ Loss: 1.3538459539413452 [Batch 9936/3750, Epoch 1/5]\n",
      "Accuracy of batch 9936/3750: 0.25\n",
      "→ Loss: 1.5021227598190308 [Batch 10256/3750, Epoch 1/5]\n",
      "Accuracy of batch 10256/3750: 0.25\n",
      "→ Loss: 1.7532676458358765 [Batch 10576/3750, Epoch 1/5]\n",
      "Accuracy of batch 10576/3750: 0.125\n",
      "→ Loss: 1.4160728454589844 [Batch 10896/3750, Epoch 1/5]\n",
      "Accuracy of batch 10896/3750: 0.375\n",
      "→ Loss: 1.3237295150756836 [Batch 11216/3750, Epoch 1/5]\n",
      "Accuracy of batch 11216/3750: 0.4375\n",
      "→ Loss: 1.6051064729690552 [Batch 11536/3750, Epoch 1/5]\n",
      "Accuracy of batch 11536/3750: 0.3125\n",
      "→ Loss: 1.5579713582992554 [Batch 11856/3750, Epoch 1/5]\n",
      "Accuracy of batch 11856/3750: 0.375\n",
      "→ Loss: 1.3720803260803223 [Batch 12176/3750, Epoch 1/5]\n",
      "Accuracy of batch 12176/3750: 0.5\n",
      "→ Loss: 1.7080368995666504 [Batch 12496/3750, Epoch 1/5]\n",
      "Accuracy of batch 12496/3750: 0.1875\n",
      "→ Loss: 1.3392466306686401 [Batch 12816/3750, Epoch 1/5]\n",
      "Accuracy of batch 12816/3750: 0.5\n",
      "→ Loss: 1.6163758039474487 [Batch 13136/3750, Epoch 1/5]\n",
      "Accuracy of batch 13136/3750: 0.4375\n",
      "→ Loss: 1.5321968793869019 [Batch 13456/3750, Epoch 1/5]\n",
      "Accuracy of batch 13456/3750: 0.3125\n",
      "→ Loss: 1.5002890825271606 [Batch 13776/3750, Epoch 1/5]\n",
      "Accuracy of batch 13776/3750: 0.3125\n",
      "→ Loss: 1.5420656204223633 [Batch 14096/3750, Epoch 1/5]\n",
      "Accuracy of batch 14096/3750: 0.5625\n",
      "→ Loss: 1.5203404426574707 [Batch 14416/3750, Epoch 1/5]\n",
      "Accuracy of batch 14416/3750: 0.5625\n",
      "→ Loss: 1.5352343320846558 [Batch 14736/3750, Epoch 1/5]\n",
      "Accuracy of batch 14736/3750: 0.4375\n",
      "→ Loss: 1.481028437614441 [Batch 15056/3750, Epoch 1/5]\n",
      "Accuracy of batch 15056/3750: 0.5\n",
      "→ Loss: 1.2776308059692383 [Batch 15376/3750, Epoch 1/5]\n",
      "Accuracy of batch 15376/3750: 0.5625\n",
      "→ Loss: 1.5220670700073242 [Batch 15696/3750, Epoch 1/5]\n",
      "Accuracy of batch 15696/3750: 0.625\n",
      "→ Loss: 1.6125743389129639 [Batch 16016/3750, Epoch 1/5]\n",
      "Accuracy of batch 16016/3750: 0.1875\n",
      "→ Loss: 1.4629740715026855 [Batch 16336/3750, Epoch 1/5]\n",
      "Accuracy of batch 16336/3750: 0.5625\n",
      "→ Loss: 1.353501558303833 [Batch 16656/3750, Epoch 1/5]\n",
      "Accuracy of batch 16656/3750: 0.375\n",
      "→ Loss: 1.3624125719070435 [Batch 16976/3750, Epoch 1/5]\n",
      "Accuracy of batch 16976/3750: 0.3125\n",
      "→ Loss: 1.5038329362869263 [Batch 17296/3750, Epoch 1/5]\n",
      "Accuracy of batch 17296/3750: 0.375\n",
      "→ Loss: 1.4554035663604736 [Batch 17616/3750, Epoch 1/5]\n",
      "Accuracy of batch 17616/3750: 0.375\n",
      "→ Loss: 1.2550514936447144 [Batch 17936/3750, Epoch 1/5]\n",
      "Accuracy of batch 17936/3750: 0.4375\n",
      "→ Loss: 1.7265218496322632 [Batch 18256/3750, Epoch 1/5]\n",
      "Accuracy of batch 18256/3750: 0.3125\n",
      "→ Loss: 1.5078496932983398 [Batch 18576/3750, Epoch 1/5]\n",
      "Accuracy of batch 18576/3750: 0.625\n",
      "→ Loss: 1.542991280555725 [Batch 18896/3750, Epoch 1/5]\n",
      "Accuracy of batch 18896/3750: 0.375\n",
      "→ Loss: 1.108243465423584 [Batch 19216/3750, Epoch 1/5]\n",
      "Accuracy of batch 19216/3750: 0.75\n",
      "→ Loss: 1.3990782499313354 [Batch 19536/3750, Epoch 1/5]\n",
      "Accuracy of batch 19536/3750: 0.5\n",
      "→ Loss: 1.4332361221313477 [Batch 19856/3750, Epoch 1/5]\n",
      "Accuracy of batch 19856/3750: 0.5\n",
      "→ Loss: 1.5278396606445312 [Batch 20176/3750, Epoch 1/5]\n",
      "Accuracy of batch 20176/3750: 0.5625\n",
      "→ Loss: 1.648504376411438 [Batch 20496/3750, Epoch 1/5]\n",
      "Accuracy of batch 20496/3750: 0.75\n",
      "→ Loss: 1.5476006269454956 [Batch 20816/3750, Epoch 1/5]\n",
      "Accuracy of batch 20816/3750: 0.5625\n",
      "→ Loss: 1.3876286745071411 [Batch 21136/3750, Epoch 1/5]\n",
      "Accuracy of batch 21136/3750: 0.4375\n",
      "→ Loss: 1.4409236907958984 [Batch 21456/3750, Epoch 1/5]\n",
      "Accuracy of batch 21456/3750: 0.4375\n",
      "→ Loss: 1.4842315912246704 [Batch 21776/3750, Epoch 1/5]\n",
      "Accuracy of batch 21776/3750: 0.5\n",
      "→ Loss: 1.5443919897079468 [Batch 22096/3750, Epoch 1/5]\n",
      "Accuracy of batch 22096/3750: 0.1875\n",
      "→ Loss: 1.4918466806411743 [Batch 22416/3750, Epoch 1/5]\n",
      "Accuracy of batch 22416/3750: 0.5\n",
      "→ Loss: 1.349834680557251 [Batch 22736/3750, Epoch 1/5]\n",
      "Accuracy of batch 22736/3750: 0.625\n",
      "→ Loss: 1.5470187664031982 [Batch 23056/3750, Epoch 1/5]\n",
      "Accuracy of batch 23056/3750: 0.25\n",
      "→ Loss: 1.5871366262435913 [Batch 23376/3750, Epoch 1/5]\n",
      "Accuracy of batch 23376/3750: 0.375\n",
      "→ Loss: 1.4110761880874634 [Batch 23696/3750, Epoch 1/5]\n",
      "Accuracy of batch 23696/3750: 0.4375\n",
      "→ Loss: 1.576709270477295 [Batch 24016/3750, Epoch 1/5]\n",
      "Accuracy of batch 24016/3750: 0.5\n",
      "→ Loss: 1.5733524560928345 [Batch 24336/3750, Epoch 1/5]\n",
      "Accuracy of batch 24336/3750: 0.375\n",
      "→ Loss: 1.5520533323287964 [Batch 24656/3750, Epoch 1/5]\n",
      "Accuracy of batch 24656/3750: 0.125\n",
      "→ Loss: 1.6134388446807861 [Batch 24976/3750, Epoch 1/5]\n",
      "Accuracy of batch 24976/3750: 0.3125\n",
      "→ Loss: 1.6747897863388062 [Batch 25296/3750, Epoch 1/5]\n",
      "Accuracy of batch 25296/3750: 0.25\n",
      "→ Loss: 1.5583263635635376 [Batch 25616/3750, Epoch 1/5]\n",
      "Accuracy of batch 25616/3750: 0.375\n",
      "→ Loss: 1.4349756240844727 [Batch 25936/3750, Epoch 1/5]\n",
      "Accuracy of batch 25936/3750: 0.375\n",
      "→ Loss: 1.3732273578643799 [Batch 26256/3750, Epoch 1/5]\n",
      "Accuracy of batch 26256/3750: 0.4375\n",
      "→ Loss: 1.3827499151229858 [Batch 26576/3750, Epoch 1/5]\n",
      "Accuracy of batch 26576/3750: 0.4375\n",
      "→ Loss: 1.4305638074874878 [Batch 26896/3750, Epoch 1/5]\n",
      "Accuracy of batch 26896/3750: 0.4375\n",
      "→ Loss: 1.4036650657653809 [Batch 27216/3750, Epoch 1/5]\n",
      "Accuracy of batch 27216/3750: 0.5\n",
      "→ Loss: 1.228744626045227 [Batch 27536/3750, Epoch 1/5]\n",
      "Accuracy of batch 27536/3750: 0.5625\n",
      "→ Loss: 1.5733591318130493 [Batch 27856/3750, Epoch 1/5]\n",
      "Accuracy of batch 27856/3750: 0.375\n",
      "→ Loss: 1.5933865308761597 [Batch 28176/3750, Epoch 1/5]\n",
      "Accuracy of batch 28176/3750: 0.25\n",
      "→ Loss: 1.421891450881958 [Batch 28496/3750, Epoch 1/5]\n",
      "Accuracy of batch 28496/3750: 0.375\n",
      "→ Loss: 1.3343406915664673 [Batch 28816/3750, Epoch 1/5]\n",
      "Accuracy of batch 28816/3750: 0.4375\n",
      "→ Loss: 1.5939918756484985 [Batch 29136/3750, Epoch 1/5]\n",
      "Accuracy of batch 29136/3750: 0.3125\n",
      "→ Loss: 1.548365592956543 [Batch 29456/3750, Epoch 1/5]\n",
      "Accuracy of batch 29456/3750: 0.5625\n",
      "→ Loss: 1.4533313512802124 [Batch 29776/3750, Epoch 1/5]\n",
      "Accuracy of batch 29776/3750: 0.3125\n",
      "→ Loss: 1.638368844985962 [Batch 30096/3750, Epoch 1/5]\n",
      "Accuracy of batch 30096/3750: 0.125\n",
      "→ Loss: 1.4154621362686157 [Batch 30416/3750, Epoch 1/5]\n",
      "Accuracy of batch 30416/3750: 0.5625\n",
      "→ Loss: 1.5792402029037476 [Batch 30736/3750, Epoch 1/5]\n",
      "Accuracy of batch 30736/3750: 0.5\n",
      "→ Loss: 1.3045532703399658 [Batch 31056/3750, Epoch 1/5]\n",
      "Accuracy of batch 31056/3750: 0.5625\n",
      "→ Loss: 1.5384219884872437 [Batch 31376/3750, Epoch 1/5]\n",
      "Accuracy of batch 31376/3750: 0.3125\n",
      "→ Loss: 1.5423316955566406 [Batch 31696/3750, Epoch 1/5]\n",
      "Accuracy of batch 31696/3750: 0.375\n",
      "→ Loss: 1.6388190984725952 [Batch 32016/3750, Epoch 1/5]\n",
      "Accuracy of batch 32016/3750: 0.3125\n",
      "→ Loss: 1.445461392402649 [Batch 32336/3750, Epoch 1/5]\n",
      "Accuracy of batch 32336/3750: 0.375\n",
      "→ Loss: 1.457787275314331 [Batch 32656/3750, Epoch 1/5]\n",
      "Accuracy of batch 32656/3750: 0.3125\n",
      "→ Loss: 1.3658661842346191 [Batch 32976/3750, Epoch 1/5]\n",
      "Accuracy of batch 32976/3750: 0.5625\n",
      "→ Loss: 1.5298343896865845 [Batch 33296/3750, Epoch 1/5]\n",
      "Accuracy of batch 33296/3750: 0.5\n",
      "→ Loss: 1.2181695699691772 [Batch 33616/3750, Epoch 1/5]\n",
      "Accuracy of batch 33616/3750: 0.625\n",
      "→ Loss: 1.478553295135498 [Batch 33936/3750, Epoch 1/5]\n",
      "Accuracy of batch 33936/3750: 0.3125\n",
      "→ Loss: 1.4349639415740967 [Batch 34256/3750, Epoch 1/5]\n",
      "Accuracy of batch 34256/3750: 0.625\n",
      "→ Loss: 1.3659803867340088 [Batch 34576/3750, Epoch 1/5]\n",
      "Accuracy of batch 34576/3750: 0.5\n",
      "→ Loss: 1.4611802101135254 [Batch 34896/3750, Epoch 1/5]\n",
      "Accuracy of batch 34896/3750: 0.375\n",
      "→ Loss: 1.5217252969741821 [Batch 35216/3750, Epoch 1/5]\n",
      "Accuracy of batch 35216/3750: 0.5625\n",
      "→ Loss: 1.5343236923217773 [Batch 35536/3750, Epoch 1/5]\n",
      "Accuracy of batch 35536/3750: 0.4375\n",
      "→ Loss: 1.4089940786361694 [Batch 35856/3750, Epoch 1/5]\n",
      "Accuracy of batch 35856/3750: 0.25\n",
      "→ Loss: 1.3913863897323608 [Batch 36176/3750, Epoch 1/5]\n",
      "Accuracy of batch 36176/3750: 0.4375\n",
      "→ Loss: 1.320438027381897 [Batch 36496/3750, Epoch 1/5]\n",
      "Accuracy of batch 36496/3750: 0.375\n",
      "→ Loss: 1.4331716299057007 [Batch 36816/3750, Epoch 1/5]\n",
      "Accuracy of batch 36816/3750: 0.5625\n",
      "→ Loss: 1.2703522443771362 [Batch 37136/3750, Epoch 1/5]\n",
      "Accuracy of batch 37136/3750: 0.4375\n",
      "→ Loss: 1.4653747081756592 [Batch 37456/3750, Epoch 1/5]\n",
      "Accuracy of batch 37456/3750: 0.375\n",
      "→ Loss: 1.6908849477767944 [Batch 37776/3750, Epoch 1/5]\n",
      "Accuracy of batch 37776/3750: 0.25\n",
      "→ Loss: 1.4042537212371826 [Batch 38096/3750, Epoch 1/5]\n",
      "Accuracy of batch 38096/3750: 0.5625\n",
      "→ Loss: 1.406123399734497 [Batch 38416/3750, Epoch 1/5]\n",
      "Accuracy of batch 38416/3750: 0.375\n",
      "→ Loss: 1.4073491096496582 [Batch 38736/3750, Epoch 1/5]\n",
      "Accuracy of batch 38736/3750: 0.6875\n",
      "→ Loss: 1.4752668142318726 [Batch 39056/3750, Epoch 1/5]\n",
      "Accuracy of batch 39056/3750: 0.5\n",
      "→ Loss: 1.3042653799057007 [Batch 39376/3750, Epoch 1/5]\n",
      "Accuracy of batch 39376/3750: 0.5\n",
      "→ Loss: 1.3447188138961792 [Batch 39696/3750, Epoch 1/5]\n",
      "Accuracy of batch 39696/3750: 0.5625\n",
      "→ Loss: 1.3771003484725952 [Batch 40016/3750, Epoch 1/5]\n",
      "Accuracy of batch 40016/3750: 0.5625\n",
      "→ Loss: 1.4546339511871338 [Batch 40336/3750, Epoch 1/5]\n",
      "Accuracy of batch 40336/3750: 0.5625\n",
      "→ Loss: 1.332771897315979 [Batch 40656/3750, Epoch 1/5]\n",
      "Accuracy of batch 40656/3750: 0.6875\n",
      "→ Loss: 1.1835559606552124 [Batch 40976/3750, Epoch 1/5]\n",
      "Accuracy of batch 40976/3750: 0.5625\n",
      "→ Loss: 1.3720567226409912 [Batch 41296/3750, Epoch 1/5]\n",
      "Accuracy of batch 41296/3750: 0.625\n",
      "→ Loss: 1.3043506145477295 [Batch 41616/3750, Epoch 1/5]\n",
      "Accuracy of batch 41616/3750: 0.4375\n",
      "→ Loss: 1.2811685800552368 [Batch 41936/3750, Epoch 1/5]\n",
      "Accuracy of batch 41936/3750: 0.375\n",
      "→ Loss: 1.517134428024292 [Batch 42256/3750, Epoch 1/5]\n",
      "Accuracy of batch 42256/3750: 0.625\n",
      "→ Loss: 0.9276655316352844 [Batch 42576/3750, Epoch 1/5]\n",
      "Accuracy of batch 42576/3750: 0.5625\n",
      "→ Loss: 1.3283586502075195 [Batch 42896/3750, Epoch 1/5]\n",
      "Accuracy of batch 42896/3750: 0.625\n",
      "→ Loss: 1.454185128211975 [Batch 43216/3750, Epoch 1/5]\n",
      "Accuracy of batch 43216/3750: 0.4375\n",
      "→ Loss: 1.4709570407867432 [Batch 43536/3750, Epoch 1/5]\n",
      "Accuracy of batch 43536/3750: 0.3125\n",
      "→ Loss: 1.3983750343322754 [Batch 43856/3750, Epoch 1/5]\n",
      "Accuracy of batch 43856/3750: 0.3125\n",
      "→ Loss: 1.4062987565994263 [Batch 44176/3750, Epoch 1/5]\n",
      "Accuracy of batch 44176/3750: 0.625\n",
      "→ Loss: 1.0444902181625366 [Batch 44496/3750, Epoch 1/5]\n",
      "Accuracy of batch 44496/3750: 0.8125\n",
      "→ Loss: 1.380300521850586 [Batch 44816/3750, Epoch 1/5]\n",
      "Accuracy of batch 44816/3750: 0.375\n",
      "→ Loss: 1.2591429948806763 [Batch 45136/3750, Epoch 1/5]\n",
      "Accuracy of batch 45136/3750: 0.4375\n",
      "→ Loss: 1.3972893953323364 [Batch 45456/3750, Epoch 1/5]\n",
      "Accuracy of batch 45456/3750: 0.3125\n",
      "→ Loss: 1.2782368659973145 [Batch 45776/3750, Epoch 1/5]\n",
      "Accuracy of batch 45776/3750: 0.375\n",
      "→ Loss: 1.1719155311584473 [Batch 46096/3750, Epoch 1/5]\n",
      "Accuracy of batch 46096/3750: 0.625\n",
      "→ Loss: 1.3262791633605957 [Batch 46416/3750, Epoch 1/5]\n",
      "Accuracy of batch 46416/3750: 0.5\n",
      "→ Loss: 1.1214354038238525 [Batch 46736/3750, Epoch 1/5]\n",
      "Accuracy of batch 46736/3750: 0.5\n",
      "→ Loss: 1.3531020879745483 [Batch 47056/3750, Epoch 1/5]\n",
      "Accuracy of batch 47056/3750: 0.5\n",
      "→ Loss: 1.2989212274551392 [Batch 47376/3750, Epoch 1/5]\n",
      "Accuracy of batch 47376/3750: 0.625\n",
      "→ Loss: 1.156597375869751 [Batch 47696/3750, Epoch 1/5]\n",
      "Accuracy of batch 47696/3750: 0.5625\n",
      "→ Loss: 1.1214274168014526 [Batch 48016/3750, Epoch 1/5]\n",
      "Accuracy of batch 48016/3750: 0.5\n",
      "→ Loss: 1.3979642391204834 [Batch 48336/3750, Epoch 1/5]\n",
      "Accuracy of batch 48336/3750: 0.3125\n",
      "→ Loss: 1.4480537176132202 [Batch 48656/3750, Epoch 1/5]\n",
      "Accuracy of batch 48656/3750: 0.4375\n",
      "→ Loss: 1.1904854774475098 [Batch 48976/3750, Epoch 1/5]\n",
      "Accuracy of batch 48976/3750: 0.5625\n",
      "→ Loss: 1.2400139570236206 [Batch 49296/3750, Epoch 1/5]\n",
      "Accuracy of batch 49296/3750: 0.5\n",
      "→ Loss: 1.2926753759384155 [Batch 49616/3750, Epoch 1/5]\n",
      "Accuracy of batch 49616/3750: 0.5625\n",
      "→ Loss: 1.1649973392486572 [Batch 49936/3750, Epoch 1/5]\n",
      "Accuracy of batch 49936/3750: 0.625\n",
      "→ Loss: 1.1542633771896362 [Batch 50256/3750, Epoch 1/5]\n",
      "Accuracy of batch 50256/3750: 0.625\n",
      "→ Loss: 1.4042197465896606 [Batch 50576/3750, Epoch 1/5]\n",
      "Accuracy of batch 50576/3750: 0.375\n",
      "→ Loss: 1.2033132314682007 [Batch 50896/3750, Epoch 1/5]\n",
      "Accuracy of batch 50896/3750: 0.5\n",
      "→ Loss: 1.0358390808105469 [Batch 51216/3750, Epoch 1/5]\n",
      "Accuracy of batch 51216/3750: 0.625\n",
      "→ Loss: 1.4970510005950928 [Batch 51536/3750, Epoch 1/5]\n",
      "Accuracy of batch 51536/3750: 0.375\n",
      "→ Loss: 1.4426655769348145 [Batch 51856/3750, Epoch 1/5]\n",
      "Accuracy of batch 51856/3750: 0.3125\n",
      "→ Loss: 1.238534688949585 [Batch 52176/3750, Epoch 1/5]\n",
      "Accuracy of batch 52176/3750: 0.5\n",
      "→ Loss: 1.2877506017684937 [Batch 52496/3750, Epoch 1/5]\n",
      "Accuracy of batch 52496/3750: 0.625\n",
      "→ Loss: 1.3112363815307617 [Batch 52816/3750, Epoch 1/5]\n",
      "Accuracy of batch 52816/3750: 0.3125\n",
      "→ Loss: 1.3495622873306274 [Batch 53136/3750, Epoch 1/5]\n",
      "Accuracy of batch 53136/3750: 0.375\n",
      "→ Loss: 1.1877342462539673 [Batch 53456/3750, Epoch 1/5]\n",
      "Accuracy of batch 53456/3750: 0.625\n",
      "→ Loss: 1.1846134662628174 [Batch 53776/3750, Epoch 1/5]\n",
      "Accuracy of batch 53776/3750: 0.6875\n",
      "→ Loss: 1.2465089559555054 [Batch 54096/3750, Epoch 1/5]\n",
      "Accuracy of batch 54096/3750: 0.25\n",
      "→ Loss: 1.378544807434082 [Batch 54416/3750, Epoch 1/5]\n",
      "Accuracy of batch 54416/3750: 0.5625\n",
      "→ Loss: 1.0952174663543701 [Batch 54736/3750, Epoch 1/5]\n",
      "Accuracy of batch 54736/3750: 0.5625\n",
      "→ Loss: 1.168615460395813 [Batch 55056/3750, Epoch 1/5]\n",
      "Accuracy of batch 55056/3750: 0.625\n",
      "→ Loss: 1.4004679918289185 [Batch 55376/3750, Epoch 1/5]\n",
      "Accuracy of batch 55376/3750: 0.375\n",
      "→ Loss: 1.131776213645935 [Batch 55696/3750, Epoch 1/5]\n",
      "Accuracy of batch 55696/3750: 0.6875\n",
      "→ Loss: 1.4252135753631592 [Batch 56016/3750, Epoch 1/5]\n",
      "Accuracy of batch 56016/3750: 0.375\n",
      "→ Loss: 1.1889506578445435 [Batch 56336/3750, Epoch 1/5]\n",
      "Accuracy of batch 56336/3750: 0.5\n",
      "→ Loss: 1.047486662864685 [Batch 56656/3750, Epoch 1/5]\n",
      "Accuracy of batch 56656/3750: 0.6875\n",
      "→ Loss: 1.4083601236343384 [Batch 56976/3750, Epoch 1/5]\n",
      "Accuracy of batch 56976/3750: 0.5\n",
      "→ Loss: 1.170138955116272 [Batch 57296/3750, Epoch 1/5]\n",
      "Accuracy of batch 57296/3750: 0.5625\n",
      "→ Loss: 1.274917721748352 [Batch 57616/3750, Epoch 1/5]\n",
      "Accuracy of batch 57616/3750: 0.5\n",
      "→ Loss: 1.206333875656128 [Batch 57936/3750, Epoch 1/5]\n",
      "Accuracy of batch 57936/3750: 0.5625\n",
      "→ Loss: 1.2060141563415527 [Batch 58256/3750, Epoch 1/5]\n",
      "Accuracy of batch 58256/3750: 0.625\n",
      "→ Loss: 1.1300009489059448 [Batch 58576/3750, Epoch 1/5]\n",
      "Accuracy of batch 58576/3750: 0.5625\n",
      "→ Loss: 1.2445728778839111 [Batch 58896/3750, Epoch 1/5]\n",
      "Accuracy of batch 58896/3750: 0.4375\n",
      "→ Loss: 1.1297191381454468 [Batch 59216/3750, Epoch 1/5]\n",
      "Accuracy of batch 59216/3750: 0.5625\n",
      "→ Loss: 1.216267466545105 [Batch 59536/3750, Epoch 1/5]\n",
      "Accuracy of batch 59536/3750: 0.625\n",
      "→ Loss: 1.1569404602050781 [Batch 59856/3750, Epoch 1/5]\n",
      "Accuracy of batch 59856/3750: 0.5625\n",
      "=== The epoch 1/5 has finished training ===\n",
      "→ Final accuracy of the epoch: 0.4424867033958435\n",
      "→ Loss: 1.2295811176300049 [Batch 16/3750, Epoch 2/5]\n",
      "Accuracy of batch 16/3750: 0.5\n",
      "→ Loss: 1.3786003589630127 [Batch 336/3750, Epoch 2/5]\n",
      "Accuracy of batch 336/3750: 0.4375\n",
      "→ Loss: 1.0557050704956055 [Batch 656/3750, Epoch 2/5]\n",
      "Accuracy of batch 656/3750: 0.75\n",
      "→ Loss: 1.3920819759368896 [Batch 976/3750, Epoch 2/5]\n",
      "Accuracy of batch 976/3750: 0.375\n",
      "→ Loss: 1.2620437145233154 [Batch 1296/3750, Epoch 2/5]\n",
      "Accuracy of batch 1296/3750: 0.5\n",
      "→ Loss: 1.2432719469070435 [Batch 1616/3750, Epoch 2/5]\n",
      "Accuracy of batch 1616/3750: 0.4375\n",
      "→ Loss: 1.4618797302246094 [Batch 1936/3750, Epoch 2/5]\n",
      "Accuracy of batch 1936/3750: 0.375\n",
      "→ Loss: 1.089615821838379 [Batch 2256/3750, Epoch 2/5]\n",
      "Accuracy of batch 2256/3750: 0.625\n",
      "→ Loss: 1.114540696144104 [Batch 2576/3750, Epoch 2/5]\n",
      "Accuracy of batch 2576/3750: 0.375\n",
      "→ Loss: 1.380026936531067 [Batch 2896/3750, Epoch 2/5]\n",
      "Accuracy of batch 2896/3750: 0.5\n",
      "→ Loss: 1.1631792783737183 [Batch 3216/3750, Epoch 2/5]\n",
      "Accuracy of batch 3216/3750: 0.3125\n",
      "→ Loss: 1.2046144008636475 [Batch 3536/3750, Epoch 2/5]\n",
      "Accuracy of batch 3536/3750: 0.4375\n",
      "→ Loss: 1.0218842029571533 [Batch 3856/3750, Epoch 2/5]\n",
      "Accuracy of batch 3856/3750: 0.375\n",
      "→ Loss: 1.054632544517517 [Batch 4176/3750, Epoch 2/5]\n",
      "Accuracy of batch 4176/3750: 0.4375\n",
      "→ Loss: 1.2164250612258911 [Batch 4496/3750, Epoch 2/5]\n",
      "Accuracy of batch 4496/3750: 0.4375\n",
      "→ Loss: 1.1941733360290527 [Batch 4816/3750, Epoch 2/5]\n",
      "Accuracy of batch 4816/3750: 0.5625\n",
      "→ Loss: 1.167850375175476 [Batch 5136/3750, Epoch 2/5]\n",
      "Accuracy of batch 5136/3750: 0.5\n",
      "→ Loss: 1.1994054317474365 [Batch 5456/3750, Epoch 2/5]\n",
      "Accuracy of batch 5456/3750: 0.5625\n",
      "→ Loss: 1.4005320072174072 [Batch 5776/3750, Epoch 2/5]\n",
      "Accuracy of batch 5776/3750: 0.25\n",
      "→ Loss: 1.2323569059371948 [Batch 6096/3750, Epoch 2/5]\n",
      "Accuracy of batch 6096/3750: 0.5625\n",
      "→ Loss: 1.1443462371826172 [Batch 6416/3750, Epoch 2/5]\n",
      "Accuracy of batch 6416/3750: 0.5625\n",
      "→ Loss: 1.2330507040023804 [Batch 6736/3750, Epoch 2/5]\n",
      "Accuracy of batch 6736/3750: 0.625\n",
      "→ Loss: 1.4181065559387207 [Batch 7056/3750, Epoch 2/5]\n",
      "Accuracy of batch 7056/3750: 0.3125\n",
      "→ Loss: 1.3348443508148193 [Batch 7376/3750, Epoch 2/5]\n",
      "Accuracy of batch 7376/3750: 0.5\n",
      "→ Loss: 0.9759730696678162 [Batch 7696/3750, Epoch 2/5]\n",
      "Accuracy of batch 7696/3750: 0.75\n",
      "→ Loss: 1.1639888286590576 [Batch 8016/3750, Epoch 2/5]\n",
      "Accuracy of batch 8016/3750: 0.375\n",
      "→ Loss: 1.4857028722763062 [Batch 8336/3750, Epoch 2/5]\n",
      "Accuracy of batch 8336/3750: 0.375\n",
      "→ Loss: 1.2143431901931763 [Batch 8656/3750, Epoch 2/5]\n",
      "Accuracy of batch 8656/3750: 0.5\n",
      "→ Loss: 1.3045880794525146 [Batch 8976/3750, Epoch 2/5]\n",
      "Accuracy of batch 8976/3750: 0.5\n",
      "→ Loss: 1.0690441131591797 [Batch 9296/3750, Epoch 2/5]\n",
      "Accuracy of batch 9296/3750: 0.75\n",
      "→ Loss: 1.3335541486740112 [Batch 9616/3750, Epoch 2/5]\n",
      "Accuracy of batch 9616/3750: 0.3125\n",
      "→ Loss: 1.012673258781433 [Batch 9936/3750, Epoch 2/5]\n",
      "Accuracy of batch 9936/3750: 0.6875\n",
      "→ Loss: 1.2183969020843506 [Batch 10256/3750, Epoch 2/5]\n",
      "Accuracy of batch 10256/3750: 0.5\n",
      "→ Loss: 1.2474136352539062 [Batch 10576/3750, Epoch 2/5]\n",
      "Accuracy of batch 10576/3750: 0.5\n",
      "→ Loss: 1.0761370658874512 [Batch 10896/3750, Epoch 2/5]\n",
      "Accuracy of batch 10896/3750: 0.5\n",
      "→ Loss: 1.0487385988235474 [Batch 11216/3750, Epoch 2/5]\n",
      "Accuracy of batch 11216/3750: 0.5\n",
      "→ Loss: 1.3694164752960205 [Batch 11536/3750, Epoch 2/5]\n",
      "Accuracy of batch 11536/3750: 0.4375\n",
      "→ Loss: 1.2318410873413086 [Batch 11856/3750, Epoch 2/5]\n",
      "Accuracy of batch 11856/3750: 0.5\n",
      "→ Loss: 1.0454190969467163 [Batch 12176/3750, Epoch 2/5]\n",
      "Accuracy of batch 12176/3750: 0.6875\n",
      "→ Loss: 1.2736973762512207 [Batch 12496/3750, Epoch 2/5]\n",
      "Accuracy of batch 12496/3750: 0.5625\n",
      "→ Loss: 1.0283761024475098 [Batch 12816/3750, Epoch 2/5]\n",
      "Accuracy of batch 12816/3750: 0.5625\n",
      "→ Loss: 1.28533136844635 [Batch 13136/3750, Epoch 2/5]\n",
      "Accuracy of batch 13136/3750: 0.5\n",
      "→ Loss: 1.2207759618759155 [Batch 13456/3750, Epoch 2/5]\n",
      "Accuracy of batch 13456/3750: 0.625\n",
      "→ Loss: 1.223254919052124 [Batch 13776/3750, Epoch 2/5]\n",
      "Accuracy of batch 13776/3750: 0.375\n",
      "→ Loss: 1.011314868927002 [Batch 14096/3750, Epoch 2/5]\n",
      "Accuracy of batch 14096/3750: 0.6875\n",
      "→ Loss: 1.0966546535491943 [Batch 14416/3750, Epoch 2/5]\n",
      "Accuracy of batch 14416/3750: 0.6875\n",
      "→ Loss: 1.1246826648712158 [Batch 14736/3750, Epoch 2/5]\n",
      "Accuracy of batch 14736/3750: 0.4375\n",
      "→ Loss: 1.0649917125701904 [Batch 15056/3750, Epoch 2/5]\n",
      "Accuracy of batch 15056/3750: 0.5\n",
      "→ Loss: 1.1680996417999268 [Batch 15376/3750, Epoch 2/5]\n",
      "Accuracy of batch 15376/3750: 0.625\n",
      "→ Loss: 1.0207701921463013 [Batch 15696/3750, Epoch 2/5]\n",
      "Accuracy of batch 15696/3750: 0.6875\n",
      "→ Loss: 1.26935613155365 [Batch 16016/3750, Epoch 2/5]\n",
      "Accuracy of batch 16016/3750: 0.3125\n",
      "→ Loss: 1.008415699005127 [Batch 16336/3750, Epoch 2/5]\n",
      "Accuracy of batch 16336/3750: 0.8125\n",
      "→ Loss: 1.1057569980621338 [Batch 16656/3750, Epoch 2/5]\n",
      "Accuracy of batch 16656/3750: 0.625\n",
      "→ Loss: 1.0751597881317139 [Batch 16976/3750, Epoch 2/5]\n",
      "Accuracy of batch 16976/3750: 0.5\n",
      "→ Loss: 1.0682395696640015 [Batch 17296/3750, Epoch 2/5]\n",
      "Accuracy of batch 17296/3750: 0.625\n",
      "→ Loss: 1.1956745386123657 [Batch 17616/3750, Epoch 2/5]\n",
      "Accuracy of batch 17616/3750: 0.4375\n",
      "→ Loss: 0.8637786507606506 [Batch 17936/3750, Epoch 2/5]\n",
      "Accuracy of batch 17936/3750: 0.625\n",
      "→ Loss: 1.4846019744873047 [Batch 18256/3750, Epoch 2/5]\n",
      "Accuracy of batch 18256/3750: 0.375\n",
      "→ Loss: 1.3721932172775269 [Batch 18576/3750, Epoch 2/5]\n",
      "Accuracy of batch 18576/3750: 0.5\n",
      "→ Loss: 1.185666561126709 [Batch 18896/3750, Epoch 2/5]\n",
      "Accuracy of batch 18896/3750: 0.625\n",
      "→ Loss: 0.809586226940155 [Batch 19216/3750, Epoch 2/5]\n",
      "Accuracy of batch 19216/3750: 0.5625\n",
      "→ Loss: 1.0134947299957275 [Batch 19536/3750, Epoch 2/5]\n",
      "Accuracy of batch 19536/3750: 0.625\n",
      "→ Loss: 1.1598103046417236 [Batch 19856/3750, Epoch 2/5]\n",
      "Accuracy of batch 19856/3750: 0.5625\n",
      "→ Loss: 1.033653974533081 [Batch 20176/3750, Epoch 2/5]\n",
      "Accuracy of batch 20176/3750: 0.6875\n",
      "→ Loss: 1.065055012702942 [Batch 20496/3750, Epoch 2/5]\n",
      "Accuracy of batch 20496/3750: 0.625\n",
      "→ Loss: 1.3264106512069702 [Batch 20816/3750, Epoch 2/5]\n",
      "Accuracy of batch 20816/3750: 0.5625\n",
      "→ Loss: 0.9129130244255066 [Batch 21136/3750, Epoch 2/5]\n",
      "Accuracy of batch 21136/3750: 0.75\n",
      "→ Loss: 1.183706283569336 [Batch 21456/3750, Epoch 2/5]\n",
      "Accuracy of batch 21456/3750: 0.625\n",
      "→ Loss: 1.2962383031845093 [Batch 21776/3750, Epoch 2/5]\n",
      "Accuracy of batch 21776/3750: 0.5\n",
      "→ Loss: 1.2350353002548218 [Batch 22096/3750, Epoch 2/5]\n",
      "Accuracy of batch 22096/3750: 0.3125\n",
      "→ Loss: 1.0847665071487427 [Batch 22416/3750, Epoch 2/5]\n",
      "Accuracy of batch 22416/3750: 0.625\n",
      "→ Loss: 0.8542084693908691 [Batch 22736/3750, Epoch 2/5]\n",
      "Accuracy of batch 22736/3750: 0.75\n",
      "→ Loss: 1.3142577409744263 [Batch 23056/3750, Epoch 2/5]\n",
      "Accuracy of batch 23056/3750: 0.5\n",
      "→ Loss: 1.0643975734710693 [Batch 23376/3750, Epoch 2/5]\n",
      "Accuracy of batch 23376/3750: 0.5625\n",
      "→ Loss: 1.1203972101211548 [Batch 23696/3750, Epoch 2/5]\n",
      "Accuracy of batch 23696/3750: 0.5\n",
      "→ Loss: 1.2537652254104614 [Batch 24016/3750, Epoch 2/5]\n",
      "Accuracy of batch 24016/3750: 0.375\n",
      "→ Loss: 1.2141700983047485 [Batch 24336/3750, Epoch 2/5]\n",
      "Accuracy of batch 24336/3750: 0.6875\n",
      "→ Loss: 1.1369576454162598 [Batch 24656/3750, Epoch 2/5]\n",
      "Accuracy of batch 24656/3750: 0.625\n",
      "→ Loss: 1.2462444305419922 [Batch 24976/3750, Epoch 2/5]\n",
      "Accuracy of batch 24976/3750: 0.6875\n",
      "→ Loss: 1.2972456216812134 [Batch 25296/3750, Epoch 2/5]\n",
      "Accuracy of batch 25296/3750: 0.5\n",
      "→ Loss: 1.1866564750671387 [Batch 25616/3750, Epoch 2/5]\n",
      "Accuracy of batch 25616/3750: 0.625\n",
      "→ Loss: 1.110751986503601 [Batch 25936/3750, Epoch 2/5]\n",
      "Accuracy of batch 25936/3750: 0.5625\n",
      "→ Loss: 1.2404009103775024 [Batch 26256/3750, Epoch 2/5]\n",
      "Accuracy of batch 26256/3750: 0.5\n",
      "→ Loss: 1.0316039323806763 [Batch 26576/3750, Epoch 2/5]\n",
      "Accuracy of batch 26576/3750: 0.5\n",
      "→ Loss: 1.0506001710891724 [Batch 26896/3750, Epoch 2/5]\n",
      "Accuracy of batch 26896/3750: 0.625\n",
      "→ Loss: 1.011637568473816 [Batch 27216/3750, Epoch 2/5]\n",
      "Accuracy of batch 27216/3750: 0.5625\n",
      "→ Loss: 0.7545025944709778 [Batch 27536/3750, Epoch 2/5]\n",
      "Accuracy of batch 27536/3750: 0.9375\n",
      "→ Loss: 1.161589503288269 [Batch 27856/3750, Epoch 2/5]\n",
      "Accuracy of batch 27856/3750: 0.5\n",
      "→ Loss: 1.3064569234848022 [Batch 28176/3750, Epoch 2/5]\n",
      "Accuracy of batch 28176/3750: 0.4375\n",
      "→ Loss: 1.2585327625274658 [Batch 28496/3750, Epoch 2/5]\n",
      "Accuracy of batch 28496/3750: 0.5625\n",
      "→ Loss: 1.1776046752929688 [Batch 28816/3750, Epoch 2/5]\n",
      "Accuracy of batch 28816/3750: 0.4375\n",
      "→ Loss: 1.2012748718261719 [Batch 29136/3750, Epoch 2/5]\n",
      "Accuracy of batch 29136/3750: 0.375\n",
      "→ Loss: 1.2736247777938843 [Batch 29456/3750, Epoch 2/5]\n",
      "Accuracy of batch 29456/3750: 0.375\n",
      "→ Loss: 1.0777194499969482 [Batch 29776/3750, Epoch 2/5]\n",
      "Accuracy of batch 29776/3750: 0.75\n",
      "→ Loss: 1.3307225704193115 [Batch 30096/3750, Epoch 2/5]\n",
      "Accuracy of batch 30096/3750: 0.5\n",
      "→ Loss: 1.030686616897583 [Batch 30416/3750, Epoch 2/5]\n",
      "Accuracy of batch 30416/3750: 0.5\n",
      "→ Loss: 0.9868922829627991 [Batch 30736/3750, Epoch 2/5]\n",
      "Accuracy of batch 30736/3750: 0.6875\n",
      "→ Loss: 0.9209304451942444 [Batch 31056/3750, Epoch 2/5]\n",
      "Accuracy of batch 31056/3750: 0.6875\n",
      "→ Loss: 1.1476017236709595 [Batch 31376/3750, Epoch 2/5]\n",
      "Accuracy of batch 31376/3750: 0.5\n",
      "→ Loss: 1.0641812086105347 [Batch 31696/3750, Epoch 2/5]\n",
      "Accuracy of batch 31696/3750: 0.5625\n",
      "→ Loss: 1.1016103029251099 [Batch 32016/3750, Epoch 2/5]\n",
      "Accuracy of batch 32016/3750: 0.625\n",
      "→ Loss: 1.0484049320220947 [Batch 32336/3750, Epoch 2/5]\n",
      "Accuracy of batch 32336/3750: 0.5\n",
      "→ Loss: 1.1156409978866577 [Batch 32656/3750, Epoch 2/5]\n",
      "Accuracy of batch 32656/3750: 0.6875\n",
      "→ Loss: 0.9558268785476685 [Batch 32976/3750, Epoch 2/5]\n",
      "Accuracy of batch 32976/3750: 0.6875\n",
      "→ Loss: 1.0240306854248047 [Batch 33296/3750, Epoch 2/5]\n",
      "Accuracy of batch 33296/3750: 0.8125\n",
      "→ Loss: 0.8366330862045288 [Batch 33616/3750, Epoch 2/5]\n",
      "Accuracy of batch 33616/3750: 0.6875\n",
      "→ Loss: 1.0518521070480347 [Batch 33936/3750, Epoch 2/5]\n",
      "Accuracy of batch 33936/3750: 0.625\n",
      "→ Loss: 0.9042962789535522 [Batch 34256/3750, Epoch 2/5]\n",
      "Accuracy of batch 34256/3750: 0.75\n",
      "→ Loss: 0.8778599500656128 [Batch 34576/3750, Epoch 2/5]\n",
      "Accuracy of batch 34576/3750: 0.8125\n",
      "→ Loss: 1.1463227272033691 [Batch 34896/3750, Epoch 2/5]\n",
      "Accuracy of batch 34896/3750: 0.625\n",
      "→ Loss: 1.033767580986023 [Batch 35216/3750, Epoch 2/5]\n",
      "Accuracy of batch 35216/3750: 0.8125\n",
      "→ Loss: 1.190429449081421 [Batch 35536/3750, Epoch 2/5]\n",
      "Accuracy of batch 35536/3750: 0.625\n",
      "→ Loss: 1.094313383102417 [Batch 35856/3750, Epoch 2/5]\n",
      "Accuracy of batch 35856/3750: 0.625\n",
      "→ Loss: 1.0501466989517212 [Batch 36176/3750, Epoch 2/5]\n",
      "Accuracy of batch 36176/3750: 0.625\n",
      "→ Loss: 0.9754923582077026 [Batch 36496/3750, Epoch 2/5]\n",
      "Accuracy of batch 36496/3750: 0.625\n",
      "→ Loss: 1.0411477088928223 [Batch 36816/3750, Epoch 2/5]\n",
      "Accuracy of batch 36816/3750: 0.625\n",
      "→ Loss: 0.9623223543167114 [Batch 37136/3750, Epoch 2/5]\n",
      "Accuracy of batch 37136/3750: 0.5\n",
      "→ Loss: 1.0626838207244873 [Batch 37456/3750, Epoch 2/5]\n",
      "Accuracy of batch 37456/3750: 0.5\n",
      "→ Loss: 1.1748881340026855 [Batch 37776/3750, Epoch 2/5]\n",
      "Accuracy of batch 37776/3750: 0.6875\n",
      "→ Loss: 1.02254319190979 [Batch 38096/3750, Epoch 2/5]\n",
      "Accuracy of batch 38096/3750: 0.5625\n",
      "→ Loss: 1.151576042175293 [Batch 38416/3750, Epoch 2/5]\n",
      "Accuracy of batch 38416/3750: 0.4375\n",
      "→ Loss: 1.0301384925842285 [Batch 38736/3750, Epoch 2/5]\n",
      "Accuracy of batch 38736/3750: 0.5\n",
      "→ Loss: 0.9853770732879639 [Batch 39056/3750, Epoch 2/5]\n",
      "Accuracy of batch 39056/3750: 0.8125\n",
      "→ Loss: 0.8991523385047913 [Batch 39376/3750, Epoch 2/5]\n",
      "Accuracy of batch 39376/3750: 0.625\n",
      "→ Loss: 0.9778355956077576 [Batch 39696/3750, Epoch 2/5]\n",
      "Accuracy of batch 39696/3750: 0.5\n",
      "→ Loss: 1.020628571510315 [Batch 40016/3750, Epoch 2/5]\n",
      "Accuracy of batch 40016/3750: 0.8125\n",
      "→ Loss: 1.0112247467041016 [Batch 40336/3750, Epoch 2/5]\n",
      "Accuracy of batch 40336/3750: 0.75\n",
      "→ Loss: 0.867946982383728 [Batch 40656/3750, Epoch 2/5]\n",
      "Accuracy of batch 40656/3750: 0.6875\n",
      "→ Loss: 0.9444993734359741 [Batch 40976/3750, Epoch 2/5]\n",
      "Accuracy of batch 40976/3750: 0.6875\n",
      "→ Loss: 1.1053187847137451 [Batch 41296/3750, Epoch 2/5]\n",
      "Accuracy of batch 41296/3750: 0.5625\n",
      "→ Loss: 0.9782917499542236 [Batch 41616/3750, Epoch 2/5]\n",
      "Accuracy of batch 41616/3750: 0.8125\n",
      "→ Loss: 1.019710898399353 [Batch 41936/3750, Epoch 2/5]\n",
      "Accuracy of batch 41936/3750: 0.625\n",
      "→ Loss: 1.0476436614990234 [Batch 42256/3750, Epoch 2/5]\n",
      "Accuracy of batch 42256/3750: 0.8125\n",
      "→ Loss: 0.5941953659057617 [Batch 42576/3750, Epoch 2/5]\n",
      "Accuracy of batch 42576/3750: 0.875\n",
      "→ Loss: 0.9858022332191467 [Batch 42896/3750, Epoch 2/5]\n",
      "Accuracy of batch 42896/3750: 0.6875\n",
      "→ Loss: 0.9703292846679688 [Batch 43216/3750, Epoch 2/5]\n",
      "Accuracy of batch 43216/3750: 0.6875\n",
      "→ Loss: 1.0411152839660645 [Batch 43536/3750, Epoch 2/5]\n",
      "Accuracy of batch 43536/3750: 0.5\n",
      "→ Loss: 0.9492549896240234 [Batch 43856/3750, Epoch 2/5]\n",
      "Accuracy of batch 43856/3750: 0.5625\n",
      "→ Loss: 0.9516347646713257 [Batch 44176/3750, Epoch 2/5]\n",
      "Accuracy of batch 44176/3750: 0.6875\n",
      "→ Loss: 0.676672637462616 [Batch 44496/3750, Epoch 2/5]\n",
      "Accuracy of batch 44496/3750: 0.9375\n",
      "→ Loss: 0.9213314652442932 [Batch 44816/3750, Epoch 2/5]\n",
      "Accuracy of batch 44816/3750: 0.5625\n",
      "→ Loss: 0.9651082158088684 [Batch 45136/3750, Epoch 2/5]\n",
      "Accuracy of batch 45136/3750: 0.6875\n",
      "→ Loss: 0.9654291868209839 [Batch 45456/3750, Epoch 2/5]\n",
      "Accuracy of batch 45456/3750: 0.4375\n",
      "→ Loss: 0.843002200126648 [Batch 45776/3750, Epoch 2/5]\n",
      "Accuracy of batch 45776/3750: 0.75\n",
      "→ Loss: 0.8472042083740234 [Batch 46096/3750, Epoch 2/5]\n",
      "Accuracy of batch 46096/3750: 0.6875\n",
      "→ Loss: 1.0476516485214233 [Batch 46416/3750, Epoch 2/5]\n",
      "Accuracy of batch 46416/3750: 0.75\n",
      "→ Loss: 0.8463950157165527 [Batch 46736/3750, Epoch 2/5]\n",
      "Accuracy of batch 46736/3750: 0.6875\n",
      "→ Loss: 0.982932448387146 [Batch 47056/3750, Epoch 2/5]\n",
      "Accuracy of batch 47056/3750: 0.6875\n",
      "→ Loss: 1.0128729343414307 [Batch 47376/3750, Epoch 2/5]\n",
      "Accuracy of batch 47376/3750: 0.75\n",
      "→ Loss: 0.8570894002914429 [Batch 47696/3750, Epoch 2/5]\n",
      "Accuracy of batch 47696/3750: 0.75\n",
      "→ Loss: 0.8456472754478455 [Batch 48016/3750, Epoch 2/5]\n",
      "Accuracy of batch 48016/3750: 0.6875\n",
      "→ Loss: 1.0307320356369019 [Batch 48336/3750, Epoch 2/5]\n",
      "Accuracy of batch 48336/3750: 0.4375\n",
      "→ Loss: 1.1962345838546753 [Batch 48656/3750, Epoch 2/5]\n",
      "Accuracy of batch 48656/3750: 0.6875\n",
      "→ Loss: 0.8314915299415588 [Batch 48976/3750, Epoch 2/5]\n",
      "Accuracy of batch 48976/3750: 0.75\n",
      "→ Loss: 0.9153659343719482 [Batch 49296/3750, Epoch 2/5]\n",
      "Accuracy of batch 49296/3750: 0.75\n",
      "→ Loss: 0.9169344902038574 [Batch 49616/3750, Epoch 2/5]\n",
      "Accuracy of batch 49616/3750: 0.625\n",
      "→ Loss: 0.8609729409217834 [Batch 49936/3750, Epoch 2/5]\n",
      "Accuracy of batch 49936/3750: 0.6875\n",
      "→ Loss: 0.8388092517852783 [Batch 50256/3750, Epoch 2/5]\n",
      "Accuracy of batch 50256/3750: 0.8125\n",
      "→ Loss: 1.020381212234497 [Batch 50576/3750, Epoch 2/5]\n",
      "Accuracy of batch 50576/3750: 0.625\n",
      "→ Loss: 0.7786129713058472 [Batch 50896/3750, Epoch 2/5]\n",
      "Accuracy of batch 50896/3750: 0.75\n",
      "→ Loss: 0.6624652147293091 [Batch 51216/3750, Epoch 2/5]\n",
      "Accuracy of batch 51216/3750: 0.75\n",
      "→ Loss: 1.050298810005188 [Batch 51536/3750, Epoch 2/5]\n",
      "Accuracy of batch 51536/3750: 0.625\n",
      "→ Loss: 1.1787515878677368 [Batch 51856/3750, Epoch 2/5]\n",
      "Accuracy of batch 51856/3750: 0.625\n",
      "→ Loss: 0.8862608671188354 [Batch 52176/3750, Epoch 2/5]\n",
      "Accuracy of batch 52176/3750: 0.625\n",
      "→ Loss: 1.0383739471435547 [Batch 52496/3750, Epoch 2/5]\n",
      "Accuracy of batch 52496/3750: 0.5\n",
      "→ Loss: 0.9889611005783081 [Batch 52816/3750, Epoch 2/5]\n",
      "Accuracy of batch 52816/3750: 0.5625\n",
      "→ Loss: 1.0599336624145508 [Batch 53136/3750, Epoch 2/5]\n",
      "Accuracy of batch 53136/3750: 0.625\n",
      "→ Loss: 0.9311658143997192 [Batch 53456/3750, Epoch 2/5]\n",
      "Accuracy of batch 53456/3750: 0.6875\n",
      "→ Loss: 0.8923625946044922 [Batch 53776/3750, Epoch 2/5]\n",
      "Accuracy of batch 53776/3750: 0.625\n",
      "→ Loss: 1.1454989910125732 [Batch 54096/3750, Epoch 2/5]\n",
      "Accuracy of batch 54096/3750: 0.5625\n",
      "→ Loss: 1.065172553062439 [Batch 54416/3750, Epoch 2/5]\n",
      "Accuracy of batch 54416/3750: 0.6875\n",
      "→ Loss: 0.6594052314758301 [Batch 54736/3750, Epoch 2/5]\n",
      "Accuracy of batch 54736/3750: 0.875\n",
      "→ Loss: 0.7774656414985657 [Batch 55056/3750, Epoch 2/5]\n",
      "Accuracy of batch 55056/3750: 0.6875\n",
      "→ Loss: 1.0356285572052002 [Batch 55376/3750, Epoch 2/5]\n",
      "Accuracy of batch 55376/3750: 0.5625\n",
      "→ Loss: 0.9671730995178223 [Batch 55696/3750, Epoch 2/5]\n",
      "Accuracy of batch 55696/3750: 0.5625\n",
      "→ Loss: 1.1685593128204346 [Batch 56016/3750, Epoch 2/5]\n",
      "Accuracy of batch 56016/3750: 0.625\n",
      "→ Loss: 0.8355922698974609 [Batch 56336/3750, Epoch 2/5]\n",
      "Accuracy of batch 56336/3750: 0.75\n",
      "→ Loss: 0.7424646019935608 [Batch 56656/3750, Epoch 2/5]\n",
      "Accuracy of batch 56656/3750: 0.875\n",
      "→ Loss: 1.1770637035369873 [Batch 56976/3750, Epoch 2/5]\n",
      "Accuracy of batch 56976/3750: 0.5\n",
      "→ Loss: 0.8753994107246399 [Batch 57296/3750, Epoch 2/5]\n",
      "Accuracy of batch 57296/3750: 0.75\n",
      "→ Loss: 1.01419997215271 [Batch 57616/3750, Epoch 2/5]\n",
      "Accuracy of batch 57616/3750: 0.5625\n",
      "→ Loss: 0.8774693608283997 [Batch 57936/3750, Epoch 2/5]\n",
      "Accuracy of batch 57936/3750: 0.625\n",
      "→ Loss: 0.8728010654449463 [Batch 58256/3750, Epoch 2/5]\n",
      "Accuracy of batch 58256/3750: 0.6875\n",
      "→ Loss: 0.9236530065536499 [Batch 58576/3750, Epoch 2/5]\n",
      "Accuracy of batch 58576/3750: 0.6875\n",
      "→ Loss: 1.0325366258621216 [Batch 58896/3750, Epoch 2/5]\n",
      "Accuracy of batch 58896/3750: 0.5\n",
      "→ Loss: 0.7780513763427734 [Batch 59216/3750, Epoch 2/5]\n",
      "Accuracy of batch 59216/3750: 0.6875\n",
      "→ Loss: 0.9132576584815979 [Batch 59536/3750, Epoch 2/5]\n",
      "Accuracy of batch 59536/3750: 0.75\n",
      "→ Loss: 0.8079198598861694 [Batch 59856/3750, Epoch 2/5]\n",
      "Accuracy of batch 59856/3750: 0.625\n",
      "=== The epoch 2/5 has finished training ===\n",
      "→ Final accuracy of the epoch: 0.5957446694374084\n",
      "→ Loss: 0.9305124282836914 [Batch 16/3750, Epoch 3/5]\n",
      "Accuracy of batch 16/3750: 0.5625\n",
      "→ Loss: 0.8343090415000916 [Batch 336/3750, Epoch 3/5]\n",
      "Accuracy of batch 336/3750: 0.875\n",
      "→ Loss: 0.682962954044342 [Batch 656/3750, Epoch 3/5]\n",
      "Accuracy of batch 656/3750: 0.9375\n",
      "→ Loss: 1.2221869230270386 [Batch 976/3750, Epoch 3/5]\n",
      "Accuracy of batch 976/3750: 0.4375\n",
      "→ Loss: 0.9394399523735046 [Batch 1296/3750, Epoch 3/5]\n",
      "Accuracy of batch 1296/3750: 0.5625\n",
      "→ Loss: 0.9936227798461914 [Batch 1616/3750, Epoch 3/5]\n",
      "Accuracy of batch 1616/3750: 0.6875\n",
      "→ Loss: 1.1297012567520142 [Batch 1936/3750, Epoch 3/5]\n",
      "Accuracy of batch 1936/3750: 0.625\n",
      "→ Loss: 0.7464202642440796 [Batch 2256/3750, Epoch 3/5]\n",
      "Accuracy of batch 2256/3750: 0.6875\n",
      "→ Loss: 0.8408710360527039 [Batch 2576/3750, Epoch 3/5]\n",
      "Accuracy of batch 2576/3750: 0.5625\n",
      "→ Loss: 1.2160236835479736 [Batch 2896/3750, Epoch 3/5]\n",
      "Accuracy of batch 2896/3750: 0.625\n",
      "→ Loss: 0.8443014621734619 [Batch 3216/3750, Epoch 3/5]\n",
      "Accuracy of batch 3216/3750: 0.625\n",
      "→ Loss: 0.9018160700798035 [Batch 3536/3750, Epoch 3/5]\n",
      "Accuracy of batch 3536/3750: 0.6875\n",
      "→ Loss: 0.8101218938827515 [Batch 3856/3750, Epoch 3/5]\n",
      "Accuracy of batch 3856/3750: 0.6875\n",
      "→ Loss: 0.8433948159217834 [Batch 4176/3750, Epoch 3/5]\n",
      "Accuracy of batch 4176/3750: 0.5625\n",
      "→ Loss: 0.8950380086898804 [Batch 4496/3750, Epoch 3/5]\n",
      "Accuracy of batch 4496/3750: 0.8125\n",
      "→ Loss: 0.8612678050994873 [Batch 4816/3750, Epoch 3/5]\n",
      "Accuracy of batch 4816/3750: 0.75\n",
      "→ Loss: 1.051363468170166 [Batch 5136/3750, Epoch 3/5]\n",
      "Accuracy of batch 5136/3750: 0.5625\n",
      "→ Loss: 0.9161422252655029 [Batch 5456/3750, Epoch 3/5]\n",
      "Accuracy of batch 5456/3750: 0.6875\n",
      "→ Loss: 1.0169775485992432 [Batch 5776/3750, Epoch 3/5]\n",
      "Accuracy of batch 5776/3750: 0.5\n",
      "→ Loss: 1.0139994621276855 [Batch 6096/3750, Epoch 3/5]\n",
      "Accuracy of batch 6096/3750: 0.5625\n",
      "→ Loss: 0.8565140962600708 [Batch 6416/3750, Epoch 3/5]\n",
      "Accuracy of batch 6416/3750: 0.6875\n",
      "→ Loss: 0.8315619230270386 [Batch 6736/3750, Epoch 3/5]\n",
      "Accuracy of batch 6736/3750: 0.8125\n",
      "→ Loss: 1.17536199092865 [Batch 7056/3750, Epoch 3/5]\n",
      "Accuracy of batch 7056/3750: 0.5\n",
      "→ Loss: 1.0965569019317627 [Batch 7376/3750, Epoch 3/5]\n",
      "Accuracy of batch 7376/3750: 0.625\n",
      "→ Loss: 0.8048575520515442 [Batch 7696/3750, Epoch 3/5]\n",
      "Accuracy of batch 7696/3750: 0.8125\n",
      "→ Loss: 0.9640772342681885 [Batch 8016/3750, Epoch 3/5]\n",
      "Accuracy of batch 8016/3750: 0.5625\n",
      "→ Loss: 1.3019362688064575 [Batch 8336/3750, Epoch 3/5]\n",
      "Accuracy of batch 8336/3750: 0.375\n",
      "→ Loss: 0.9173351526260376 [Batch 8656/3750, Epoch 3/5]\n",
      "Accuracy of batch 8656/3750: 0.625\n",
      "→ Loss: 1.2411630153656006 [Batch 8976/3750, Epoch 3/5]\n",
      "Accuracy of batch 8976/3750: 0.5\n",
      "→ Loss: 0.7432265281677246 [Batch 9296/3750, Epoch 3/5]\n",
      "Accuracy of batch 9296/3750: 0.75\n",
      "→ Loss: 1.120383620262146 [Batch 9616/3750, Epoch 3/5]\n",
      "Accuracy of batch 9616/3750: 0.375\n",
      "→ Loss: 0.7555294036865234 [Batch 9936/3750, Epoch 3/5]\n",
      "Accuracy of batch 9936/3750: 0.875\n",
      "→ Loss: 1.0109269618988037 [Batch 10256/3750, Epoch 3/5]\n",
      "Accuracy of batch 10256/3750: 0.625\n",
      "→ Loss: 0.7965985536575317 [Batch 10576/3750, Epoch 3/5]\n",
      "Accuracy of batch 10576/3750: 0.8125\n",
      "→ Loss: 0.8573816418647766 [Batch 10896/3750, Epoch 3/5]\n",
      "Accuracy of batch 10896/3750: 0.625\n",
      "→ Loss: 0.8805782198905945 [Batch 11216/3750, Epoch 3/5]\n",
      "Accuracy of batch 11216/3750: 0.4375\n",
      "→ Loss: 1.1517970561981201 [Batch 11536/3750, Epoch 3/5]\n",
      "Accuracy of batch 11536/3750: 0.5625\n",
      "→ Loss: 0.9593286514282227 [Batch 11856/3750, Epoch 3/5]\n",
      "Accuracy of batch 11856/3750: 0.625\n",
      "→ Loss: 0.8034109473228455 [Batch 12176/3750, Epoch 3/5]\n",
      "Accuracy of batch 12176/3750: 0.75\n",
      "→ Loss: 0.9811879396438599 [Batch 12496/3750, Epoch 3/5]\n",
      "Accuracy of batch 12496/3750: 0.5625\n",
      "→ Loss: 0.8571112155914307 [Batch 12816/3750, Epoch 3/5]\n",
      "Accuracy of batch 12816/3750: 0.5625\n",
      "→ Loss: 0.9382187724113464 [Batch 13136/3750, Epoch 3/5]\n",
      "Accuracy of batch 13136/3750: 0.6875\n",
      "→ Loss: 0.9870957136154175 [Batch 13456/3750, Epoch 3/5]\n",
      "Accuracy of batch 13456/3750: 0.6875\n",
      "→ Loss: 1.0365500450134277 [Batch 13776/3750, Epoch 3/5]\n",
      "Accuracy of batch 13776/3750: 0.375\n",
      "→ Loss: 0.7621557712554932 [Batch 14096/3750, Epoch 3/5]\n",
      "Accuracy of batch 14096/3750: 0.6875\n",
      "→ Loss: 0.7483802437782288 [Batch 14416/3750, Epoch 3/5]\n",
      "Accuracy of batch 14416/3750: 0.75\n",
      "→ Loss: 0.8637261986732483 [Batch 14736/3750, Epoch 3/5]\n",
      "Accuracy of batch 14736/3750: 0.625\n",
      "→ Loss: 0.729392945766449 [Batch 15056/3750, Epoch 3/5]\n",
      "Accuracy of batch 15056/3750: 0.75\n",
      "→ Loss: 0.9505519866943359 [Batch 15376/3750, Epoch 3/5]\n",
      "Accuracy of batch 15376/3750: 0.6875\n",
      "→ Loss: 0.762285053730011 [Batch 15696/3750, Epoch 3/5]\n",
      "Accuracy of batch 15696/3750: 0.875\n",
      "→ Loss: 0.9277893900871277 [Batch 16016/3750, Epoch 3/5]\n",
      "Accuracy of batch 16016/3750: 0.75\n",
      "→ Loss: 0.7149376273155212 [Batch 16336/3750, Epoch 3/5]\n",
      "Accuracy of batch 16336/3750: 0.8125\n",
      "→ Loss: 0.8748964071273804 [Batch 16656/3750, Epoch 3/5]\n",
      "Accuracy of batch 16656/3750: 0.875\n",
      "→ Loss: 0.9545889496803284 [Batch 16976/3750, Epoch 3/5]\n",
      "Accuracy of batch 16976/3750: 0.5625\n",
      "→ Loss: 0.7308945655822754 [Batch 17296/3750, Epoch 3/5]\n",
      "Accuracy of batch 17296/3750: 0.8125\n",
      "→ Loss: 0.9153794646263123 [Batch 17616/3750, Epoch 3/5]\n",
      "Accuracy of batch 17616/3750: 0.5625\n",
      "→ Loss: 0.6360726952552795 [Batch 17936/3750, Epoch 3/5]\n",
      "Accuracy of batch 17936/3750: 0.6875\n",
      "→ Loss: 1.444221019744873 [Batch 18256/3750, Epoch 3/5]\n",
      "Accuracy of batch 18256/3750: 0.5\n",
      "→ Loss: 1.1718157529830933 [Batch 18576/3750, Epoch 3/5]\n",
      "Accuracy of batch 18576/3750: 0.5625\n",
      "→ Loss: 0.934444010257721 [Batch 18896/3750, Epoch 3/5]\n",
      "Accuracy of batch 18896/3750: 0.6875\n",
      "→ Loss: 0.7028340101242065 [Batch 19216/3750, Epoch 3/5]\n",
      "Accuracy of batch 19216/3750: 0.6875\n",
      "→ Loss: 0.730512797832489 [Batch 19536/3750, Epoch 3/5]\n",
      "Accuracy of batch 19536/3750: 0.75\n",
      "→ Loss: 1.0517516136169434 [Batch 19856/3750, Epoch 3/5]\n",
      "Accuracy of batch 19856/3750: 0.6875\n",
      "→ Loss: 0.8318621516227722 [Batch 20176/3750, Epoch 3/5]\n",
      "Accuracy of batch 20176/3750: 0.75\n",
      "→ Loss: 0.7763652801513672 [Batch 20496/3750, Epoch 3/5]\n",
      "Accuracy of batch 20496/3750: 0.5625\n",
      "→ Loss: 1.2388478517532349 [Batch 20816/3750, Epoch 3/5]\n",
      "Accuracy of batch 20816/3750: 0.5625\n",
      "→ Loss: 0.6448594331741333 [Batch 21136/3750, Epoch 3/5]\n",
      "Accuracy of batch 21136/3750: 0.8125\n",
      "→ Loss: 1.039982557296753 [Batch 21456/3750, Epoch 3/5]\n",
      "Accuracy of batch 21456/3750: 0.4375\n",
      "→ Loss: 1.0889455080032349 [Batch 21776/3750, Epoch 3/5]\n",
      "Accuracy of batch 21776/3750: 0.5625\n",
      "→ Loss: 1.0181498527526855 [Batch 22096/3750, Epoch 3/5]\n",
      "Accuracy of batch 22096/3750: 0.75\n",
      "→ Loss: 0.8123108744621277 [Batch 22416/3750, Epoch 3/5]\n",
      "Accuracy of batch 22416/3750: 0.625\n",
      "→ Loss: 0.5811183452606201 [Batch 22736/3750, Epoch 3/5]\n",
      "Accuracy of batch 22736/3750: 0.8125\n",
      "→ Loss: 1.0750375986099243 [Batch 23056/3750, Epoch 3/5]\n",
      "Accuracy of batch 23056/3750: 0.5625\n",
      "→ Loss: 0.7614258527755737 [Batch 23376/3750, Epoch 3/5]\n",
      "Accuracy of batch 23376/3750: 0.6875\n",
      "→ Loss: 1.0077041387557983 [Batch 23696/3750, Epoch 3/5]\n",
      "Accuracy of batch 23696/3750: 0.625\n",
      "→ Loss: 1.0624967813491821 [Batch 24016/3750, Epoch 3/5]\n",
      "Accuracy of batch 24016/3750: 0.5\n",
      "→ Loss: 0.8923755288124084 [Batch 24336/3750, Epoch 3/5]\n",
      "Accuracy of batch 24336/3750: 0.5625\n",
      "→ Loss: 0.7924917936325073 [Batch 24656/3750, Epoch 3/5]\n",
      "Accuracy of batch 24656/3750: 0.6875\n",
      "→ Loss: 1.1156001091003418 [Batch 24976/3750, Epoch 3/5]\n",
      "Accuracy of batch 24976/3750: 0.5625\n",
      "→ Loss: 0.9846077561378479 [Batch 25296/3750, Epoch 3/5]\n",
      "Accuracy of batch 25296/3750: 0.6875\n",
      "→ Loss: 0.9545212388038635 [Batch 25616/3750, Epoch 3/5]\n",
      "Accuracy of batch 25616/3750: 0.6875\n",
      "→ Loss: 0.8351119756698608 [Batch 25936/3750, Epoch 3/5]\n",
      "Accuracy of batch 25936/3750: 0.75\n",
      "→ Loss: 1.1948184967041016 [Batch 26256/3750, Epoch 3/5]\n",
      "Accuracy of batch 26256/3750: 0.5\n",
      "→ Loss: 0.8266355395317078 [Batch 26576/3750, Epoch 3/5]\n",
      "Accuracy of batch 26576/3750: 0.625\n",
      "→ Loss: 0.8455665111541748 [Batch 26896/3750, Epoch 3/5]\n",
      "Accuracy of batch 26896/3750: 0.75\n",
      "→ Loss: 0.7743476629257202 [Batch 27216/3750, Epoch 3/5]\n",
      "Accuracy of batch 27216/3750: 0.6875\n",
      "→ Loss: 0.4942348599433899 [Batch 27536/3750, Epoch 3/5]\n",
      "Accuracy of batch 27536/3750: 1.0\n",
      "→ Loss: 0.8456924557685852 [Batch 27856/3750, Epoch 3/5]\n",
      "Accuracy of batch 27856/3750: 0.6875\n",
      "→ Loss: 1.1283766031265259 [Batch 28176/3750, Epoch 3/5]\n",
      "Accuracy of batch 28176/3750: 0.5625\n",
      "→ Loss: 1.0866941213607788 [Batch 28496/3750, Epoch 3/5]\n",
      "Accuracy of batch 28496/3750: 0.625\n",
      "→ Loss: 1.0795167684555054 [Batch 28816/3750, Epoch 3/5]\n",
      "Accuracy of batch 28816/3750: 0.625\n",
      "→ Loss: 0.9992698431015015 [Batch 29136/3750, Epoch 3/5]\n",
      "Accuracy of batch 29136/3750: 0.5625\n",
      "→ Loss: 1.0893305540084839 [Batch 29456/3750, Epoch 3/5]\n",
      "Accuracy of batch 29456/3750: 0.4375\n",
      "→ Loss: 0.8092235326766968 [Batch 29776/3750, Epoch 3/5]\n",
      "Accuracy of batch 29776/3750: 0.8125\n",
      "→ Loss: 1.0549815893173218 [Batch 30096/3750, Epoch 3/5]\n",
      "Accuracy of batch 30096/3750: 0.625\n",
      "→ Loss: 0.7527006268501282 [Batch 30416/3750, Epoch 3/5]\n",
      "Accuracy of batch 30416/3750: 0.5625\n",
      "→ Loss: 0.6591669321060181 [Batch 30736/3750, Epoch 3/5]\n",
      "Accuracy of batch 30736/3750: 0.6875\n",
      "→ Loss: 0.7728807926177979 [Batch 31056/3750, Epoch 3/5]\n",
      "Accuracy of batch 31056/3750: 0.75\n",
      "→ Loss: 0.8643822073936462 [Batch 31376/3750, Epoch 3/5]\n",
      "Accuracy of batch 31376/3750: 0.6875\n",
      "→ Loss: 0.8204452991485596 [Batch 31696/3750, Epoch 3/5]\n",
      "Accuracy of batch 31696/3750: 0.6875\n",
      "→ Loss: 0.7889000177383423 [Batch 32016/3750, Epoch 3/5]\n",
      "Accuracy of batch 32016/3750: 0.75\n",
      "→ Loss: 0.8571569919586182 [Batch 32336/3750, Epoch 3/5]\n",
      "Accuracy of batch 32336/3750: 0.5\n",
      "→ Loss: 0.889393150806427 [Batch 32656/3750, Epoch 3/5]\n",
      "Accuracy of batch 32656/3750: 0.75\n",
      "→ Loss: 0.70335453748703 [Batch 32976/3750, Epoch 3/5]\n",
      "Accuracy of batch 32976/3750: 0.75\n",
      "→ Loss: 0.6668190360069275 [Batch 33296/3750, Epoch 3/5]\n",
      "Accuracy of batch 33296/3750: 0.8125\n",
      "→ Loss: 0.587031900882721 [Batch 33616/3750, Epoch 3/5]\n",
      "Accuracy of batch 33616/3750: 0.6875\n",
      "→ Loss: 0.7730463743209839 [Batch 33936/3750, Epoch 3/5]\n",
      "Accuracy of batch 33936/3750: 0.75\n",
      "→ Loss: 0.5400463342666626 [Batch 34256/3750, Epoch 3/5]\n",
      "Accuracy of batch 34256/3750: 0.9375\n",
      "→ Loss: 0.49691733717918396 [Batch 34576/3750, Epoch 3/5]\n",
      "Accuracy of batch 34576/3750: 0.9375\n",
      "→ Loss: 0.8830510973930359 [Batch 34896/3750, Epoch 3/5]\n",
      "Accuracy of batch 34896/3750: 0.75\n",
      "→ Loss: 0.6356157064437866 [Batch 35216/3750, Epoch 3/5]\n",
      "Accuracy of batch 35216/3750: 0.875\n",
      "→ Loss: 0.9285631775856018 [Batch 35536/3750, Epoch 3/5]\n",
      "Accuracy of batch 35536/3750: 0.625\n",
      "→ Loss: 0.8705843091011047 [Batch 35856/3750, Epoch 3/5]\n",
      "Accuracy of batch 35856/3750: 0.6875\n",
      "→ Loss: 0.7677028179168701 [Batch 36176/3750, Epoch 3/5]\n",
      "Accuracy of batch 36176/3750: 0.8125\n",
      "→ Loss: 0.7505232691764832 [Batch 36496/3750, Epoch 3/5]\n",
      "Accuracy of batch 36496/3750: 0.75\n",
      "→ Loss: 0.7886090278625488 [Batch 36816/3750, Epoch 3/5]\n",
      "Accuracy of batch 36816/3750: 0.625\n",
      "→ Loss: 0.8685347437858582 [Batch 37136/3750, Epoch 3/5]\n",
      "Accuracy of batch 37136/3750: 0.5\n",
      "→ Loss: 0.8283274173736572 [Batch 37456/3750, Epoch 3/5]\n",
      "Accuracy of batch 37456/3750: 0.75\n",
      "→ Loss: 0.93585604429245 [Batch 37776/3750, Epoch 3/5]\n",
      "Accuracy of batch 37776/3750: 0.75\n",
      "→ Loss: 0.7987134456634521 [Batch 38096/3750, Epoch 3/5]\n",
      "Accuracy of batch 38096/3750: 0.6875\n",
      "→ Loss: 1.0381869077682495 [Batch 38416/3750, Epoch 3/5]\n",
      "Accuracy of batch 38416/3750: 0.625\n",
      "→ Loss: 0.8712101578712463 [Batch 38736/3750, Epoch 3/5]\n",
      "Accuracy of batch 38736/3750: 0.6875\n",
      "→ Loss: 0.707543134689331 [Batch 39056/3750, Epoch 3/5]\n",
      "Accuracy of batch 39056/3750: 0.75\n",
      "→ Loss: 0.7557741403579712 [Batch 39376/3750, Epoch 3/5]\n",
      "Accuracy of batch 39376/3750: 0.8125\n",
      "→ Loss: 0.7596820592880249 [Batch 39696/3750, Epoch 3/5]\n",
      "Accuracy of batch 39696/3750: 0.5625\n",
      "→ Loss: 0.8276682496070862 [Batch 40016/3750, Epoch 3/5]\n",
      "Accuracy of batch 40016/3750: 0.75\n",
      "→ Loss: 0.7109825015068054 [Batch 40336/3750, Epoch 3/5]\n",
      "Accuracy of batch 40336/3750: 0.75\n",
      "→ Loss: 0.6301177144050598 [Batch 40656/3750, Epoch 3/5]\n",
      "Accuracy of batch 40656/3750: 0.625\n",
      "→ Loss: 0.8262456655502319 [Batch 40976/3750, Epoch 3/5]\n",
      "Accuracy of batch 40976/3750: 0.5625\n",
      "→ Loss: 0.9260787963867188 [Batch 41296/3750, Epoch 3/5]\n",
      "Accuracy of batch 41296/3750: 0.6875\n",
      "→ Loss: 0.7482592463493347 [Batch 41616/3750, Epoch 3/5]\n",
      "Accuracy of batch 41616/3750: 0.75\n",
      "→ Loss: 0.8749495148658752 [Batch 41936/3750, Epoch 3/5]\n",
      "Accuracy of batch 41936/3750: 0.75\n",
      "→ Loss: 0.749379575252533 [Batch 42256/3750, Epoch 3/5]\n",
      "Accuracy of batch 42256/3750: 0.8125\n",
      "→ Loss: 0.4099987745285034 [Batch 42576/3750, Epoch 3/5]\n",
      "Accuracy of batch 42576/3750: 0.9375\n",
      "→ Loss: 0.8159973621368408 [Batch 42896/3750, Epoch 3/5]\n",
      "Accuracy of batch 42896/3750: 0.75\n",
      "→ Loss: 0.7162824273109436 [Batch 43216/3750, Epoch 3/5]\n",
      "Accuracy of batch 43216/3750: 0.6875\n",
      "→ Loss: 0.8008960485458374 [Batch 43536/3750, Epoch 3/5]\n",
      "Accuracy of batch 43536/3750: 0.5\n",
      "→ Loss: 0.6156907677650452 [Batch 43856/3750, Epoch 3/5]\n",
      "Accuracy of batch 43856/3750: 0.75\n",
      "→ Loss: 0.7081559300422668 [Batch 44176/3750, Epoch 3/5]\n",
      "Accuracy of batch 44176/3750: 0.75\n",
      "→ Loss: 0.5006234645843506 [Batch 44496/3750, Epoch 3/5]\n",
      "Accuracy of batch 44496/3750: 0.9375\n",
      "→ Loss: 0.6589733958244324 [Batch 44816/3750, Epoch 3/5]\n",
      "Accuracy of batch 44816/3750: 0.75\n",
      "→ Loss: 0.7679919004440308 [Batch 45136/3750, Epoch 3/5]\n",
      "Accuracy of batch 45136/3750: 0.75\n",
      "→ Loss: 0.7954275608062744 [Batch 45456/3750, Epoch 3/5]\n",
      "Accuracy of batch 45456/3750: 0.5\n",
      "→ Loss: 0.6069919466972351 [Batch 45776/3750, Epoch 3/5]\n",
      "Accuracy of batch 45776/3750: 0.75\n",
      "→ Loss: 0.7065936923027039 [Batch 46096/3750, Epoch 3/5]\n",
      "Accuracy of batch 46096/3750: 0.5625\n",
      "→ Loss: 0.7964257597923279 [Batch 46416/3750, Epoch 3/5]\n",
      "Accuracy of batch 46416/3750: 0.75\n",
      "→ Loss: 0.673303484916687 [Batch 46736/3750, Epoch 3/5]\n",
      "Accuracy of batch 46736/3750: 0.75\n",
      "→ Loss: 0.8611264824867249 [Batch 47056/3750, Epoch 3/5]\n",
      "Accuracy of batch 47056/3750: 0.6875\n",
      "→ Loss: 0.8202935457229614 [Batch 47376/3750, Epoch 3/5]\n",
      "Accuracy of batch 47376/3750: 0.75\n",
      "→ Loss: 0.654747486114502 [Batch 47696/3750, Epoch 3/5]\n",
      "Accuracy of batch 47696/3750: 0.8125\n",
      "→ Loss: 0.6859350204467773 [Batch 48016/3750, Epoch 3/5]\n",
      "Accuracy of batch 48016/3750: 0.8125\n",
      "→ Loss: 0.7549722790718079 [Batch 48336/3750, Epoch 3/5]\n",
      "Accuracy of batch 48336/3750: 0.625\n",
      "→ Loss: 1.053054928779602 [Batch 48656/3750, Epoch 3/5]\n",
      "Accuracy of batch 48656/3750: 0.75\n",
      "→ Loss: 0.5941876769065857 [Batch 48976/3750, Epoch 3/5]\n",
      "Accuracy of batch 48976/3750: 0.875\n",
      "→ Loss: 0.7327070236206055 [Batch 49296/3750, Epoch 3/5]\n",
      "Accuracy of batch 49296/3750: 0.8125\n",
      "→ Loss: 0.7025448083877563 [Batch 49616/3750, Epoch 3/5]\n",
      "Accuracy of batch 49616/3750: 0.6875\n",
      "→ Loss: 0.6929726600646973 [Batch 49936/3750, Epoch 3/5]\n",
      "Accuracy of batch 49936/3750: 0.6875\n",
      "→ Loss: 0.6477497220039368 [Batch 50256/3750, Epoch 3/5]\n",
      "Accuracy of batch 50256/3750: 0.875\n",
      "→ Loss: 0.7719194293022156 [Batch 50576/3750, Epoch 3/5]\n",
      "Accuracy of batch 50576/3750: 0.6875\n",
      "→ Loss: 0.5431227684020996 [Batch 50896/3750, Epoch 3/5]\n",
      "Accuracy of batch 50896/3750: 0.8125\n",
      "→ Loss: 0.5213290452957153 [Batch 51216/3750, Epoch 3/5]\n",
      "Accuracy of batch 51216/3750: 0.75\n",
      "→ Loss: 0.8161963224411011 [Batch 51536/3750, Epoch 3/5]\n",
      "Accuracy of batch 51536/3750: 0.6875\n",
      "→ Loss: 1.003860592842102 [Batch 51856/3750, Epoch 3/5]\n",
      "Accuracy of batch 51856/3750: 0.625\n",
      "→ Loss: 0.7511298656463623 [Batch 52176/3750, Epoch 3/5]\n",
      "Accuracy of batch 52176/3750: 0.6875\n",
      "→ Loss: 0.9534014463424683 [Batch 52496/3750, Epoch 3/5]\n",
      "Accuracy of batch 52496/3750: 0.5625\n",
      "→ Loss: 0.7931275367736816 [Batch 52816/3750, Epoch 3/5]\n",
      "Accuracy of batch 52816/3750: 0.6875\n",
      "→ Loss: 0.9250166416168213 [Batch 53136/3750, Epoch 3/5]\n",
      "Accuracy of batch 53136/3750: 0.625\n",
      "→ Loss: 0.8151050209999084 [Batch 53456/3750, Epoch 3/5]\n",
      "Accuracy of batch 53456/3750: 0.8125\n",
      "→ Loss: 0.7333678603172302 [Batch 53776/3750, Epoch 3/5]\n",
      "Accuracy of batch 53776/3750: 0.6875\n",
      "→ Loss: 1.0438803434371948 [Batch 54096/3750, Epoch 3/5]\n",
      "Accuracy of batch 54096/3750: 0.5625\n",
      "→ Loss: 0.971265435218811 [Batch 54416/3750, Epoch 3/5]\n",
      "Accuracy of batch 54416/3750: 0.8125\n",
      "→ Loss: 0.4200967252254486 [Batch 54736/3750, Epoch 3/5]\n",
      "Accuracy of batch 54736/3750: 0.9375\n",
      "→ Loss: 0.5908376574516296 [Batch 55056/3750, Epoch 3/5]\n",
      "Accuracy of batch 55056/3750: 0.75\n",
      "→ Loss: 0.8263471126556396 [Batch 55376/3750, Epoch 3/5]\n",
      "Accuracy of batch 55376/3750: 0.625\n",
      "→ Loss: 0.8970385789871216 [Batch 55696/3750, Epoch 3/5]\n",
      "Accuracy of batch 55696/3750: 0.625\n",
      "→ Loss: 0.9883401393890381 [Batch 56016/3750, Epoch 3/5]\n",
      "Accuracy of batch 56016/3750: 0.6875\n",
      "→ Loss: 0.6679292917251587 [Batch 56336/3750, Epoch 3/5]\n",
      "Accuracy of batch 56336/3750: 0.6875\n",
      "→ Loss: 0.5459024906158447 [Batch 56656/3750, Epoch 3/5]\n",
      "Accuracy of batch 56656/3750: 0.8125\n",
      "→ Loss: 1.0485528707504272 [Batch 56976/3750, Epoch 3/5]\n",
      "Accuracy of batch 56976/3750: 0.5\n",
      "→ Loss: 0.7249869108200073 [Batch 57296/3750, Epoch 3/5]\n",
      "Accuracy of batch 57296/3750: 0.75\n",
      "→ Loss: 0.8895772695541382 [Batch 57616/3750, Epoch 3/5]\n",
      "Accuracy of batch 57616/3750: 0.625\n",
      "→ Loss: 0.7522783279418945 [Batch 57936/3750, Epoch 3/5]\n",
      "Accuracy of batch 57936/3750: 0.625\n",
      "→ Loss: 0.6927741765975952 [Batch 58256/3750, Epoch 3/5]\n",
      "Accuracy of batch 58256/3750: 0.8125\n",
      "→ Loss: 0.8706369996070862 [Batch 58576/3750, Epoch 3/5]\n",
      "Accuracy of batch 58576/3750: 0.625\n",
      "→ Loss: 0.9382015466690063 [Batch 58896/3750, Epoch 3/5]\n",
      "Accuracy of batch 58896/3750: 0.6875\n",
      "→ Loss: 0.6038384437561035 [Batch 59216/3750, Epoch 3/5]\n",
      "Accuracy of batch 59216/3750: 0.75\n",
      "→ Loss: 0.7320244312286377 [Batch 59536/3750, Epoch 3/5]\n",
      "Accuracy of batch 59536/3750: 0.625\n",
      "→ Loss: 0.5974441766738892 [Batch 59856/3750, Epoch 3/5]\n",
      "Accuracy of batch 59856/3750: 0.8125\n",
      "=== The epoch 3/5 has finished training ===\n",
      "→ Final accuracy of the epoch: 0.683178186416626\n",
      "→ Loss: 0.8378521203994751 [Batch 16/3750, Epoch 4/5]\n",
      "Accuracy of batch 16/3750: 0.8125\n",
      "→ Loss: 0.44978708028793335 [Batch 336/3750, Epoch 4/5]\n",
      "Accuracy of batch 336/3750: 0.9375\n",
      "→ Loss: 0.4741085171699524 [Batch 656/3750, Epoch 4/5]\n",
      "Accuracy of batch 656/3750: 1.0\n",
      "→ Loss: 1.1602109670639038 [Batch 976/3750, Epoch 4/5]\n",
      "Accuracy of batch 976/3750: 0.5625\n",
      "→ Loss: 0.7988499999046326 [Batch 1296/3750, Epoch 4/5]\n",
      "Accuracy of batch 1296/3750: 0.625\n",
      "→ Loss: 0.8406516313552856 [Batch 1616/3750, Epoch 4/5]\n",
      "Accuracy of batch 1616/3750: 0.6875\n",
      "→ Loss: 0.9867391586303711 [Batch 1936/3750, Epoch 4/5]\n",
      "Accuracy of batch 1936/3750: 0.75\n",
      "→ Loss: 0.541874349117279 [Batch 2256/3750, Epoch 4/5]\n",
      "Accuracy of batch 2256/3750: 0.8125\n",
      "→ Loss: 0.7781752347946167 [Batch 2576/3750, Epoch 4/5]\n",
      "Accuracy of batch 2576/3750: 0.625\n",
      "→ Loss: 1.03696870803833 [Batch 2896/3750, Epoch 4/5]\n",
      "Accuracy of batch 2896/3750: 0.625\n",
      "→ Loss: 0.7178622484207153 [Batch 3216/3750, Epoch 4/5]\n",
      "Accuracy of batch 3216/3750: 0.6875\n",
      "→ Loss: 0.7301895022392273 [Batch 3536/3750, Epoch 4/5]\n",
      "Accuracy of batch 3536/3750: 0.8125\n",
      "→ Loss: 0.724902868270874 [Batch 3856/3750, Epoch 4/5]\n",
      "Accuracy of batch 3856/3750: 0.6875\n",
      "→ Loss: 0.6733580231666565 [Batch 4176/3750, Epoch 4/5]\n",
      "Accuracy of batch 4176/3750: 0.75\n",
      "→ Loss: 0.7086621522903442 [Batch 4496/3750, Epoch 4/5]\n",
      "Accuracy of batch 4496/3750: 0.875\n",
      "→ Loss: 0.70121830701828 [Batch 4816/3750, Epoch 4/5]\n",
      "Accuracy of batch 4816/3750: 0.75\n",
      "→ Loss: 0.9415398836135864 [Batch 5136/3750, Epoch 4/5]\n",
      "Accuracy of batch 5136/3750: 0.625\n",
      "→ Loss: 0.8086019158363342 [Batch 5456/3750, Epoch 4/5]\n",
      "Accuracy of batch 5456/3750: 0.6875\n",
      "→ Loss: 0.8549623489379883 [Batch 5776/3750, Epoch 4/5]\n",
      "Accuracy of batch 5776/3750: 0.75\n",
      "→ Loss: 0.8628484606742859 [Batch 6096/3750, Epoch 4/5]\n",
      "Accuracy of batch 6096/3750: 0.5625\n",
      "→ Loss: 0.6899997591972351 [Batch 6416/3750, Epoch 4/5]\n",
      "Accuracy of batch 6416/3750: 0.75\n",
      "→ Loss: 0.681640088558197 [Batch 6736/3750, Epoch 4/5]\n",
      "Accuracy of batch 6736/3750: 0.8125\n",
      "→ Loss: 0.9786201119422913 [Batch 7056/3750, Epoch 4/5]\n",
      "Accuracy of batch 7056/3750: 0.5625\n",
      "→ Loss: 0.9876449108123779 [Batch 7376/3750, Epoch 4/5]\n",
      "Accuracy of batch 7376/3750: 0.8125\n",
      "→ Loss: 0.7211479544639587 [Batch 7696/3750, Epoch 4/5]\n",
      "Accuracy of batch 7696/3750: 0.8125\n",
      "→ Loss: 0.8691937327384949 [Batch 8016/3750, Epoch 4/5]\n",
      "Accuracy of batch 8016/3750: 0.6875\n",
      "→ Loss: 1.218641757965088 [Batch 8336/3750, Epoch 4/5]\n",
      "Accuracy of batch 8336/3750: 0.4375\n",
      "→ Loss: 0.7694017291069031 [Batch 8656/3750, Epoch 4/5]\n",
      "Accuracy of batch 8656/3750: 0.5625\n",
      "→ Loss: 1.2219815254211426 [Batch 8976/3750, Epoch 4/5]\n",
      "Accuracy of batch 8976/3750: 0.5\n",
      "→ Loss: 0.5763680934906006 [Batch 9296/3750, Epoch 4/5]\n",
      "Accuracy of batch 9296/3750: 0.8125\n",
      "→ Loss: 0.945238471031189 [Batch 9616/3750, Epoch 4/5]\n",
      "Accuracy of batch 9616/3750: 0.6875\n",
      "→ Loss: 0.5338534712791443 [Batch 9936/3750, Epoch 4/5]\n",
      "Accuracy of batch 9936/3750: 0.9375\n",
      "→ Loss: 0.9127717018127441 [Batch 10256/3750, Epoch 4/5]\n",
      "Accuracy of batch 10256/3750: 0.625\n",
      "→ Loss: 0.5753244161605835 [Batch 10576/3750, Epoch 4/5]\n",
      "Accuracy of batch 10576/3750: 0.875\n",
      "→ Loss: 0.719994843006134 [Batch 10896/3750, Epoch 4/5]\n",
      "Accuracy of batch 10896/3750: 0.6875\n",
      "→ Loss: 0.7333511710166931 [Batch 11216/3750, Epoch 4/5]\n",
      "Accuracy of batch 11216/3750: 0.625\n",
      "→ Loss: 0.9838086366653442 [Batch 11536/3750, Epoch 4/5]\n",
      "Accuracy of batch 11536/3750: 0.625\n",
      "→ Loss: 0.8046354651451111 [Batch 11856/3750, Epoch 4/5]\n",
      "Accuracy of batch 11856/3750: 0.625\n",
      "→ Loss: 0.7132118344306946 [Batch 12176/3750, Epoch 4/5]\n",
      "Accuracy of batch 12176/3750: 0.6875\n",
      "→ Loss: 0.8763519525527954 [Batch 12496/3750, Epoch 4/5]\n",
      "Accuracy of batch 12496/3750: 0.5625\n",
      "→ Loss: 0.7873148322105408 [Batch 12816/3750, Epoch 4/5]\n",
      "Accuracy of batch 12816/3750: 0.5625\n",
      "→ Loss: 0.8134686946868896 [Batch 13136/3750, Epoch 4/5]\n",
      "Accuracy of batch 13136/3750: 0.75\n",
      "→ Loss: 0.8852779865264893 [Batch 13456/3750, Epoch 4/5]\n",
      "Accuracy of batch 13456/3750: 0.6875\n",
      "→ Loss: 0.9262572526931763 [Batch 13776/3750, Epoch 4/5]\n",
      "Accuracy of batch 13776/3750: 0.5625\n",
      "→ Loss: 0.6915717124938965 [Batch 14096/3750, Epoch 4/5]\n",
      "Accuracy of batch 14096/3750: 0.75\n",
      "→ Loss: 0.5685292482376099 [Batch 14416/3750, Epoch 4/5]\n",
      "Accuracy of batch 14416/3750: 0.875\n",
      "→ Loss: 0.7327149510383606 [Batch 14736/3750, Epoch 4/5]\n",
      "Accuracy of batch 14736/3750: 0.75\n",
      "→ Loss: 0.574522852897644 [Batch 15056/3750, Epoch 4/5]\n",
      "Accuracy of batch 15056/3750: 0.75\n",
      "→ Loss: 0.8468120694160461 [Batch 15376/3750, Epoch 4/5]\n",
      "Accuracy of batch 15376/3750: 0.625\n",
      "→ Loss: 0.6633791327476501 [Batch 15696/3750, Epoch 4/5]\n",
      "Accuracy of batch 15696/3750: 0.875\n",
      "→ Loss: 0.7966037392616272 [Batch 16016/3750, Epoch 4/5]\n",
      "Accuracy of batch 16016/3750: 0.875\n",
      "→ Loss: 0.5796498656272888 [Batch 16336/3750, Epoch 4/5]\n",
      "Accuracy of batch 16336/3750: 0.8125\n",
      "→ Loss: 0.708484947681427 [Batch 16656/3750, Epoch 4/5]\n",
      "Accuracy of batch 16656/3750: 0.875\n",
      "→ Loss: 0.8204351663589478 [Batch 16976/3750, Epoch 4/5]\n",
      "Accuracy of batch 16976/3750: 0.6875\n",
      "→ Loss: 0.5727041363716125 [Batch 17296/3750, Epoch 4/5]\n",
      "Accuracy of batch 17296/3750: 0.8125\n",
      "→ Loss: 0.727787971496582 [Batch 17616/3750, Epoch 4/5]\n",
      "Accuracy of batch 17616/3750: 0.625\n",
      "→ Loss: 0.6118451952934265 [Batch 17936/3750, Epoch 4/5]\n",
      "Accuracy of batch 17936/3750: 0.6875\n",
      "→ Loss: 1.3877284526824951 [Batch 18256/3750, Epoch 4/5]\n",
      "Accuracy of batch 18256/3750: 0.625\n",
      "→ Loss: 1.0129491090774536 [Batch 18576/3750, Epoch 4/5]\n",
      "Accuracy of batch 18576/3750: 0.5625\n",
      "→ Loss: 0.8484119772911072 [Batch 18896/3750, Epoch 4/5]\n",
      "Accuracy of batch 18896/3750: 0.6875\n",
      "→ Loss: 0.5749397873878479 [Batch 19216/3750, Epoch 4/5]\n",
      "Accuracy of batch 19216/3750: 0.8125\n",
      "→ Loss: 0.6319670677185059 [Batch 19536/3750, Epoch 4/5]\n",
      "Accuracy of batch 19536/3750: 0.8125\n",
      "→ Loss: 1.048797369003296 [Batch 19856/3750, Epoch 4/5]\n",
      "Accuracy of batch 19856/3750: 0.625\n",
      "→ Loss: 0.7157013416290283 [Batch 20176/3750, Epoch 4/5]\n",
      "Accuracy of batch 20176/3750: 0.75\n",
      "→ Loss: 0.6840124130249023 [Batch 20496/3750, Epoch 4/5]\n",
      "Accuracy of batch 20496/3750: 0.5625\n",
      "→ Loss: 1.1624003648757935 [Batch 20816/3750, Epoch 4/5]\n",
      "Accuracy of batch 20816/3750: 0.625\n",
      "→ Loss: 0.513140857219696 [Batch 21136/3750, Epoch 4/5]\n",
      "Accuracy of batch 21136/3750: 0.875\n",
      "→ Loss: 0.9840714335441589 [Batch 21456/3750, Epoch 4/5]\n",
      "Accuracy of batch 21456/3750: 0.5625\n",
      "→ Loss: 1.0300731658935547 [Batch 21776/3750, Epoch 4/5]\n",
      "Accuracy of batch 21776/3750: 0.625\n",
      "→ Loss: 0.8220860958099365 [Batch 22096/3750, Epoch 4/5]\n",
      "Accuracy of batch 22096/3750: 0.8125\n",
      "→ Loss: 0.6944110989570618 [Batch 22416/3750, Epoch 4/5]\n",
      "Accuracy of batch 22416/3750: 0.6875\n",
      "→ Loss: 0.46494218707084656 [Batch 22736/3750, Epoch 4/5]\n",
      "Accuracy of batch 22736/3750: 0.875\n",
      "→ Loss: 0.9871305227279663 [Batch 23056/3750, Epoch 4/5]\n",
      "Accuracy of batch 23056/3750: 0.5625\n",
      "→ Loss: 0.6766049265861511 [Batch 23376/3750, Epoch 4/5]\n",
      "Accuracy of batch 23376/3750: 0.6875\n",
      "→ Loss: 0.911670982837677 [Batch 23696/3750, Epoch 4/5]\n",
      "Accuracy of batch 23696/3750: 0.625\n",
      "→ Loss: 0.9174944758415222 [Batch 24016/3750, Epoch 4/5]\n",
      "Accuracy of batch 24016/3750: 0.5625\n",
      "→ Loss: 0.7508950233459473 [Batch 24336/3750, Epoch 4/5]\n",
      "Accuracy of batch 24336/3750: 0.75\n",
      "→ Loss: 0.6464996337890625 [Batch 24656/3750, Epoch 4/5]\n",
      "Accuracy of batch 24656/3750: 0.6875\n",
      "→ Loss: 1.0630959272384644 [Batch 24976/3750, Epoch 4/5]\n",
      "Accuracy of batch 24976/3750: 0.6875\n",
      "→ Loss: 0.8342153429985046 [Batch 25296/3750, Epoch 4/5]\n",
      "Accuracy of batch 25296/3750: 0.75\n",
      "→ Loss: 0.7947733402252197 [Batch 25616/3750, Epoch 4/5]\n",
      "Accuracy of batch 25616/3750: 0.8125\n",
      "→ Loss: 0.7061077356338501 [Batch 25936/3750, Epoch 4/5]\n",
      "Accuracy of batch 25936/3750: 0.75\n",
      "→ Loss: 1.1423192024230957 [Batch 26256/3750, Epoch 4/5]\n",
      "Accuracy of batch 26256/3750: 0.625\n",
      "→ Loss: 0.7161368131637573 [Batch 26576/3750, Epoch 4/5]\n",
      "Accuracy of batch 26576/3750: 0.75\n",
      "→ Loss: 0.7243642210960388 [Batch 26896/3750, Epoch 4/5]\n",
      "Accuracy of batch 26896/3750: 0.75\n",
      "→ Loss: 0.7216680645942688 [Batch 27216/3750, Epoch 4/5]\n",
      "Accuracy of batch 27216/3750: 0.625\n",
      "→ Loss: 0.37259936332702637 [Batch 27536/3750, Epoch 4/5]\n",
      "Accuracy of batch 27536/3750: 1.0\n",
      "→ Loss: 0.696131706237793 [Batch 27856/3750, Epoch 4/5]\n",
      "Accuracy of batch 27856/3750: 0.75\n",
      "→ Loss: 0.9564508199691772 [Batch 28176/3750, Epoch 4/5]\n",
      "Accuracy of batch 28176/3750: 0.625\n",
      "→ Loss: 0.9924672245979309 [Batch 28496/3750, Epoch 4/5]\n",
      "Accuracy of batch 28496/3750: 0.625\n",
      "→ Loss: 1.0271533727645874 [Batch 28816/3750, Epoch 4/5]\n",
      "Accuracy of batch 28816/3750: 0.625\n",
      "→ Loss: 0.9242508411407471 [Batch 29136/3750, Epoch 4/5]\n",
      "Accuracy of batch 29136/3750: 0.5625\n",
      "→ Loss: 0.9748553037643433 [Batch 29456/3750, Epoch 4/5]\n",
      "Accuracy of batch 29456/3750: 0.5\n",
      "→ Loss: 0.7181912660598755 [Batch 29776/3750, Epoch 4/5]\n",
      "Accuracy of batch 29776/3750: 0.8125\n",
      "→ Loss: 0.9619330763816833 [Batch 30096/3750, Epoch 4/5]\n",
      "Accuracy of batch 30096/3750: 0.625\n",
      "→ Loss: 0.6079014539718628 [Batch 30416/3750, Epoch 4/5]\n",
      "Accuracy of batch 30416/3750: 0.75\n",
      "→ Loss: 0.5317684412002563 [Batch 30736/3750, Epoch 4/5]\n",
      "Accuracy of batch 30736/3750: 0.75\n",
      "→ Loss: 0.6379196643829346 [Batch 31056/3750, Epoch 4/5]\n",
      "Accuracy of batch 31056/3750: 0.8125\n",
      "→ Loss: 0.6752714514732361 [Batch 31376/3750, Epoch 4/5]\n",
      "Accuracy of batch 31376/3750: 0.75\n",
      "→ Loss: 0.7722758650779724 [Batch 31696/3750, Epoch 4/5]\n",
      "Accuracy of batch 31696/3750: 0.625\n",
      "→ Loss: 0.6052332520484924 [Batch 32016/3750, Epoch 4/5]\n",
      "Accuracy of batch 32016/3750: 0.8125\n",
      "→ Loss: 0.7797691822052002 [Batch 32336/3750, Epoch 4/5]\n",
      "Accuracy of batch 32336/3750: 0.5625\n",
      "→ Loss: 0.7391363382339478 [Batch 32656/3750, Epoch 4/5]\n",
      "Accuracy of batch 32656/3750: 0.75\n",
      "→ Loss: 0.5769450664520264 [Batch 32976/3750, Epoch 4/5]\n",
      "Accuracy of batch 32976/3750: 0.8125\n",
      "→ Loss: 0.5767964124679565 [Batch 33296/3750, Epoch 4/5]\n",
      "Accuracy of batch 33296/3750: 0.75\n",
      "→ Loss: 0.4583963453769684 [Batch 33616/3750, Epoch 4/5]\n",
      "Accuracy of batch 33616/3750: 0.9375\n",
      "→ Loss: 0.6480510234832764 [Batch 33936/3750, Epoch 4/5]\n",
      "Accuracy of batch 33936/3750: 0.8125\n",
      "→ Loss: 0.4188550114631653 [Batch 34256/3750, Epoch 4/5]\n",
      "Accuracy of batch 34256/3750: 0.9375\n",
      "→ Loss: 0.3559105396270752 [Batch 34576/3750, Epoch 4/5]\n",
      "Accuracy of batch 34576/3750: 1.0\n",
      "→ Loss: 0.7649816274642944 [Batch 34896/3750, Epoch 4/5]\n",
      "Accuracy of batch 34896/3750: 0.8125\n",
      "→ Loss: 0.5069129467010498 [Batch 35216/3750, Epoch 4/5]\n",
      "Accuracy of batch 35216/3750: 0.875\n",
      "→ Loss: 0.825957715511322 [Batch 35536/3750, Epoch 4/5]\n",
      "Accuracy of batch 35536/3750: 0.75\n",
      "→ Loss: 0.772783637046814 [Batch 35856/3750, Epoch 4/5]\n",
      "Accuracy of batch 35856/3750: 0.6875\n",
      "→ Loss: 0.6269321441650391 [Batch 36176/3750, Epoch 4/5]\n",
      "Accuracy of batch 36176/3750: 0.875\n",
      "→ Loss: 0.6456660032272339 [Batch 36496/3750, Epoch 4/5]\n",
      "Accuracy of batch 36496/3750: 0.75\n",
      "→ Loss: 0.7280279994010925 [Batch 36816/3750, Epoch 4/5]\n",
      "Accuracy of batch 36816/3750: 0.6875\n",
      "→ Loss: 0.7833109498023987 [Batch 37136/3750, Epoch 4/5]\n",
      "Accuracy of batch 37136/3750: 0.625\n",
      "→ Loss: 0.7128949165344238 [Batch 37456/3750, Epoch 4/5]\n",
      "Accuracy of batch 37456/3750: 0.75\n",
      "→ Loss: 0.8753080368041992 [Batch 37776/3750, Epoch 4/5]\n",
      "Accuracy of batch 37776/3750: 0.6875\n",
      "→ Loss: 0.6494095921516418 [Batch 38096/3750, Epoch 4/5]\n",
      "Accuracy of batch 38096/3750: 0.75\n",
      "→ Loss: 0.9791836142539978 [Batch 38416/3750, Epoch 4/5]\n",
      "Accuracy of batch 38416/3750: 0.75\n",
      "→ Loss: 0.7903474569320679 [Batch 38736/3750, Epoch 4/5]\n",
      "Accuracy of batch 38736/3750: 0.6875\n",
      "→ Loss: 0.6306832432746887 [Batch 39056/3750, Epoch 4/5]\n",
      "Accuracy of batch 39056/3750: 0.875\n",
      "→ Loss: 0.7103883624076843 [Batch 39376/3750, Epoch 4/5]\n",
      "Accuracy of batch 39376/3750: 0.8125\n",
      "→ Loss: 0.6532773375511169 [Batch 39696/3750, Epoch 4/5]\n",
      "Accuracy of batch 39696/3750: 0.8125\n",
      "→ Loss: 0.7867441773414612 [Batch 40016/3750, Epoch 4/5]\n",
      "Accuracy of batch 40016/3750: 0.6875\n",
      "→ Loss: 0.5588252544403076 [Batch 40336/3750, Epoch 4/5]\n",
      "Accuracy of batch 40336/3750: 0.9375\n",
      "→ Loss: 0.5297941565513611 [Batch 40656/3750, Epoch 4/5]\n",
      "Accuracy of batch 40656/3750: 0.8125\n",
      "→ Loss: 0.7369335293769836 [Batch 40976/3750, Epoch 4/5]\n",
      "Accuracy of batch 40976/3750: 0.625\n",
      "→ Loss: 0.8179998397827148 [Batch 41296/3750, Epoch 4/5]\n",
      "Accuracy of batch 41296/3750: 0.75\n",
      "→ Loss: 0.6770810484886169 [Batch 41616/3750, Epoch 4/5]\n",
      "Accuracy of batch 41616/3750: 0.75\n",
      "→ Loss: 0.7171347737312317 [Batch 41936/3750, Epoch 4/5]\n",
      "Accuracy of batch 41936/3750: 0.75\n",
      "→ Loss: 0.5993672609329224 [Batch 42256/3750, Epoch 4/5]\n",
      "Accuracy of batch 42256/3750: 0.875\n",
      "→ Loss: 0.3320896327495575 [Batch 42576/3750, Epoch 4/5]\n",
      "Accuracy of batch 42576/3750: 0.9375\n",
      "→ Loss: 0.7087315320968628 [Batch 42896/3750, Epoch 4/5]\n",
      "Accuracy of batch 42896/3750: 0.75\n",
      "→ Loss: 0.6409575939178467 [Batch 43216/3750, Epoch 4/5]\n",
      "Accuracy of batch 43216/3750: 0.6875\n",
      "→ Loss: 0.6801043152809143 [Batch 43536/3750, Epoch 4/5]\n",
      "Accuracy of batch 43536/3750: 0.625\n",
      "→ Loss: 0.5135891437530518 [Batch 43856/3750, Epoch 4/5]\n",
      "Accuracy of batch 43856/3750: 0.8125\n",
      "→ Loss: 0.5869904160499573 [Batch 44176/3750, Epoch 4/5]\n",
      "Accuracy of batch 44176/3750: 0.8125\n",
      "→ Loss: 0.40953508019447327 [Batch 44496/3750, Epoch 4/5]\n",
      "Accuracy of batch 44496/3750: 0.875\n",
      "→ Loss: 0.5638405084609985 [Batch 44816/3750, Epoch 4/5]\n",
      "Accuracy of batch 44816/3750: 0.8125\n",
      "→ Loss: 0.7516346573829651 [Batch 45136/3750, Epoch 4/5]\n",
      "Accuracy of batch 45136/3750: 0.6875\n",
      "→ Loss: 0.7031175494194031 [Batch 45456/3750, Epoch 4/5]\n",
      "Accuracy of batch 45456/3750: 0.6875\n",
      "→ Loss: 0.47080448269844055 [Batch 45776/3750, Epoch 4/5]\n",
      "Accuracy of batch 45776/3750: 1.0\n",
      "→ Loss: 0.6846023201942444 [Batch 46096/3750, Epoch 4/5]\n",
      "Accuracy of batch 46096/3750: 0.625\n",
      "→ Loss: 0.6642378568649292 [Batch 46416/3750, Epoch 4/5]\n",
      "Accuracy of batch 46416/3750: 0.8125\n",
      "→ Loss: 0.5440005660057068 [Batch 46736/3750, Epoch 4/5]\n",
      "Accuracy of batch 46736/3750: 0.8125\n",
      "→ Loss: 0.8164550065994263 [Batch 47056/3750, Epoch 4/5]\n",
      "Accuracy of batch 47056/3750: 0.75\n",
      "→ Loss: 0.7169914245605469 [Batch 47376/3750, Epoch 4/5]\n",
      "Accuracy of batch 47376/3750: 0.8125\n",
      "→ Loss: 0.5325267910957336 [Batch 47696/3750, Epoch 4/5]\n",
      "Accuracy of batch 47696/3750: 0.8125\n",
      "→ Loss: 0.558610200881958 [Batch 48016/3750, Epoch 4/5]\n",
      "Accuracy of batch 48016/3750: 0.8125\n",
      "→ Loss: 0.6651389598846436 [Batch 48336/3750, Epoch 4/5]\n",
      "Accuracy of batch 48336/3750: 0.75\n",
      "→ Loss: 0.9273630380630493 [Batch 48656/3750, Epoch 4/5]\n",
      "Accuracy of batch 48656/3750: 0.6875\n",
      "→ Loss: 0.5342783331871033 [Batch 48976/3750, Epoch 4/5]\n",
      "Accuracy of batch 48976/3750: 0.875\n",
      "→ Loss: 0.6437699198722839 [Batch 49296/3750, Epoch 4/5]\n",
      "Accuracy of batch 49296/3750: 0.875\n",
      "→ Loss: 0.6320765614509583 [Batch 49616/3750, Epoch 4/5]\n",
      "Accuracy of batch 49616/3750: 0.75\n",
      "→ Loss: 0.5793949961662292 [Batch 49936/3750, Epoch 4/5]\n",
      "Accuracy of batch 49936/3750: 0.8125\n",
      "→ Loss: 0.5618706941604614 [Batch 50256/3750, Epoch 4/5]\n",
      "Accuracy of batch 50256/3750: 0.9375\n",
      "→ Loss: 0.6397213339805603 [Batch 50576/3750, Epoch 4/5]\n",
      "Accuracy of batch 50576/3750: 0.8125\n",
      "→ Loss: 0.48112475872039795 [Batch 50896/3750, Epoch 4/5]\n",
      "Accuracy of batch 50896/3750: 0.8125\n",
      "→ Loss: 0.461344838142395 [Batch 51216/3750, Epoch 4/5]\n",
      "Accuracy of batch 51216/3750: 0.75\n",
      "→ Loss: 0.6879933476448059 [Batch 51536/3750, Epoch 4/5]\n",
      "Accuracy of batch 51536/3750: 0.75\n",
      "→ Loss: 0.8891428709030151 [Batch 51856/3750, Epoch 4/5]\n",
      "Accuracy of batch 51856/3750: 0.6875\n",
      "→ Loss: 0.6812329292297363 [Batch 52176/3750, Epoch 4/5]\n",
      "Accuracy of batch 52176/3750: 0.8125\n",
      "→ Loss: 0.8781135678291321 [Batch 52496/3750, Epoch 4/5]\n",
      "Accuracy of batch 52496/3750: 0.625\n",
      "→ Loss: 0.6574440598487854 [Batch 52816/3750, Epoch 4/5]\n",
      "Accuracy of batch 52816/3750: 0.75\n",
      "→ Loss: 0.7986282110214233 [Batch 53136/3750, Epoch 4/5]\n",
      "Accuracy of batch 53136/3750: 0.625\n",
      "→ Loss: 0.7268503308296204 [Batch 53456/3750, Epoch 4/5]\n",
      "Accuracy of batch 53456/3750: 0.8125\n",
      "→ Loss: 0.6768207550048828 [Batch 53776/3750, Epoch 4/5]\n",
      "Accuracy of batch 53776/3750: 0.75\n",
      "→ Loss: 0.9791384935379028 [Batch 54096/3750, Epoch 4/5]\n",
      "Accuracy of batch 54096/3750: 0.625\n",
      "→ Loss: 0.9453072547912598 [Batch 54416/3750, Epoch 4/5]\n",
      "Accuracy of batch 54416/3750: 0.8125\n",
      "→ Loss: 0.32788318395614624 [Batch 54736/3750, Epoch 4/5]\n",
      "Accuracy of batch 54736/3750: 0.9375\n",
      "→ Loss: 0.47946304082870483 [Batch 55056/3750, Epoch 4/5]\n",
      "Accuracy of batch 55056/3750: 0.8125\n",
      "→ Loss: 0.7078107595443726 [Batch 55376/3750, Epoch 4/5]\n",
      "Accuracy of batch 55376/3750: 0.75\n",
      "→ Loss: 0.7921691536903381 [Batch 55696/3750, Epoch 4/5]\n",
      "Accuracy of batch 55696/3750: 0.75\n",
      "→ Loss: 0.8754000067710876 [Batch 56016/3750, Epoch 4/5]\n",
      "Accuracy of batch 56016/3750: 0.6875\n",
      "→ Loss: 0.6063042879104614 [Batch 56336/3750, Epoch 4/5]\n",
      "Accuracy of batch 56336/3750: 0.8125\n",
      "→ Loss: 0.4132402837276459 [Batch 56656/3750, Epoch 4/5]\n",
      "Accuracy of batch 56656/3750: 0.9375\n",
      "→ Loss: 0.9256231784820557 [Batch 56976/3750, Epoch 4/5]\n",
      "Accuracy of batch 56976/3750: 0.5625\n",
      "→ Loss: 0.6665738224983215 [Batch 57296/3750, Epoch 4/5]\n",
      "Accuracy of batch 57296/3750: 0.75\n",
      "→ Loss: 0.7729203701019287 [Batch 57616/3750, Epoch 4/5]\n",
      "Accuracy of batch 57616/3750: 0.6875\n",
      "→ Loss: 0.6567614078521729 [Batch 57936/3750, Epoch 4/5]\n",
      "Accuracy of batch 57936/3750: 0.6875\n",
      "→ Loss: 0.584403932094574 [Batch 58256/3750, Epoch 4/5]\n",
      "Accuracy of batch 58256/3750: 0.8125\n",
      "→ Loss: 0.8085945248603821 [Batch 58576/3750, Epoch 4/5]\n",
      "Accuracy of batch 58576/3750: 0.625\n",
      "→ Loss: 0.8231207728385925 [Batch 58896/3750, Epoch 4/5]\n",
      "Accuracy of batch 58896/3750: 0.6875\n",
      "→ Loss: 0.4605867862701416 [Batch 59216/3750, Epoch 4/5]\n",
      "Accuracy of batch 59216/3750: 0.8125\n",
      "→ Loss: 0.597954511642456 [Batch 59536/3750, Epoch 4/5]\n",
      "Accuracy of batch 59536/3750: 0.75\n",
      "→ Loss: 0.501611053943634 [Batch 59856/3750, Epoch 4/5]\n",
      "Accuracy of batch 59856/3750: 0.875\n",
      "=== The epoch 4/5 has finished training ===\n",
      "→ Final accuracy of the epoch: 0.7390292286872864\n",
      "→ Accuracy for image 0: 0.625\n",
      "→ Accuracy for image 1: 0.5625\n",
      "→ Accuracy for image 2: 0.75\n",
      "→ Accuracy for image 3: 0.75\n",
      "→ Accuracy for image 4: 0.6875\n",
      "→ Accuracy for image 5: 0.8125\n",
      "→ Accuracy for image 6: 0.8125\n",
      "→ Accuracy for image 7: 0.875\n",
      "→ Accuracy for image 8: 0.9375\n",
      "→ Accuracy for image 9: 0.6875\n",
      "→ Accuracy for image 10: 0.75\n",
      "→ Accuracy for image 11: 0.8125\n",
      "→ Accuracy for image 12: 0.875\n",
      "→ Accuracy for image 13: 0.8125\n",
      "→ Accuracy for image 14: 0.875\n",
      "→ Accuracy for image 15: 0.5625\n",
      "→ Accuracy for image 16: 0.8125\n",
      "→ Accuracy for image 17: 0.75\n",
      "→ Accuracy for image 18: 0.75\n",
      "→ Accuracy for image 19: 0.5625\n",
      "→ Accuracy for image 20: 0.6875\n",
      "→ Accuracy for image 21: 0.9375\n",
      "→ Accuracy for image 22: 0.875\n",
      "→ Accuracy for image 23: 0.875\n",
      "→ Accuracy for image 24: 0.75\n",
      "→ Accuracy for image 25: 0.75\n",
      "→ Accuracy for image 26: 0.8125\n",
      "→ Accuracy for image 27: 0.6875\n",
      "→ Accuracy for image 28: 0.6875\n",
      "→ Accuracy for image 29: 0.75\n",
      "→ Accuracy for image 30: 0.8125\n",
      "→ Accuracy for image 31: 0.6875\n",
      "→ Accuracy for image 32: 0.9375\n",
      "→ Accuracy for image 33: 0.875\n",
      "→ Accuracy for image 34: 0.75\n",
      "→ Accuracy for image 35: 0.5625\n",
      "→ Accuracy for image 36: 0.5625\n",
      "→ Accuracy for image 37: 0.8125\n",
      "→ Accuracy for image 38: 0.75\n",
      "→ Accuracy for image 39: 0.625\n",
      "→ Accuracy for image 40: 0.9375\n",
      "→ Accuracy for image 41: 0.625\n",
      "→ Accuracy for image 42: 0.8125\n",
      "→ Accuracy for image 43: 0.6875\n",
      "→ Accuracy for image 44: 0.6875\n",
      "→ Accuracy for image 45: 0.5625\n",
      "→ Accuracy for image 46: 0.8125\n",
      "→ Accuracy for image 47: 0.625\n",
      "→ Accuracy for image 48: 1.0\n",
      "→ Accuracy for image 49: 0.75\n",
      "→ Accuracy for image 50: 0.8125\n",
      "→ Accuracy for image 51: 0.75\n",
      "→ Accuracy for image 52: 0.875\n",
      "→ Accuracy for image 53: 0.875\n",
      "→ Accuracy for image 54: 0.875\n",
      "→ Accuracy for image 55: 0.6875\n",
      "→ Accuracy for image 56: 0.625\n",
      "→ Accuracy for image 57: 0.8125\n",
      "→ Accuracy for image 58: 0.875\n",
      "→ Accuracy for image 59: 0.6875\n",
      "→ Accuracy for image 60: 0.625\n",
      "→ Accuracy for image 61: 0.5\n",
      "→ Accuracy for image 62: 0.625\n",
      "→ Accuracy for image 63: 0.875\n",
      "→ Accuracy for image 64: 0.8125\n",
      "→ Accuracy for image 65: 0.75\n",
      "→ Accuracy for image 66: 0.75\n",
      "→ Accuracy for image 67: 0.8125\n",
      "→ Accuracy for image 68: 0.8125\n",
      "→ Accuracy for image 69: 0.6875\n",
      "→ Accuracy for image 70: 0.6875\n",
      "→ Accuracy for image 71: 0.875\n",
      "→ Accuracy for image 72: 0.5625\n",
      "→ Accuracy for image 73: 0.875\n",
      "→ Accuracy for image 74: 0.625\n",
      "→ Accuracy for image 75: 0.8125\n",
      "→ Accuracy for image 76: 0.75\n",
      "→ Accuracy for image 77: 0.8125\n",
      "→ Accuracy for image 78: 0.75\n",
      "→ Accuracy for image 79: 0.9375\n",
      "→ Accuracy for image 80: 0.75\n",
      "→ Accuracy for image 81: 0.6875\n",
      "→ Accuracy for image 82: 0.8125\n",
      "→ Accuracy for image 83: 0.8125\n",
      "→ Accuracy for image 84: 0.6875\n",
      "→ Accuracy for image 85: 0.875\n",
      "→ Accuracy for image 86: 0.875\n",
      "→ Accuracy for image 87: 0.6875\n",
      "→ Accuracy for image 88: 0.9375\n",
      "→ Accuracy for image 89: 0.8125\n",
      "→ Accuracy for image 90: 0.875\n",
      "→ Accuracy for image 91: 0.6875\n",
      "→ Accuracy for image 92: 0.8125\n",
      "→ Accuracy for image 93: 0.75\n",
      "→ Accuracy for image 94: 0.875\n",
      "→ Accuracy for image 95: 0.75\n",
      "→ Accuracy for image 96: 0.8125\n",
      "→ Accuracy for image 97: 0.875\n",
      "→ Accuracy for image 98: 1.0\n",
      "→ Accuracy for image 99: 0.5625\n",
      "→ Accuracy for image 100: 0.875\n",
      "→ Accuracy for image 101: 0.875\n",
      "→ Accuracy for image 102: 0.625\n",
      "→ Accuracy for image 103: 0.5625\n",
      "→ Accuracy for image 104: 0.75\n",
      "→ Accuracy for image 105: 0.9375\n",
      "→ Accuracy for image 106: 0.6875\n",
      "→ Accuracy for image 107: 0.625\n",
      "→ Accuracy for image 108: 0.625\n",
      "→ Accuracy for image 109: 0.625\n",
      "→ Accuracy for image 110: 0.8125\n",
      "→ Accuracy for image 111: 0.75\n",
      "→ Accuracy for image 112: 0.875\n",
      "→ Accuracy for image 113: 0.625\n",
      "→ Accuracy for image 114: 0.75\n",
      "→ Accuracy for image 115: 0.6875\n",
      "→ Accuracy for image 116: 0.875\n",
      "→ Accuracy for image 117: 1.0\n",
      "→ Accuracy for image 118: 1.0\n",
      "→ Accuracy for image 119: 0.8125\n",
      "→ Accuracy for image 120: 0.625\n",
      "→ Accuracy for image 121: 0.625\n",
      "→ Accuracy for image 122: 0.75\n",
      "→ Accuracy for image 123: 0.6875\n",
      "→ Accuracy for image 124: 0.8125\n",
      "→ Accuracy for image 125: 0.75\n",
      "→ Accuracy for image 126: 0.75\n",
      "→ Accuracy for image 127: 0.875\n",
      "→ Accuracy for image 128: 0.875\n",
      "→ Accuracy for image 129: 0.625\n",
      "→ Accuracy for image 130: 0.625\n",
      "→ Accuracy for image 131: 0.75\n",
      "→ Accuracy for image 132: 0.9375\n",
      "→ Accuracy for image 133: 0.75\n",
      "→ Accuracy for image 134: 0.75\n",
      "→ Accuracy for image 135: 0.875\n",
      "→ Accuracy for image 136: 0.75\n",
      "→ Accuracy for image 137: 0.875\n",
      "→ Accuracy for image 138: 0.8125\n",
      "→ Accuracy for image 139: 0.9375\n",
      "→ Accuracy for image 140: 0.6875\n",
      "→ Accuracy for image 141: 0.9375\n",
      "→ Accuracy for image 142: 0.875\n",
      "→ Accuracy for image 143: 0.5625\n",
      "→ Accuracy for image 144: 0.75\n",
      "→ Accuracy for image 145: 0.875\n",
      "→ Accuracy for image 146: 0.875\n",
      "→ Accuracy for image 147: 0.8125\n",
      "→ Accuracy for image 148: 0.8125\n",
      "→ Accuracy for image 149: 0.875\n",
      "→ Accuracy for image 150: 0.8125\n",
      "→ Accuracy for image 151: 0.625\n",
      "→ Accuracy for image 152: 0.9375\n",
      "→ Accuracy for image 153: 0.6875\n",
      "→ Accuracy for image 154: 0.75\n",
      "→ Accuracy for image 155: 0.8125\n",
      "→ Accuracy for image 156: 0.625\n",
      "→ Accuracy for image 157: 0.9375\n",
      "→ Accuracy for image 158: 0.75\n",
      "→ Accuracy for image 159: 0.875\n",
      "→ Accuracy for image 160: 0.75\n",
      "→ Accuracy for image 161: 0.8125\n",
      "→ Accuracy for image 162: 0.625\n",
      "→ Accuracy for image 163: 0.8125\n",
      "→ Accuracy for image 164: 0.75\n",
      "→ Accuracy for image 165: 0.6875\n",
      "→ Accuracy for image 166: 0.625\n",
      "→ Accuracy for image 167: 0.6875\n",
      "→ Accuracy for image 168: 0.5\n",
      "→ Accuracy for image 169: 0.875\n",
      "→ Accuracy for image 170: 0.875\n",
      "→ Accuracy for image 171: 0.8125\n",
      "→ Accuracy for image 172: 0.75\n",
      "→ Accuracy for image 173: 0.6875\n",
      "→ Accuracy for image 174: 0.875\n",
      "→ Accuracy for image 175: 0.75\n",
      "→ Accuracy for image 176: 0.6875\n",
      "→ Accuracy for image 177: 0.625\n",
      "→ Accuracy for image 178: 0.875\n",
      "→ Accuracy for image 179: 0.6875\n",
      "→ Accuracy for image 180: 0.6875\n",
      "→ Accuracy for image 181: 0.5625\n",
      "→ Accuracy for image 182: 0.625\n",
      "→ Accuracy for image 183: 0.5625\n",
      "→ Accuracy for image 184: 0.625\n",
      "→ Accuracy for image 185: 0.875\n",
      "→ Accuracy for image 186: 0.6875\n",
      "→ Accuracy for image 187: 0.75\n",
      "→ Accuracy for image 188: 0.75\n",
      "→ Accuracy for image 189: 0.75\n",
      "→ Accuracy for image 190: 0.6875\n",
      "→ Accuracy for image 191: 0.75\n",
      "→ Accuracy for image 192: 0.6875\n",
      "→ Accuracy for image 193: 0.8125\n",
      "→ Accuracy for image 194: 0.5\n",
      "→ Accuracy for image 195: 0.6875\n",
      "→ Accuracy for image 196: 0.75\n",
      "→ Accuracy for image 197: 0.8125\n",
      "→ Accuracy for image 198: 0.6875\n",
      "→ Accuracy for image 199: 0.5625\n",
      "→ Accuracy for image 200: 0.625\n",
      "→ Accuracy for image 201: 0.625\n",
      "→ Accuracy for image 202: 0.4375\n",
      "→ Accuracy for image 203: 0.6875\n",
      "→ Accuracy for image 204: 0.875\n",
      "→ Accuracy for image 205: 0.75\n",
      "→ Accuracy for image 206: 0.8125\n",
      "→ Accuracy for image 207: 0.75\n",
      "→ Accuracy for image 208: 0.8125\n",
      "→ Accuracy for image 209: 0.8125\n",
      "→ Accuracy for image 210: 0.6875\n",
      "→ Accuracy for image 211: 0.8125\n",
      "→ Accuracy for image 212: 0.75\n",
      "→ Accuracy for image 213: 0.8125\n",
      "→ Accuracy for image 214: 0.8125\n",
      "→ Accuracy for image 215: 0.75\n",
      "→ Accuracy for image 216: 0.625\n",
      "→ Accuracy for image 217: 0.75\n",
      "→ Accuracy for image 218: 0.625\n",
      "→ Accuracy for image 219: 0.8125\n",
      "→ Accuracy for image 220: 0.6875\n",
      "→ Accuracy for image 221: 0.75\n",
      "→ Accuracy for image 222: 0.875\n",
      "→ Accuracy for image 223: 0.875\n",
      "→ Accuracy for image 224: 0.875\n",
      "→ Accuracy for image 225: 0.6875\n",
      "→ Accuracy for image 226: 0.625\n",
      "→ Accuracy for image 227: 0.875\n",
      "→ Accuracy for image 228: 0.6875\n",
      "→ Accuracy for image 229: 0.6875\n",
      "→ Accuracy for image 230: 0.75\n",
      "→ Accuracy for image 231: 0.625\n",
      "→ Accuracy for image 232: 0.625\n",
      "→ Accuracy for image 233: 0.6875\n",
      "→ Accuracy for image 234: 0.625\n",
      "→ Accuracy for image 235: 0.75\n",
      "→ Accuracy for image 236: 0.8125\n",
      "→ Accuracy for image 237: 0.625\n",
      "→ Accuracy for image 238: 0.75\n",
      "→ Accuracy for image 239: 0.75\n",
      "→ Accuracy for image 240: 0.875\n",
      "→ Accuracy for image 241: 0.75\n",
      "→ Accuracy for image 242: 0.5625\n",
      "→ Accuracy for image 243: 0.75\n",
      "→ Accuracy for image 244: 0.75\n",
      "→ Accuracy for image 245: 0.6875\n",
      "→ Accuracy for image 246: 0.5\n",
      "→ Accuracy for image 247: 0.5625\n",
      "→ Accuracy for image 248: 0.8125\n",
      "→ Accuracy for image 249: 0.6875\n",
      "→ Accuracy for image 250: 0.875\n",
      "→ Accuracy for image 251: 0.75\n",
      "→ Accuracy for image 252: 0.75\n",
      "→ Accuracy for image 253: 0.8125\n",
      "→ Accuracy for image 254: 0.6875\n",
      "→ Accuracy for image 255: 0.4375\n",
      "→ Accuracy for image 256: 0.5\n",
      "→ Accuracy for image 257: 0.875\n",
      "→ Accuracy for image 258: 0.625\n",
      "→ Accuracy for image 259: 0.5625\n",
      "→ Accuracy for image 260: 0.6875\n",
      "→ Accuracy for image 261: 0.875\n",
      "→ Accuracy for image 262: 0.875\n",
      "→ Accuracy for image 263: 0.875\n",
      "→ Accuracy for image 264: 0.625\n",
      "→ Accuracy for image 265: 0.6875\n",
      "→ Accuracy for image 266: 0.75\n",
      "→ Accuracy for image 267: 0.6875\n",
      "→ Accuracy for image 268: 0.8125\n",
      "→ Accuracy for image 269: 0.75\n",
      "→ Accuracy for image 270: 0.875\n",
      "→ Accuracy for image 271: 0.9375\n",
      "→ Accuracy for image 272: 0.875\n",
      "→ Accuracy for image 273: 0.875\n",
      "→ Accuracy for image 274: 0.8125\n",
      "→ Accuracy for image 275: 0.8125\n",
      "→ Accuracy for image 276: 0.6875\n",
      "→ Accuracy for image 277: 1.0\n",
      "→ Accuracy for image 278: 0.875\n",
      "→ Accuracy for image 279: 0.875\n",
      "→ Accuracy for image 280: 0.8125\n",
      "→ Accuracy for image 281: 0.5625\n",
      "→ Accuracy for image 282: 0.9375\n",
      "→ Accuracy for image 283: 0.875\n",
      "→ Accuracy for image 284: 0.8125\n",
      "→ Accuracy for image 285: 0.8125\n",
      "→ Accuracy for image 286: 0.9375\n",
      "→ Accuracy for image 287: 0.75\n",
      "→ Accuracy for image 288: 0.8125\n",
      "→ Accuracy for image 289: 0.9375\n",
      "→ Accuracy for image 290: 0.75\n",
      "→ Accuracy for image 291: 0.6875\n",
      "→ Accuracy for image 292: 0.5\n",
      "→ Accuracy for image 293: 0.8125\n",
      "→ Accuracy for image 294: 0.8125\n",
      "→ Accuracy for image 295: 0.6875\n",
      "→ Accuracy for image 296: 0.6875\n",
      "→ Accuracy for image 297: 0.75\n",
      "→ Accuracy for image 298: 0.75\n",
      "→ Accuracy for image 299: 0.8125\n",
      "→ Accuracy for image 300: 0.9375\n",
      "→ Accuracy for image 301: 0.625\n",
      "→ Accuracy for image 302: 0.6875\n",
      "→ Accuracy for image 303: 0.875\n",
      "→ Accuracy for image 304: 0.875\n",
      "→ Accuracy for image 305: 0.75\n",
      "→ Accuracy for image 306: 0.6875\n",
      "→ Accuracy for image 307: 0.6875\n",
      "→ Accuracy for image 308: 0.625\n",
      "→ Accuracy for image 309: 0.8125\n",
      "→ Accuracy for image 310: 0.875\n",
      "→ Accuracy for image 311: 0.875\n",
      "→ Accuracy for image 312: 0.5625\n",
      "→ Accuracy for image 313: 0.75\n",
      "→ Accuracy for image 314: 0.8125\n",
      "→ Accuracy for image 315: 0.6875\n",
      "→ Accuracy for image 316: 0.625\n",
      "→ Accuracy for image 317: 0.6875\n",
      "→ Accuracy for image 318: 0.75\n",
      "→ Accuracy for image 319: 0.875\n",
      "→ Accuracy for image 320: 0.8125\n",
      "→ Accuracy for image 321: 0.6875\n",
      "→ Accuracy for image 322: 0.6875\n",
      "→ Accuracy for image 323: 0.6875\n",
      "→ Accuracy for image 324: 0.625\n",
      "→ Accuracy for image 325: 0.8125\n",
      "→ Accuracy for image 326: 0.8125\n",
      "→ Accuracy for image 327: 0.8125\n",
      "→ Accuracy for image 328: 0.5625\n",
      "→ Accuracy for image 329: 0.6875\n",
      "→ Accuracy for image 330: 0.8125\n",
      "→ Accuracy for image 331: 0.875\n",
      "→ Accuracy for image 332: 0.8125\n",
      "→ Accuracy for image 333: 0.75\n",
      "→ Accuracy for image 334: 0.8125\n",
      "→ Accuracy for image 335: 0.9375\n",
      "→ Accuracy for image 336: 0.8125\n",
      "→ Accuracy for image 337: 0.75\n",
      "→ Accuracy for image 338: 0.875\n",
      "→ Accuracy for image 339: 0.75\n",
      "→ Accuracy for image 340: 0.875\n",
      "→ Accuracy for image 341: 0.75\n",
      "→ Accuracy for image 342: 0.625\n",
      "→ Accuracy for image 343: 0.75\n",
      "→ Accuracy for image 344: 0.6875\n",
      "→ Accuracy for image 345: 0.5625\n",
      "→ Accuracy for image 346: 1.0\n",
      "→ Accuracy for image 347: 0.75\n",
      "→ Accuracy for image 348: 0.5625\n",
      "→ Accuracy for image 349: 0.5\n",
      "→ Accuracy for image 350: 0.8125\n",
      "→ Accuracy for image 351: 0.6875\n",
      "→ Accuracy for image 352: 0.8125\n",
      "→ Accuracy for image 353: 0.625\n",
      "→ Accuracy for image 354: 0.625\n",
      "→ Accuracy for image 355: 0.5\n",
      "→ Accuracy for image 356: 0.8125\n",
      "→ Accuracy for image 357: 0.6875\n",
      "→ Accuracy for image 358: 0.8125\n",
      "→ Accuracy for image 359: 0.8125\n",
      "→ Accuracy for image 360: 0.5625\n",
      "→ Accuracy for image 361: 0.875\n",
      "→ Accuracy for image 362: 0.625\n",
      "→ Accuracy for image 363: 0.75\n",
      "→ Accuracy for image 364: 0.75\n",
      "→ Accuracy for image 365: 0.6875\n",
      "→ Accuracy for image 366: 0.9375\n",
      "→ Accuracy for image 367: 0.6875\n",
      "→ Accuracy for image 368: 0.875\n",
      "→ Accuracy for image 369: 0.75\n",
      "→ Accuracy for image 370: 0.9375\n",
      "→ Accuracy for image 371: 0.875\n",
      "→ Accuracy for image 372: 0.875\n",
      "→ Accuracy for image 373: 0.6875\n",
      "→ Accuracy for image 374: 0.8125\n",
      "→ Accuracy for image 375: 0.75\n",
      "→ Accuracy for image 376: 0.625\n",
      "→ Accuracy for image 377: 0.8125\n",
      "→ Accuracy for image 378: 0.875\n",
      "→ Accuracy for image 379: 0.875\n",
      "→ Accuracy for image 380: 0.875\n",
      "→ Accuracy for image 381: 0.875\n",
      "→ Accuracy for image 382: 0.5625\n",
      "→ Accuracy for image 383: 0.6875\n",
      "→ Accuracy for image 384: 0.75\n",
      "→ Accuracy for image 385: 0.5625\n",
      "→ Accuracy for image 386: 0.8125\n",
      "→ Accuracy for image 387: 0.75\n",
      "→ Accuracy for image 388: 0.8125\n",
      "→ Accuracy for image 389: 0.625\n",
      "→ Accuracy for image 390: 0.5625\n",
      "→ Accuracy for image 391: 0.5625\n",
      "→ Accuracy for image 392: 0.6875\n",
      "→ Accuracy for image 393: 0.8125\n",
      "→ Accuracy for image 394: 0.9375\n",
      "→ Accuracy for image 395: 0.75\n",
      "→ Accuracy for image 396: 0.875\n",
      "→ Accuracy for image 397: 0.6875\n",
      "→ Accuracy for image 398: 0.8125\n",
      "→ Accuracy for image 399: 0.75\n",
      "→ Accuracy for image 400: 0.75\n",
      "→ Accuracy for image 401: 0.875\n",
      "→ Accuracy for image 402: 0.5625\n",
      "→ Accuracy for image 403: 0.875\n",
      "→ Accuracy for image 404: 0.6875\n",
      "→ Accuracy for image 405: 0.6875\n",
      "→ Accuracy for image 406: 0.625\n",
      "→ Accuracy for image 407: 0.6875\n",
      "→ Accuracy for image 408: 0.8125\n",
      "→ Accuracy for image 409: 0.5625\n",
      "→ Accuracy for image 410: 0.6875\n",
      "→ Accuracy for image 411: 0.6875\n",
      "→ Accuracy for image 412: 0.8125\n",
      "→ Accuracy for image 413: 0.8125\n",
      "→ Accuracy for image 414: 0.75\n",
      "→ Accuracy for image 415: 0.6875\n",
      "→ Accuracy for image 416: 0.5625\n",
      "→ Accuracy for image 417: 0.5625\n",
      "→ Accuracy for image 418: 0.625\n",
      "→ Accuracy for image 419: 0.875\n",
      "→ Accuracy for image 420: 0.875\n",
      "→ Accuracy for image 421: 0.9375\n",
      "→ Accuracy for image 422: 0.9375\n",
      "→ Accuracy for image 423: 0.875\n",
      "→ Accuracy for image 424: 0.5625\n",
      "→ Accuracy for image 425: 0.75\n",
      "→ Accuracy for image 426: 0.75\n",
      "→ Accuracy for image 427: 0.75\n",
      "→ Accuracy for image 428: 0.6875\n",
      "→ Accuracy for image 429: 0.75\n",
      "→ Accuracy for image 430: 0.8125\n",
      "→ Accuracy for image 431: 0.875\n",
      "→ Accuracy for image 432: 0.8125\n",
      "→ Accuracy for image 433: 0.8125\n",
      "→ Accuracy for image 434: 0.6875\n",
      "→ Accuracy for image 435: 0.8125\n",
      "→ Accuracy for image 436: 0.9375\n",
      "→ Accuracy for image 437: 0.6875\n",
      "→ Accuracy for image 438: 0.75\n",
      "→ Accuracy for image 439: 0.625\n",
      "→ Accuracy for image 440: 0.75\n",
      "→ Accuracy for image 441: 0.6875\n",
      "→ Accuracy for image 442: 0.875\n",
      "→ Accuracy for image 443: 0.75\n",
      "→ Accuracy for image 444: 0.625\n",
      "→ Accuracy for image 445: 0.8125\n",
      "→ Accuracy for image 446: 0.9375\n",
      "→ Accuracy for image 447: 0.9375\n",
      "→ Accuracy for image 448: 0.8125\n",
      "→ Accuracy for image 449: 0.625\n",
      "→ Accuracy for image 450: 0.625\n",
      "→ Accuracy for image 451: 0.625\n",
      "→ Accuracy for image 452: 0.6875\n",
      "→ Accuracy for image 453: 0.8125\n",
      "→ Accuracy for image 454: 0.875\n",
      "→ Accuracy for image 455: 0.875\n",
      "→ Accuracy for image 456: 0.75\n",
      "→ Accuracy for image 457: 0.8125\n",
      "→ Accuracy for image 458: 0.75\n",
      "→ Accuracy for image 459: 0.75\n",
      "→ Accuracy for image 460: 0.6875\n",
      "→ Accuracy for image 461: 0.75\n",
      "→ Accuracy for image 462: 0.9375\n",
      "→ Accuracy for image 463: 0.8125\n",
      "→ Accuracy for image 464: 0.5\n",
      "→ Accuracy for image 465: 0.75\n",
      "→ Accuracy for image 466: 0.8125\n",
      "→ Accuracy for image 467: 0.875\n",
      "→ Accuracy for image 468: 0.75\n",
      "→ Accuracy for image 469: 0.6875\n",
      "→ Accuracy for image 470: 0.875\n",
      "→ Accuracy for image 471: 0.75\n",
      "→ Accuracy for image 472: 0.8125\n",
      "→ Accuracy for image 473: 0.8125\n",
      "→ Accuracy for image 474: 0.875\n",
      "→ Accuracy for image 475: 0.6875\n",
      "→ Accuracy for image 476: 0.9375\n",
      "→ Accuracy for image 477: 0.75\n",
      "→ Accuracy for image 478: 0.8125\n",
      "→ Accuracy for image 479: 0.8125\n",
      "→ Accuracy for image 480: 0.8125\n",
      "→ Accuracy for image 481: 0.75\n",
      "→ Accuracy for image 482: 0.6875\n",
      "→ Accuracy for image 483: 0.875\n",
      "→ Accuracy for image 484: 0.6875\n",
      "→ Accuracy for image 485: 0.875\n",
      "→ Accuracy for image 486: 0.8125\n",
      "→ Accuracy for image 487: 0.9375\n",
      "→ Accuracy for image 488: 0.5\n",
      "→ Accuracy for image 489: 0.8125\n",
      "→ Accuracy for image 490: 0.5625\n",
      "→ Accuracy for image 491: 0.8125\n",
      "→ Accuracy for image 492: 0.625\n",
      "→ Accuracy for image 493: 0.8125\n",
      "→ Accuracy for image 494: 0.6875\n",
      "→ Accuracy for image 495: 0.75\n",
      "→ Accuracy for image 496: 0.75\n",
      "→ Accuracy for image 497: 0.75\n",
      "→ Accuracy for image 498: 0.8125\n",
      "→ Accuracy for image 499: 0.75\n",
      "→ Accuracy for image 500: 0.5625\n",
      "→ Accuracy for image 501: 0.8125\n",
      "→ Accuracy for image 502: 0.6875\n",
      "→ Accuracy for image 503: 0.75\n",
      "→ Accuracy for image 504: 0.75\n",
      "→ Accuracy for image 505: 0.875\n",
      "→ Accuracy for image 506: 0.875\n",
      "→ Accuracy for image 507: 0.8125\n",
      "→ Accuracy for image 508: 0.6875\n",
      "→ Accuracy for image 509: 0.6875\n",
      "→ Accuracy for image 510: 0.75\n",
      "→ Accuracy for image 511: 0.9375\n",
      "→ Accuracy for image 512: 0.6875\n",
      "→ Accuracy for image 513: 0.6875\n",
      "→ Accuracy for image 514: 0.75\n",
      "→ Accuracy for image 515: 0.875\n",
      "→ Accuracy for image 516: 0.625\n",
      "→ Accuracy for image 517: 0.6875\n",
      "→ Accuracy for image 518: 0.75\n",
      "→ Accuracy for image 519: 0.8125\n",
      "→ Accuracy for image 520: 0.6875\n",
      "→ Accuracy for image 521: 0.8125\n",
      "→ Accuracy for image 522: 0.8125\n",
      "→ Accuracy for image 523: 0.75\n",
      "→ Accuracy for image 524: 0.8125\n",
      "→ Accuracy for image 525: 0.8125\n",
      "→ Accuracy for image 526: 0.75\n",
      "→ Accuracy for image 527: 0.75\n",
      "→ Accuracy for image 528: 0.75\n",
      "→ Accuracy for image 529: 0.75\n",
      "→ Accuracy for image 530: 0.875\n",
      "→ Accuracy for image 531: 0.75\n",
      "→ Accuracy for image 532: 0.75\n",
      "→ Accuracy for image 533: 0.625\n",
      "→ Accuracy for image 534: 0.75\n",
      "→ Accuracy for image 535: 0.75\n",
      "→ Accuracy for image 536: 0.875\n",
      "→ Accuracy for image 537: 0.875\n",
      "→ Accuracy for image 538: 0.6875\n",
      "→ Accuracy for image 539: 0.75\n",
      "→ Accuracy for image 540: 0.875\n",
      "→ Accuracy for image 541: 0.6875\n",
      "→ Accuracy for image 542: 0.6875\n",
      "→ Accuracy for image 543: 0.9375\n",
      "→ Accuracy for image 544: 0.5\n",
      "→ Accuracy for image 545: 0.8125\n",
      "→ Accuracy for image 546: 0.6875\n",
      "→ Accuracy for image 547: 0.5\n",
      "→ Accuracy for image 548: 0.625\n",
      "→ Accuracy for image 549: 0.875\n",
      "→ Accuracy for image 550: 0.8125\n",
      "→ Accuracy for image 551: 0.9375\n",
      "→ Accuracy for image 552: 0.6875\n",
      "→ Accuracy for image 553: 0.8125\n",
      "→ Accuracy for image 554: 0.625\n",
      "→ Accuracy for image 555: 0.875\n",
      "→ Accuracy for image 556: 0.875\n",
      "→ Accuracy for image 557: 0.75\n",
      "→ Accuracy for image 558: 0.875\n",
      "→ Accuracy for image 559: 0.8125\n",
      "→ Accuracy for image 560: 0.6875\n",
      "→ Accuracy for image 561: 0.75\n",
      "→ Accuracy for image 562: 0.75\n",
      "→ Accuracy for image 563: 0.75\n",
      "→ Accuracy for image 564: 0.6875\n",
      "→ Accuracy for image 565: 0.875\n",
      "→ Accuracy for image 566: 0.6875\n",
      "→ Accuracy for image 567: 0.6875\n",
      "→ Accuracy for image 568: 0.8125\n",
      "→ Accuracy for image 569: 0.875\n",
      "→ Accuracy for image 570: 0.625\n",
      "→ Accuracy for image 571: 0.6875\n",
      "→ Accuracy for image 572: 0.6875\n",
      "→ Accuracy for image 573: 0.8125\n",
      "→ Accuracy for image 574: 0.8125\n",
      "→ Accuracy for image 575: 0.6875\n",
      "→ Accuracy for image 576: 0.75\n",
      "→ Accuracy for image 577: 0.8125\n",
      "→ Accuracy for image 578: 0.625\n",
      "→ Accuracy for image 579: 0.6875\n",
      "→ Accuracy for image 580: 0.75\n",
      "→ Accuracy for image 581: 0.75\n",
      "→ Accuracy for image 582: 0.8125\n",
      "→ Accuracy for image 583: 0.75\n",
      "→ Accuracy for image 584: 0.625\n",
      "→ Accuracy for image 585: 0.625\n",
      "→ Accuracy for image 586: 0.8125\n",
      "→ Accuracy for image 587: 0.8125\n",
      "→ Accuracy for image 588: 0.8125\n",
      "→ Accuracy for image 589: 0.9375\n",
      "→ Accuracy for image 590: 0.6875\n",
      "→ Accuracy for image 591: 0.8125\n",
      "→ Accuracy for image 592: 0.625\n",
      "→ Accuracy for image 593: 0.625\n",
      "→ Accuracy for image 594: 0.6875\n",
      "→ Accuracy for image 595: 0.8125\n",
      "→ Accuracy for image 596: 0.625\n",
      "→ Accuracy for image 597: 0.75\n",
      "→ Accuracy for image 598: 0.75\n",
      "→ Accuracy for image 599: 0.9375\n",
      "→ Accuracy for image 600: 0.75\n",
      "→ Accuracy for image 601: 0.8125\n",
      "→ Accuracy for image 602: 0.6875\n",
      "→ Accuracy for image 603: 0.6875\n",
      "→ Accuracy for image 604: 0.9375\n",
      "→ Accuracy for image 605: 0.75\n",
      "→ Accuracy for image 606: 1.0\n",
      "→ Accuracy for image 607: 0.75\n",
      "→ Accuracy for image 608: 0.8125\n",
      "→ Accuracy for image 609: 0.9375\n",
      "→ Accuracy for image 610: 0.9375\n",
      "→ Accuracy for image 611: 0.8125\n",
      "→ Accuracy for image 612: 0.625\n",
      "→ Accuracy for image 613: 0.9375\n",
      "→ Accuracy for image 614: 0.8125\n",
      "→ Accuracy for image 615: 0.8125\n",
      "→ Accuracy for image 616: 0.625\n",
      "→ Accuracy for image 617: 0.75\n",
      "→ Accuracy for image 618: 0.8125\n",
      "→ Accuracy for image 619: 0.8125\n",
      "→ Accuracy for image 620: 0.8125\n",
      "→ Accuracy for image 621: 0.8125\n",
      "→ Accuracy for image 622: 0.5\n",
      "→ Accuracy for image 623: 0.75\n",
      "→ Accuracy for image 624: 0.9375\n",
      "=== The testing loop has finished ===\n",
      "→ Final testing accuracy of the model: 0.937037037037037\n",
      "=== The training has finished ===\n"
     ]
    }
   ],
   "source": [
    "for epoch_ind in range(epochs):\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, epoch_ind)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"=== The training has finished ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train a non sequential model. We just repeat the two previous steps with the non sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_ind in range(epochs):\n",
    "    train_loop(train_dataloader, model_no_seq, loss_fn, optimizer, epoch_ind)\n",
    "\n",
    "test_loop(test_dataloader, model_no_seq, loss_fn)\n",
    "\n",
    "print(\"=== The training has finished ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
