{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><b>AI Lab: Computer Vision and NLP</b></h1>\n",
    "<h3 align=\"center\">Lessons 19-20: Convolutional Neural Networks</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**onvolutional **N**eural **N**etworks (**CNN**s) are a specific type of neural networks that are composed by convolution layers and pooling layers. At the end of the CNN we still have a MLP. The purpose of the CNN is to extract features from the images (in fact, CNNs are usually used with images).\n",
    "\n",
    "What are the pooling layers? They are layers which resize (usually downsize) the image [RECOVER ALL THEORY]\n",
    "\n",
    "CNNs can be implemented with `pytorch` in the following way. First, we must import the packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, import the dataset and detect if there is an nVidia GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"pytorch_datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"pytorch_datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can proceed to create our CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, 3), # 1 is the input (because it's a gray-scale image),\n",
    "                                # 5 is the output, 3 is the kernel size\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 10, 3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(24 * 24 * 10, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        x = self.cnn(x)\n",
    "        if debug: print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if debug: print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need to compute the size of the feature map? We always have to remember that MLPs must have the layers coded in such a way that the output size of a layer equals the input layer of the next layer. The same goes for the `Conv2D()` layer. If we notice, the first `Conv2D()` layer has 1 as input size and 5 as output size, and the second layer has 5 as input size and 10 as output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OurCNN().to(device)\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer. We can use either SGD or AdamW, but AdamW is more performant than SGD\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to train, let's try to test the model with a random tensor. Differently from `OpenCV` and `numpy`, in `PyTorch` a tensor is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{tensor} \\; = \\; \\begin{pmatrix}\n",
    "    \\text{number of channels}, & \\text{width}, & \\text{height}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "test_x = torch.rand((1, 28, 28))\n",
    "test_y = model(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever there must be a training of images, there usually is a fourth parameter:\n",
    "\n",
    "$$\n",
    "\\text{tensor} \\; = \\; \\begin{pmatrix}\n",
    "    \\colorbox{#645e0d}{\\text{batch size}}, & \\text{number of channels}, & \\text{width}, & \\text{height}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Let's now define training and testing loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, epoch=None, debug=True):\n",
    "    \"\"\"Trains an epoch of the model\n",
    "    \n",
    "    Parameters:\n",
    "        - `dataloader`: the dataloader of the dataset\n",
    "        - `model`: the model used\n",
    "        - `loss_fn`: the loss function of the model\n",
    "        - `optimizer`: the optimizer\n",
    "        - `epoch`: the index of the epoch\n",
    "    \"\"\"\n",
    "    size = len(dataloader)\n",
    "\n",
    "    # Get the batch from the dataset\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        # Move data to the device used\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute the prediction and the loss\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Adjust the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Print some information\n",
    "        if batch % 20 == 0:\n",
    "            loss_value, current_batch = loss.item(), (batch + 1) * len(x)\n",
    "            if debug: print(f\"→ Loss: {loss_value} [Batch {current_batch}/{size}, Epoch {epoch}/{epochs}]\")\n",
    "            accuracy = metric(pred, y)\n",
    "            if debug: print(f\"Accuracy of batch {current_batch}/{size}: {accuracy}\")\n",
    "        \n",
    "    accuracy = metric.compute()\n",
    "    print(f\"=== The epoch {epoch}/{epochs} has finished training ===\")\n",
    "    if debug: print(f\"→ Final accuracy of the epoch: {accuracy}\")\n",
    "    metric.reset()\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, debug=True):\n",
    "    size = len(dataloader)\n",
    "\n",
    "    # Remove the Dropout and Batch normalization layers\n",
    "    model.eval()\n",
    "\n",
    "    # Disable the updating of the weights\n",
    "    with torch.no_grad():\n",
    "        for index, (x, y) in enumerate(dataloader):\n",
    "            # Move the data to the device used for testing\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Get the model prediction\n",
    "            pred = model(x)\n",
    "\n",
    "            # Get the accuracy score\n",
    "            acc = metric(pred, y)\n",
    "            if debug: print(f\"→ Accuracy for image {index}: {acc}\")\n",
    "    acc = metric.compute()\n",
    "    print(f\"===    The testing loop has finished    ===\")\n",
    "    if debug: print(f\"→ Final testing accuracy of the model: {acc}\")\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_ind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m----> 2\u001b[0m     train_loop(training_data, model, loss_fn, optimizer, epoch_ind, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m     test_loop(test_data, model, loss_fn, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== The training has finished ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, epoch, debug)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Move data to the device used\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Compute the prediction and the loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "for epoch_ind in range(epochs):\n",
    "    train_loop(training_data, model, loss_fn, optimizer, epoch_ind, debug=False)\n",
    "    test_loop(test_data, model, loss_fn, debug=False)\n",
    "\n",
    "print(\"=== The training has finished ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
